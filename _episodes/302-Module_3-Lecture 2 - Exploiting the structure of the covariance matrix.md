---
title: Module 3 Lecture 2 - Exploiting the structure of the covariance matrix
teaching: 
exercises: 
questions:
- "???"
objectives:
- "= = ="
keypoints:
- "- - -"
---

### Module 3 Lecture 2: Exploiting the structure of the covariance matrix

In this lecture, we are going to examine the structure of the covariance matrix and thus the correlation matrix to see how their properties help us undertake feature reduction. We have met the covariance metrics several times in the past. It is the starting point for the principal components transformation and along with the main vector, defines the class signature when we undertake maximum likelihood classification. In the latter context, we know that the accurate computation of the class covariance matrix can be a problem for hyperspectral data if we do not have enough training samples per class. Remember, we have to have enough independent training samples in each class in order to estimate reliably the elements of the covariance matrix. Generally, that is not a problem with the principal components transformation because then the covariance matrix is computed using all of the available training samples and not just those for an individual class. We can examine the covariance matrix for the data as a whole. We will do that now and note that it has some interesting structural properties. Rather than the covariance matrix itself though, we will look at the correlation matrix instead. Which remember, is derived from the elements of the covariance matrix. Let's look at an example of a real correlation matrix. Here we will examine the metrics for the Jasper Ridge, USA image, which has 196 bands. The correlation matrix for this image will be of dimensions 196 by 196. This is far too large for us to write down. We can, however, represent it in an image form as shown to the right. Remember, correlations are in the range of minus one to plus one. A correlation of zero means just that. There is no correlation between the pair of bands. Whereas a correlation of plus one means total positive correlation. While minus one means total negative correlation. If we choose a gray scale where black represents zero correlation and white means total positive or negative correlation, we get a very interesting representation of the correlation matrix. Note that there are regions of high correlation distributed down the principal diagonal of the matrix. That means that the bands in those blocks are highly correlated. Where there are black box, the associated bands have little correlation among them. There are some groups of bands which leave the correlations off the diagonal, suggesting some interrelationships of bands in the middle infrared and between the middle infrared and visible regions. In general, though, we can assume that most of the correlation activity is down the principal diagonal. What does that now suggest for feature reduction? Here we neglect the off diagonal correlations and represent the correlation matrix just by the blocks that form down the diagonal. That matrix now becomes a block diagonal matrix which has a number of very interesting properties. If we write the block diagonal matrix in the form shown in the center of this slide, composed of H separate blocks down the diagonal, a number of simple and useful properties arise as we now see on the next slide. First, the determinant of a block diagonal matrix is the product of the determinants of the individual blocks. That means the logarithm of the determinant is the sum of the logarithms of the individual determinants. Secondly, the trace of the matrix is the sum of the traces of the individual blocks. Finally, the inverse of the correlation matrix can be computed using the inverses of the individual blocks as shown on the bottom of this slide. Now what happens to a column vector which has the same dimensions as the covariance or correlation matrices? This could be, for example, the main vector which corresponds to the covariance matrix. Clearly, it can be represented as the concatenation vertically of a set of smaller vectors corresponding to the blocks of the covariance matrix. That leads to a particularly important matrix identity, which says that the vector, matrix vector construct that we use regularly in image processing can be decomposed into the sum of the same expression computed over each of the blocks. We now have all the material needed to show how block diagonalizing the covariance matrix simplifies the principal components transformation and the maximum likelihood procedure. Just before we do that, we can note a further simplification. Rather than utilizing the actual blocks of high correlation down the diagonal, we could just restrict our attention to the correlations that exist among immediately adjacent bands. For example, as shown in this slide, we could just use the two by two or three by three blocks indicated. While probably not as accurate as retaining the true block diagonal structure of the matrix. Such a simplification reduces dramatically the number of bands that had to be considered when training a classifier, as we will see shortly. Irrespective of whether we use that maturation simplification or stay with the original block diagonal form of the matrix, we now look at how the block structuring of the covariance matrix affects the formula for the principal components transformation. Remember, to compute the eigenvalues of the covariance matrix, we have to solve the characteristic equation shown here in determinant form. When the covariance matrix is blocked diagonalized, the characteristic equation is a product of a set of equations of smaller dimensions. Thus the roots of the equation are the eigenvalues of the component block matrices. Likewise, we can find the eigenvectors and the principal components transformation matrix by computing everything on a block basis and then combining results. Now let's see how blocked diagonalizing the covariance matrix and equivalently, petitioning the main vector affects the maximum likelihood classifier using the rules above for block diagonal matrices. The discriminant function for the Gaussian maximum likelihood classifier can be shown to reduce to the expression in the middle of this slide. Although using the block diagonal approximation to the covariance matrix ignores correlations among bands that are widely separated, it does have significant benefits in terms of reducing the dimensionality that has to be considered when applying the maximum likelihood classification rule. The largest matrix for which the elements need to be estimated using training data is that of the largest block. In general, that will be significantly smaller than the original matrix, meaning that many fewer training pixels are needed for reliable estimates, rendering the maximum likelihood rule useful for hyperspectral thematic mapping. We now look at an example of the performance of the maximum likelihood classifier with block diagonalization taken from the paper listed here. Two AVIRIS images were used based on the five different tests listed at the bottom of the slide that the last test applies just the minimum distance rule. Full details of these tests will be seen in the paper that I have referenced. The results of the tests are shown here in graphical form, so they can be compared. The classification accuracy shown is the average performance on training and testing data. Note how good the results are, particularly when the actual block diagonal form is used. What these results don't show, but which is included in the original paper is that there is a significant improvement in classification time with the smaller blocks. That is because the classification time for the maximum likelihood rule is quadratically dependent on the number of bands used. There are four key messages from this lecture. Covariance and correlation matrices can be represented as images. Strong band-to-band correlations appear in blocks, largely down the principal diagonal of the covariance or correlation matrix. The block diagonal structure can be exploited to simplify image processing operations that require computation of a covariance matrix. The block diagonal form of the covariance matrix allows the maximum likelihood classifier to be applied to hyperspectral image data. The last two questions here focus your attention on understanding what the image representation of the correlation or covariance matrix tells us. 

> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
>    {: .solution}
 {: .challenge}

{% include links.md %}

---
title: All the remaining Modules(not edited)
teaching: 
exercises: 
questions:
- "???"
objectives:
- "= = ="
keypoints:
- "- - -"

---



### Module 2 Lecture 11: The neural network as a classifier

We now come to the 4th of the classification techniques, we're going to consider in this course. We will develop it in two stages. In its original form, and its return in a more powerful inflexible form, over the past five to 10 years. The neural network, sometimes called the artificial neural network. Was popular in remote sensing in the 1990s. But be cause it was complex to design, and required significant computational power. It was overtaken by other techniques, such as a support vector machine. However, with some simple modifications that lead to improvements in performance and training. It is gained popularity again, over the past decade. Now called convolutional neural networks. These later variants also go under the name of deep learning. All that that description could just as easily have applied to the neural network in its original form. As with the support vector machine, we start by a return to the simple linear classifier. Again, we will do our development with a simple 2 dimensional, 2 class situation. But it will generalize to any number of classes, and dimensions. And again, we use our standard decision rule for linear decisions. Which will now represent diagrammatically, in the form of what is called a threshold logic unit or TLU. In this diagram, the elements up to the thresholding block, create the linear function used in the decision rule. The thresholding operation, then chicks with the value of the function is positive or negative. And thus whether the Pixel Victor is in class 1 or class 2, as required by the algebraic expression of the decision rule. Sometimes we represent the overall operation, or the single block called the tail u. As noted on the previous slide, this was one of the building blocks used in early machine learning theory. That led to a classification machine called the perceptron. It is also where we start the development of the neural network. A breakthrough came when the hard limiting operation in the TLU, also replaced by softer function. And in particular, one that could be differentiated. As we will see, that allows a training procedure to be derived. Which is not otherwise possible. We call the Soft limiting operation an activation function. Typical examples, include the inverse exponential operation, or sigmoid. And the hyperbolic tangent as shown in this slide. As seeing the behavior still represents what we want, in terms of specifying the class to which a pixel belongs. Because it implements our decision rule, but without a hard limit. The old TLU but with a soft limiting operation, is now called a processing element or PE. In the nomenclature of neural networks, we also replace the offset W subscript N plus 1, by the symbol theta. But in the bottom right hand drawing, that we will write the output of the processing element as g equals a function of z. Where the function f, is the chosen form of the activation function. And z is the linear function in our normal decision rule. The classical neural network, and that which was applied wildly in remote sensing. Is called the multilayer perceptron, or MLP. And is composed of layers of processing elements, which are fully connected with each other as seen in this slide. The blocks in the first layer, are not actually preciously elements. They just distribute the input pixel vector elements, to each of the processing elements in the next layer. The outputs of those processing elements, then form the inputs to another layer of PEs. And so on, for as many layers as chosen by the user. The outputs from the last layer of pairs, determine the class of the pixel vector fed into the first layer. Now, the user can choose how that is done. Options are, that each class could be represented by a single output. Or the set of outputs could represent, for example, a binary code that specifies the class. Note the nomenclature used with the layers of a neural network. In particular, the first layer which does any real analysis, is the first hidden layer. Note also the litter designations we apply to the layers. And while we have shown only one, there can be many hidden layers. Each being fed from the outputs of the previous layer. As the number of hidden layers increases, training the neural network becomes increasingly time consuming. In many remote sensing exercises of several decades ago, only one hidden layer was used. And found sufficient to handle many situations. But training still took along time, as we will see later. Having chosen a topology, we now need to work out how to find its unknown parameters. That is, the weights W, and the offsets theta for each processing element. As with all supervised classifiers, that is done through training, unlabeled reference data. But to make that possible, we need to understand the network equations. So the training process can be derived. That is now our immediate objective. The equations describing each processing element, of those we noted earlier that is G. Is a function of WTX plus theta. And remember, WTX plus theta is the linear function. However, we need a naming convention. To keep track of where the inputs come from, and where the outputs go. We do that by adding subscripts to the white, and offsets as shown here. And nomenclature is simplified in that is layer specific, but not PE specific. We could add a third subscript to indicate each actual PE in a layer. But that turns out to be unnecessary. We can derive the relevant network equations for training, without going to that added complexity. To start developing their training procedure, we need a measure of what we're trying to achieve. Clearly, if the network is functioning well as a classifier. We want the output to be accurate, when the network is presented with the previously unseen pixel vector. We check the networks performance by setting up an error measure. And measure that looks at how closely the actual output matches what we expect. When a training pixel is fed into the network. We choose for that measure, the squared difference error measure. Shown in the center of the slide. Remember, the set of actual outputs of the network, energy subscript k. This measure, tells us how well those actual outputs match the desired or target outputs. t subscript k for a given training pixel vectors. In the next lecture, we will use that expression to help set up the necessary equations. With which we can train the neural network. Clearly, our objective is to find the unknowns, weights and offsets. That minimize the error measure. In other words, that might be the actual class labels match as nearly as possible. The correct or target labels. In this part of the course, we are examining the use of the popular multilayer perceptrons. As a classifier in remote sensing. It consists of layers of processing elements, where each PE contains a differentiable activation function. We are now at the stage where we can set up the network equations. The second question here is important. Because it starts to develop a feel for the complexity of training a neural network. 

### Module 2 Lecture 12: Training the neural network

We now look at how we can train a neural network. While this analysis is important in its own right, it turns out that the same process can be used when we come to look at the more modern convolutional neural networks later. We now wish to devise a training pressures for the neural network by seeking to minimize the error function we set up in the last lecture. We do that by making small adjustments to the set of weights such that those adjustments lead to a reduction in the error. The approach commonly taken is to modify a weight by subtracting a small amount of the first derivative of the error from its initial value as shown in this slide. The idea is that by doing so, we will move down the error curve as indicated. This is called the gradient descent approach. There are other adjustment procedures too, but the simple gradient descent method is good for illustration. The amount of adjustment is controlled by the parameter ETA, which is called the learning right. A two larger value of ETA leads to greater adjustments, but may lead to instability. That is, oscillations between both sides of the error curve in the illustration. On the other hand, a two smaller value of ETA means training time might be lengthened. The adjustment here is shown for the weights that the j and k layers. To workout a value for the adjustment to the weight, we need to perform the differentiation shown here. That is done by the chain rule as seen in the center of the slide. We now have to get values for each of the derivatives in that chain rule expression. Here we show each of those three derivatives and the final result when they are combined in the chain rule. Choosing b equals one in the activation function, we end up with the correction increment shown on the bottom of the slide. Let's call that equation A for lighter convenience. We now move to the front of the network and look to find the correction increments for the weights which think the i and j layers. We use the same gradient descent procedure as before to do that. But here we have a small problem. Since E is not directly a function of g subscript j, we cannot compute the derivative, dE, dgj simply. Instead, we need to use another chain rule expression as shown on the bottom of the slide. To get a value for dE, dzk in this expression, we again use the chain rule which would be equals one. We have an expression for dE, dzk in terms of the actual and target outputs. That leads to the expression for the correction increment for the i to j weights as soon. Which is not only a function of the actual target outputs, that is also dependent on the k to j linear weights. From the previous step, we know those said that we now have a suitable and usable expression for delta wji correction increments. With the two sets of analyses, we can now formulate a training algorithm for the neural network. We can simplify our two previous equations if we define some new variables delta k and delta j, which allow the correction increments for the two sets of linkages to be written simply as shown at the bottom of the slide. Wall out equations are specifically focused on adjustments to the weights, the thresholds delta j and delta k in the network equations can be evaluated using the same expressions just by making the corresponding inputs unity during training. We now formulate the training strategy. The chosen network is initiated with an arbitrary set of weights that allows outputs, although in error, to be generated by the presentation of training pixel vectors at the input layer. For each training pixel, the network output is computed from the set of network equations and initially, of course, that output will be an error. Correction to the weights is in performed using the equations of the previous slide. The value of delta k is computed first since it depends on the network outputs g subscript k compared with the target outputs t sub k. Then the result can be propagated back through the network, layer by layer, if there is more than one hidden layer using the other equations on the previous slide to generate corrections to the network weights. Specifically delta wkj, which is equal to ETA delta kgj can then be found. Following which, we get the delta j and then the delta wji. When all the weights have been adjusted, the output of the network is computed again using those new weights. Hopefully, the gk will now be closer to the target values tk. New values for the delta k will then be generated and the process of weight adjustment is repeated. This process is iterated as often as needed to reduce the difference between the actual and target outputs, tk minus gk to zero, or to a value acceptably close to zero. If it is zero, then delta k will be zero meaning that no further adjustments to the weights will occur with further iteration. The network is in fully trained. In the terminology of neural networks, an iteration is also called an epoch. Because training involves working back from the outputs at each epoch or iteration, the training process is referred to as back propagation. The interesting thing about training the multilayer perceptron is that when a training pixel is presented at the input, the calculations are propagated forward through the network to generate the output. That output is checked against the correct class for that training pixel, and if found to be an error, the equations were derived in this lecture are used to propagate backwards through the network, the adjustments to those weights. The first question here draws your attention to the possibility of local minima in the error curve. 

### Module 2 Lecture 13: Neural network examples

We will now look at some examples to illustrate the training and performance of the neural network. When considering the application of the neural network, there are a number of decisions that need to be taken beforehand about the network topologies. These include how many hidden layers to use, how many nodes should be used in each layer, how to use the output layer to represent the thematic classes, and what value to assign to the learning parameter. Generally, the first layer will have as many nodes as there are elements through the pixel vector. Often there will be as many output layer nodes as there are classes unless some form of c [inaudible] to reduce their number. Generally, the number of nodes in the hidden layer should not be less than the number of output layer nodes. Note that the multi-layer perceptron has all the connections in place that we described earlier, that is the output of a processing element or a node in any layer, is connected to every node in the next layer; that is called fully-connected. Later, in the context of the convolutional neural network, we will not use all of those connections. By way of illustrations, we start with a very simple example involving two classes in a two-dimensional vector space. Note that the classes are not linearly separable, as seen in the figure. We have chosen a network with two nodes in the hidden layer and two in the output layer. The network equations are shown explicitly on the right hand network diagram. Note also that we have chosen a zero threshold theta for the hidden layer processing elements, and we have also chosen b equals one in the activation function and eta equals two as the learning perimeter. The network was initialized with a set of weights shown in the first row of this table. As seen, the error before iteration was 0.461. The network was then trained for 250 iterations at which the error had been reduced to 0.005. At the same time, the whites can be seen to be converging to fixed, or final values. We stop training at 250 iterations and use the parameter values at that point. On the right hand axis, we have plotted the arguments of the two hidden layer PEs before the application of the activation functions. Effectively, when equated to zero, they've implemented linear separating surfaces. The activation function then places a response for a given pixel on either side of those surfaces. Each surface therefore segments the data space into two regions. In the output lab PE, [inaudible] responses segment the full space into the two class regions shown. Effectively, it implements a logical OR operation that is shown mathematically in the bottom table, which shows explicitly how the output layer functions for each of the four possibilities of patterns being placed on either side of the first two surfaces. Having trained the network, we need to see how successful it is in separating sets of pixel vectors that it has not previously seen. In the table here, there are eight new pixels. They can also be seen in the vector space. Patterns A to D are in class one, while patterns E to H are in class two as is evident on the diagram. The table shows the intervening calculations and the final classification by the network for each pixel. All testing pixels have been successfully labeled. We now come to a real remote sensing example taken from a 1995 paper that is listed on this slide. The data-set consisted of the six non-thermal bands of a 900 by 900 pixel segment of the thematic mapper scene recorded over Tucson, Arizona on 1 April 1987. There are 12 classes evident in the same. They were chosen by the authors. The band for the infrared image shown here does not make those classes easily seen, but the grid structure of Tucson streets is evident. In keeping with mastery [inaudible] exercises of time involving neural networks, the authors chose a network was just one hidden layer. Since it was six bands, the input layer consisted of six nodes or processing elements. Those nodes also scaled the data to the range between zero and one. Since there were 12 classes, the output layer was chosen to have 12 nodes with each representing a single class. The scale of the outputs was chosen such that during training, an output of 0.9 on a node indicated a target class while a value of 0.1 means that the class does not respond to the training pixel being presented. The hidden layer was chosen to have 18 nodes. Since the authors decided to compare the neural network results against those obtained with a maximum likelihood classifier, the choice of the hidden layer nodes was based on having the same number of parameters to determine as for the maximum likelihood rule. This slide shows the information classes and the numbers of training and testing pixels used by the authors. Although the network was allowed to run for 50,000 iterations or epochs, the error has stabilized after about 15,000 iterations. Note that more than 96 percent of the training pixels are properly handled once the network has reached that number of iterations. It is because so many iterations are needed to train a neural network in practice, that training time can be so excessive. The network performance using unseen testing pixels was a very good 93.4 percent accuracy. If training was stopped after 10,000 iterations, the network was still capable of achieving 92 percent accuracy. If stopped at 20,0000 iterations, that improved marginally to 93 percent. A maximum likelihood classifier was also run on the same data set, although there is no indication as to whether it was optimized for the choice of sets of spectral classes to represent the specified information classes, which we will do in other examples in module 3. Nevertheless, the maximum likelihood classifier achieved 89.5 percent accuracy on the testing data, but it was 10 times faster to train. This slide shows the thematic map produced by the neural network on the right-hand side, along with the key to the colors. The authors included two variations to the standard neural network training process to improve the learning rate. The first wants to add a momentum term to the gradient descent rule used to adjust the weights. On the top of this slide, we summarize the standard gradient descent adjustment. On the bottom in green, an additional term is added. It is chosen as a proportion of the previous weight adjustment, which forces a modification to follow the pattern of the previous iteration. Another perimeter is introduced in this process, Alpha, which controls the degree of momentum used. The second modification was to adjust the learning and momentum rate adaptively in order to improve convergence. That was done every fourth iteration according to the rule shown on the top of the slide. Note that the convergence and the ultimate result of neural network training can be affected by the initial choice of words and that the initial set cannot all be the same. Otherwise, of course, the network will not train. More details on this example will be found in the paper. However, this original neural network approach is now rarely used. We introduced it here as preparation for the more recent development of the convolutional neural network, which we commence in the next lecture. When we come to the convolutional neural network, we will often talk about deep-learning. Simply put, network depth is described by the number of hidden layers. A deeper network has more. The idea is that when there are more hidden layers, the network should be more powerful. The network is in more difficult or time consuming to train because of the vastly larger number of unknowns that had to be found. When we come to the convolutional neural network, we will find that increased network depth is possible because we don't use all the connections between the nodes. By reducing the number of connections substantially, we can have more layers and still train the network. As a final comment on the operation of the layers in the neural network, this slide gives a different perspective on how the simple network of the first simpler example operates. Earlier, we regarded the hidden layer processing elements as implementing two decisions, with the third layer acting on those decisions as a logical O function. We could also view the hidden layer operation as in this slide if we examine the data as it appears at the output of the first layer processing elements. Now, represented by the variables J1 and J2, the data has been transformed into a linearly separable sit, which the output layer now handles. Again, this is a simple summary of what we've learned so far about neural networks. The second and third questions here will become important when we look at the convolutional neural network in the next series of lectures. What properties does it have to have in order that the back-propagation training algorithm can be made to work. 

### Module 2 Lecture 14: Deep learning and the convolutional neural network, part 1

Now start a series of four lectures on the transition of the neural network that we met in the past few lectures into the convolutional neural network that has become a cornerstone of Artificial Intelligence Research over the last few years. It has also been widely applied to remote-sensing problems, as we will see when we look at some examples. At the completion of this work, we will then do a detailed comparison of the major classification techniques we have covered in the course. As we will see by comparison to the classifiers, we have looked at so far the convolutional neural network does not have a standard form or topology instead it is composed of a number of building blocks that can be configured in many ways according to the approach chosen by a particular user. We will develop the tools and show several configurations helpful in remote sensing, especially for taking special neighborhoods or pixels into account when performing semantic mapping. The book by Goodfellow, and the others referenced here has become a bit of a standard treatment for Deep Learning and convolutional neural networks. It's true is at a higher mathematical level then we have adopted in these lectures but nevertheless, for those with the right background it should be consulted to fill in the gaps in the theory. Note, especially it's warning listed here about non standardization in convolutional neural networks. Before we embark upon the development of the convolutional neural network, it is important to reflect on the fact that for many decades, image analysts in remote sensing have been critically aware of the matter of a spatial context. That is, when considering the liable for a central pixel in the diagram on the slide we know for many scenes that there is a high likelihood that the surrounding pixels will be from the same class. That is especially the case for agricultural regions and many natural landscapes, and yet the classifiers we have been dealing with up to now have ignored that property. In that sense, they are simply called point or pixel specific classifiers because they focus just on a pixel independently of its neighbors. Over the use though, there have been many suggestions about how to treat spatial context, some of the more successful approaches are shown here. Echo was perhaps the earliest, developed in the mid 19 seventies, it works by growing homogeneous regions in an image. It then classifies that as objects or regions as a whole, it applies point classification methods for pixels that are not found to be part of an object, such as those on the boundaries between regions. In the late 1970s, the method of relaxation labeling was developed, it takes the results of a point classification at expressed as posterior probabilities of class membership such as in the output of a maximum likelihood classifier. Those posteriors are updated iteratively by reference to the posteriors on the neighbors, linked by a set of joint probabilities. Finally, measures of texture can be used to characterize the neighborhood about a pixel. Local texture is then used as another feature in a point classification method along with the spectral measurements of a pixel. As we will see soon, the convolutional neural network is another technique that embeds spatial context in its decisions. To employ the neural network spatial context sensitive applications, we have to use it in a slightly different way than we have up to now. Let's commence this discretion by recalling the topology we had been dealing with so far, in which the inputs are the individual components of the pixel vector. Suppose now though, we make the seemingly bold move of inputting all the pixels of an image in one go so that we have enough input nodes to accommodate the full set of spectral measurements for a full set of image pixels. For practical image, that will be a very large number of inputs, we still have a number of hidden layers and for the moment, the network is still fully connected, thus there will be a huge number of unknown white vectors and offsets to be learned through training. One immediately obvious problem with feeding the network in this manner is that the spatial interrelationships among the pixels appears to be lost. Even though this is really just a problem of how the pixels are addressed, it is more meaningful to arrange them as shown in the next slide. Suppose we present the image to the network as a square or rectangular array with the pixels in their correct special relationships, this doesn't change anything about the network other than arranging the nodes or processing elements into an array rather than a column format. For convenience, we have shown the hidden layers to be the same size and shapes as a input layer, but in general they could be any size. Note that the output layer is still one-dimensional since it represents a set of classes. With such an arrangement, the number of potential connections is enormous. Let's do a calculation of the number of unknowns between just the input and the first hidden layer. Remember that the input to each processing element in the hidden layer is z equals w transpose x plus Theta. The dimensionality of the weight vector will be equal to the number of elements in the input layer, which is N times N for an N-dimensional image. Also, there are as many weight vectors as there are nodes in the hidden layer. If we assume, for the sake of this calculation, that the hidden layer has the same dimensions as the input layer, that means altogether, we have to have N to the fourth different weights, values for which have to be found during training to make the network usable. In a similar fashion, there will be N squared values of Theta. If we had N equals 100, which would be a very small image in remote sensing, then there are more than 100 million unknowns, that would require an extraordinarily large amount of training data. Added to this, is the fact that we have multiple bands and images usually much larger than 100 by 100. Clearly, a simpler approach needs to be found, but one in which spatial inter-relationships among the pixels are still represented effectively. In this slide, we just simplify the diagram by removing the explicit input layer and just let it be represented by the image itself, perhaps with scaling, if that is found to be beneficial in some applications. In this simplified representation, each image pixel is connected to all the nodes of the first hidden layer. Also, we are still focusing on just a single band of data. We will come to multispectral images later. Here, we show the major deviation of the convolutional neural network from the fully connected neural network we have been considering so far. Instead of implementing all connections, that is as in a fully connected network, we are selective in the connections we make between the layers. In particular, we restrict the connections to a node in the hidden layer to be just those of a neighborhood of non-pixels from the input image as shown. Because of the geometry, the grip of three by three pixels is centered on the one which is in the second row and second column. The processing element in the hidden layer is also that in the two, two position, as seen in the slide. In contrast to the need to determine N to the fourth plus N squared weights and offsets overall, there are now 10 unknowns; that is nine weights and one offset. So 10 unknowns to determine per hidden layer node. Overall, therefore there are, in principle, 10 N squared unknowns defined, a considerable reduction but still a large number if N is large. We do the same thing for the 3 by 3 group, which is one column to the right. Now we take a decision that significantly reduces again, the number of unknowns to be found in training. Rather than using new set of weights and offsets, we assume we can employ the same set as for the previous slide. This is called weight reuse. While that sounds like it will reduce substantially the pair of the network to learn complicated spatial patterns in the image, it gives surprisingly good results in practice. There is also a rationale to this decision, which we will say soon. Continuing though, we then do the same thing for the next pixel group along the row, and then for all rows until the whole image is covered. While this example suggests that the actions happen sequentially, in fact all the operations are in parallel. They are just sets of connections. This is important to recognize. As we have realized, there is a problem with the edge pixels. Given the large numbers of pixels in an image, we could ignore the edge problem. But sometimes, an artificial border of zeros is created so that the edge processing elements in the hidden layer can receive inputs and thus preserve dimensionality if that is important. Even though many of the connections of a fully connected neural network have now been removed, it turns out we can still use backpropagation, surprisingly, to train this new sparser network. So thankfully, we do not need to develop a new training procedure. Let's summarize where we are at this stage. In looking at the neural network, the convolutional neural network, we are partly driven by the desire to take spatial context into account when labeling a pixel. Again, in the convolutional neural network, all the image is fed to the network in one go, but the numbers of node-to-node connections is greatly reduced and thus, so the number of unknown parameters to be found during training. The first question here asks you to think about the importance of spatial context. The last two questions are particularly important when thinking about the use of convolutional neural networks 

### Module 2 Lecture 15: Deep learning and the convolutional neural network, part 2

In this lecture, we take the development of the convolutional neural network further, still focusing just on a single band of data, but considering the evolution of its topology. The concept we adopted in the last lecture for the connections between layers is similar to the common process of convolution used to filter an image to detect spatial features. We haven't covered that material in this course, but it is moderately straightforward. In spatial convolution, a window called a kernel, is moved over an image row by row and column by column. A new brightness value is created for the pixel under the center of the kernel by taking the products of the pixel brightness values and the kernel entries, and then summing the result. That is exactly the same operation implemented by processing element in the hidden layer of the convolutional neural network just before the offset is added and the activation function is applied. It is because of that similarity that the partially connected neural network just described is called a convolutional neural network. However, in the convolutional neural network, the kernel is usually called a filter, and the set of input pixels covered bother filter is called a local receptive field. Note that any size filter and receptive field can be used. Even though we are exploring the smaller number of connections as a way of simplifying the network, and thus the number of unknowns that needs to be found during training, it is of interest to think a bit further about the practical significance of choosing a spatial neighborhood kernel of weights for that purpose. While important neural analysis of spatial context, this has particular relevance to picture processing and object recognition fields in which the convolutional neural network have been used extensively over the past five years or so. In spatial filtering, say for detecting the edges in an image, the kernel, or filter, entries are selected by the analyst for that purpose as seen in this very simple example. A three by three filter can be used to find the edges in an image. In the convolutional neural network, the kernel entries, that is the weights prior to the application of the activation function, are initially chosen randomly. However, by training, they take on values that match the image features that are characterized by the spatial nature of the training samples. If the training images strongly feature edges, it is expected that the weights will tend towards those of an edge detecting filter, for example. The strength of a convolutional neural network is that, with sufficient numbers of layers, it can learn the spatial characteristics of an image. That is why it is an important tool for performing context classification and for picture processing in general. We now introduce some more operations used in convolutional neural networks along with their associated nomenclature. The first is the concept of stride. When we looked at fading just nine outputs from one layer into a single processing element of the next layer, we did so with single pixel shifts along rows and down columns. Some authors choose to have larger shifts. The result of which is that the number of nodes in the next layer is reduced. The number of pixels shifts is what defines stride. This slide shows a stride of two. Another topological element often used is to add so-called pooling layers as seen on the right-hand side of this slide. This strengthens the dependence on neighborhood spatial information, and reduces further the number of parameters to be found through training, particularly when more than a single convolutional or hidden layer is used. Pooling is sometimes called down-sampling. We now have a decision as to how to proceed further and ultimately construct an output for the convolutional neural network. There are four common options. First, we can keep going by feeding the output of the pooling layer into another convolutional layer to provide a deeper network. We can, in principle, have as many layers as we wish. Just like with the fully connected network, we can have as many hidden layers as we like. Secondly, we could feed the output of the pooling layer into a set of output layer processing elements and thus, terminate the network. Thirdly, we've got to have the output of the pooling layer act as the inputs to normal, fully-connected neural network. In this case, the convolutional neural network acts as a feature selector for the fully-connected network. This is a common approach especially in remote sensing. Finally, we could have the output of the convolutional neural network generate a set of class probabilities. In the next slide, we are going to examine the last two options. Here, we show a network with two convolutional layers and one pooling layer feeding into a much smaller, fully connected neural network. At the top, we considered a couple of lectures ago. In effect, the convolutional neural network is acting as a feature selector for the fully-connected network. Note that we have introduced another term, flattening. That is just the process of straightening out the matrix into a vector as needed for the neural network input. Note also here, the last convolutional layer is not followed by a pooling layer. The output of a convolutional neural network can come from either layer type. After flattening, rather than feeding the results into a fully connected network, another very common option is to use a convolutional neural network outputs to generate a set of pseudo probabilities called softmax probabilities. They are defined in the slide. The convolutional neural network outputs are exponentiated and normalized as shown so that the set of softmax values replicate a set of posterior probabilities. Finally, the sigmoid activation function is usually replaced by a simpler activation function called the ReLU, the Rectified Linear Unit, which has the characteristic shown here. This choice speeds up training by improving the efficiency of the gradient descent operation used in back propagation. Note that the use of stride and pooling successively reduces the number of unknowns to be found by training. Also note that convolutional and pooling layers can be cascaded. The second question here leads to one of the design equations used with convolutional neural networks. 

### Module 2 Lecture 16: Deep learning and the convolutional neural network, part 3

In this lecture, we confront the problem of multidimensional images, color pictures made up of the three color primaries, and multispectral and hyperspectral images in remote sensing. In this slide, we see the three color primaries of a color picture. Alternatively, they could be three bands of a multispectral image. We describe the image pixels as shown by the three equations on the top of the slide. On the bottom, we share the corresponding three filter entries. In both cases, we have used three indices. The first refers to the individual band, while the others are the pixel position index. The simplest way to treat the three band image, is to carry out three separate convolutions as shown by the equations on the top of the slide. Generally, only a single offset, theta, is used. The three convolution calculations are added, to which the offset is also added, and then the activation function is applied. We now have three times the number of whites to learn by training. While this is the approach most often adopted for color pictures, we will see later, how multispectral and hyperspectral images are treated. Here we show another variation, often used in the convolutional neural network. Several convolutions can be performed in parallel in order to extract more spatial information from an image. As noted, the filters can be of the same or different sizes. Because of the complexity introduced by the various options we have discussed, it is difficult to come up with a standard form of diagram with which to represent the convolutional neural network. Most authors use their own forms of diagram. But the representation shown here is common to many and simple to understand. He we show convolutions in parallel, as just discussed on the previous slide. We also show several layers, each of which is composed of a convolution operation followed by pulling. Of course, the pulling operations are not essential, but are included here for completeness. Finally, we show the flattering operation often used at the output. As indicated, some authors even have crossed connections between the parallel paths. But that can defeat one of the benefits of the convolutional neural network by having several separate parallel paths. The network can be programmed to run on a multiple process and machine. We now come to an important practical consideration, similar to that we met with the maximum likelihood classifier, when considering the Hughes phenomenon. And that is the problem of over-fitting, which is illustrated on this slide. The concern arises because we have so many weights and offsets to be found through training. And the availability of training data determines how effectively those unknowns can be found. We must have sufficient training samples available to get reliable estimates of the unknown parameters. Otherwise the network will not generalize well. In other words, it will not perform well on previously unseen pixels. It is not sufficient to have a minimum of samples to estimate the unknowns, otherwise over-fitting will occur. This is illustrated in the example from curve fitting shown in the diagrams on the slide. Fitting a high order curve through just three points, will guarantee good fits for those points. But the behavior between the points can be way out in terms of being able to represent intervening points not used in generating the curve. If many training samples are used, then the function found interpolates or generalizes well, as indicated on the right-hand diagram. Clearly, we need many more training pixels than the minimum, to ensure we do not struck the same problem when training the neural network. Consider now the numerical complexity of analyzing hyperspectral image data. So we can make use of both spectral and spatial properties. Several approaches have been used in practice, as we will see shortly in some examples. One is to analyze the spectral information content alone. Another is to analyze the spatial information content alone, that is spatial context. Another is to do both together, but there is a processing challenge. We could treat the problem of processing hyperspectral data with a convolutional neural network by allocating one convolutional filter to each band, as we did previously for the three band color picture. But that requires about 200 times as many weights as for a single band image. For an image with 200 bands, and 3x3 kernels, the total number of unknowns, that is weights plus offsets, connecting the input image to the first convolutional layer is 2,000. Noting that the same weights are used in each filter right across a particular band. This, of course, gets multiplied upwards by the number of filters used in the convolutional layer. Often we take the path of reducing the spectral dimensionality of the hyperspectral image before applying the convolutional neural network. Although that partly defeats the purpose of using hyperspectral imagery in the first place, transforms such as the principle components transform, do allow us to concentrate the variance or information content in a small number of components. Three as shown here, but more might be necessary if we wish to retain, say, at least 95% of the image variance. If we want to analyze hyperspectral data for spectral properties alone, we can use the convolutional neural network defined the label for each pixel, based just upon its spectrum, and thus implicitly the correlations between bands. This, of course, ignores any benefit of spatial context. Here we summarize how multiband images can be handled right through to dats as complex as hyperspectral imagery. Importantly, the need to avoid over-fitting must be kept in mind at all times. The first question here asks you to propose a simple formula based on the discussion in this lecture on using principal components analysis. 

### Module 2 Lecture 17: CNN examples in remote sensing

We now present two examples that illustrate much of what we have discussed in these lectures and which help us to introduce some additional concepts. These examples illustrate how convolutional neural networks have been used to handle hyperspectral data based on Spatial properties alone and a combination or spectral and spatial properties. Our first example is taken from the paper indicated. It presents examples of hyperspectral classification using several data sets based on just the spectral properties of a pixel. Here we look at a classification of the Indian pines data set, which we saw before with the support vector classifier example. The data was recorded by the average hyperspectral sensor over a region in Indiana, USA. It consists of 220 spectral channels in the range of 0.4 to 2.45 micrometers. In treating this image, the authors chose to remove some difficult classes. They retained the class as shown in the table, which also indicates the numbers of training and testing pixels used. The authors chose to use a single layer convolutional neural network as a feature selector product classification by a fully connected neural network. They applied 20 Spectral Filters in parallel and used a fully connected network with a hidden layer of 100 nodes. Note how large their filters are. Altogether, there are 81,408 unknowns to be found from the training data. From the previous slide, there were 1600 trillion pixels at 220 channels per band. That gives 352 thousand training samples, which is sufficient. This slide shows the results in the form of a thematic map and also shows the accuracy achieved compared with that generated with a support vector classifier. The accompanying ground truth map, that is the map of correct labels, allows one to assess how good the final thematic map is. Of note though, is the speckled appearance of some classes indicating that the convolutional neural network misclassified and number of pixels. Hadn't incorporated spatial filtering to, we would expect to see a much cleaner thematic map. The second example we will consider uses a two channel convolutional neural network to account for both spectral and spatial properties of hyperspectral scenes. One channel is diverted to spectral properties alone and functions very much as in the previous example. The other channel handles the spatial analysis. Both channels develop feature subsets that are then concatenated and analyzed by a fully connected neural network. The example is taken from the paper indicated in the slide. We are going to concentrate on this Salinas, California image exercise. The image segment consists of 512 by 279 pixels with 3.7 meter spatial resolution. It has 224 recorded bands, but the authors reduced those to 200 by removing channels with poor quality. The ground truth image shows that there are 16 classes with the numbers of pixels indicated. The authors chose to train the network using different percentages of ground truth pixels. We show the results here for the training data being 25 percent of the total labeled ground truth pixels. They used all the available ground truth pixels to test the generalization of the network, that is the classification performance. Here we see the convolutional neural network topology or architecture used by the authors. It consists of a spectral path at the top and the spatial path or channel at the bottom. Notice that the spatial path has 30 filters of size three by three and the spectral Path has 20 filters of size 20 by one. Each pathway has one convolutional layer and one pooling layer. The input to the spatial path consists of a spatial neighborhood about the pixel currently under consideration during training or in classification. The outputs from the two paths are flattened, concatenated and they'd fade into a fully connected neural network with two hidden layers, each with 400 nodes thus the two path convolutional neural network is acting as a feature selector for the fully connected neural network. Note that the upper layer has 16 nodes representing the 16 classes in the Salinas image. The outputs are in the form of class conditional probabilities computed with the soft max function. There are two important aspects of this example which needs to be emphasized. The spatial layer is required to capture the neighborhood or spatial properties of the pixel. The neighborhood patch of 21 by 21 pixels centered on the pixel of interest is used. The neighborhood patches created by averaging over all the spectral channels in that neighborhood. The authors also used transfer learning. That is a technique based on the concept that networks previously trained on different images but with the same sensor will most likely perform acceptably on the image of interest. This is based on the assumption that the spatial properties are similar from image to image. The authors trained the convolutional neural network layers on a different average image and then use the weights so found to initialize the convolutional neural network weights for trading on the Salinas, saying. This is not necessary in general but is a common approach based on the concept that we, as humans adapt our learning from past experience. The results shown here indicate the benefit of both spectral and spatial context, achieving an overall accuracy of 98.3 percent which is indeed very good. It is important to note that the authors runs extensive trials to find the best topology for the network. That is the numbers of convolution layers and numbers of filters, the numbers of nodes in the hidden layers and so on. Which indicates that the preparatory stages in using a convolutional neural network can be quite extensive. The idea of using neighborhood patches seems first to have been introduced by a contests in the paper referenced in this slide. Those authors used far by far patches that maintain the full spectral dimension for the patches so that spectral information, as well as spatial neighborhood of a pixel was carried by the patch. However, in order to constrain the overall data volume of the input, the authors carried out a dimensionality reduction first using a principal components transformation. This slide gives information on where convolutional neural network software can be found. Two important points are emphasized in the summary. First, patches or neighborhoods can be fed into a convolutional neural network to carry spatial context into classification. Secondly, transfer learning can be an effective and efficient way to initialize our convolutional neural network and even a fully connected neural network. These questions ask you to think carefully about some of the quantitative aspects of using a convolutional neural network. 

### Module 2 Lecture 18: Comparing the classsifiers

Having now completed our examination of the most popular classifiers used in remote sensing. It is now benefit to compare them both in terms of performance and in relation to the user if it required. We want to compare the attributes, so we know where their relative strengths and weaknesses lie. And so that we always choose the most appropriate method for the task at hand. Of the range of algorithms we have looked at, the following three are representative set for comparison purpose and other ones we have spent most time on. They are the maximum likelihood classifier, the support vector machine and convolutional neural networks. Let's commence by summarizing the maximum likelihood algorithm. Recall that the decision rule for allocating a pixel to a class is expressed in terms of discriminate functions. Each class is defined by its mean vector and covariance matrix, and if available, the class prior probability. And is represented also by those properties in the discriminant function. Here we summarized the attributes of the maximum likelihood classifier. The support vector classifier, finds a separating hyperplane that maximizes the margin between two classes of data. While minimizing the error caused by pixels that fall on the wrong side of the hyperplane. It uses kernels in place of dot products effectively to project the data into a higher order space, so that data which is not linearly separable can be handled. The attributes of the Support Vector Machine is summarized here. That in particularly that it has to be used in a decision tree to make it capable of handling multiple classes. The Convolutional Neural Network is a modern variant of the original multilayer perceptrons. And consists of a number of layers, each usually involving sets of filters that perform convolution, activation and pooling. Those layers can then be followed by fully connected neural network and or a softmax operation. And this slide summarizes the attributes of the convolutional neural network. In this table, we bring the most important attributes together so that the three principle algorithms can be compared. In summary, the maximum likelihood classifier is much simpler to construct in trying. But is limited when presented with data of high spectral dimensionality. By comparison, the support vector machine and the convolutional neural network are more challenging to configure and train. But the support vector machine is good for handling data of high dimensionality. It does however require a decision tree framework to handle more than two classes. The convolutional neural network naturally handles special context and, like the maximum likelihood classifier, is a multiclass algorithm. It can also handle data of high dimensionality. Here we summarize what the user needs to look for in selecting an algorithm for thematic mapping in remote sensing. Again, these test questions draw attention to the types of application the analyst may have to handle leading to a choice of the most appropriate classifier algorithm. 

### Module 2 Lecture 19: Unsupervised classification and clustering

We now turn to the topic of Unsupervised Classification, in which we are still interested in thematic mapping. But without the benefit of having labeled training data available beforehand, we won't develop the topic in this series of lectures based on the procedure called clustering. Indeed, most of what we will talk about concerns clustering algorithms. But we will present some unsupervised clustering examples later in the lectures. We will make unsupervised clustering or unsupervised classification, again, in Module 3 of the course. We start by being confronted with the situation in which now obvious training data is available. Yet, we still want to do thematic mapping of remote sensing image data. Cluster Analysis forms the backbone of what we are going to do. Clustering looks for groups of similar pixels assessed on the basis of this spectral properties. The groups we're searching for are in fact clusters in spectral space. We can often identify the pixel labels produced by clustering through the use of spatial clues in the image and by using the cluster means as surrogates for spectral reflectance information. We will see that in the examples to follow. Reference data like maps and air photos also give us hints as to what the class is in a cluster map might represent. Here we look at unsupervised classification as a two-stage process in which clustering takes us from the spectral domain to a map of symbolic labels. The challenge for the analyst is to turn those symbols into meaningful ground cover labels. We get the cluster map in the same way we get a thematic map in supervised classification. Here the clustering algorithms place pixels into clusters based on spectral similarity. We then assign symbols to the clusters and use those symbols in place of the pixel itself in the image that's producing a cluster map. By examining those pixels and their spatial layout, the analyst turns the symbols into ground cover class labels. Clustering algorithms place pixels into clusters based on their similarity, as we said before. As we indicated in the previous slide, the most common measure of similarity is based on the spectral measurements of the pixels. Two pixels with very similar measurement vectors are likely to belong to the same class and thus the same cluster. The simplest way of assessing similarity is to use a metric which measures the spectral distance between pixels. The most common of which is the Euclidean distance between the pixels in spectral space. For in-band data, Euclidean distance matrix is d as a function of X1 and X2, which is given as the norm of the difference between the two vectors X1 and X2 and which in fact, in the last expression is the square root of the sum of the squares that we are most familiar with. There are other distance metrics in use, including the more efficient but perhaps less accurate city-block distance, defined as D, X1 and X2 being the sum just of the absolute differences between each of the elements of the pixel vectors. It is similar to walking between two locations in the city where the streets are laid out on a rectangular grid. It is sometimes called the Manhattan distance for that reason. But in these lectures, we will concentrate on Euclidean distance. The set of clusters for a given image is not unique, even though we have a metric for spectral similarity. Here we see two plausible clusterings of eight pixel vectors in a two-dimensional spectrum space. Which one is correct? To help assess which set of clusters best represents the pixels in an image, we need a quality of clustering criterion. A common one at which is the sum of squared error measure or SSE, with the formula shown here. The SSE checks the distances of all the pixels in a given cluster from the cluster mean and then sums those distances within that cluster. It does so for all clusters and then sums the results. In other words, it is an accumulative measure of distances of the pixel vectors from the cluster means. The smaller it is, the better. Since then the clusters are compact. Other quality of clustering measures are possible. Some look at the average compactness of clusters compared with the average distances among them. In principle, we should be able to develop a clustering algorithm by minimizing the SSE for a given data set. But that turns out to be impractical since it would require examining an enormous number of candidate clusterings of the available data to find that which has the smallest SSE. In practice, some heuristic methods have been developed that work well, two of which we will look at here. The first is called the k-means or migrating means algorithm. It is perhaps the most commonly used, particularly for remote sensing problems. It asks the user first to specify beforehand how many clusters to search for, and secondly, to specify a set of initial cluster mean vectors. Clusters are located in spectral space by their mean vectors. The algorithm starts by the user guessing the set of cluster centers, initially. The image pixels are then assigned to the cluster of the closest mean, after which the set of means is re-computed. The pixels are then assigned to the nearest of the new set of means and so on until the means and the assignments do not change. This slide shows algorithmic-ally the steps in the k-means approach. In the first two steps, clustering is initiated by specifying a starting set of cluster mean vectors, both in number and position. In Step 3, all the available pixel vectors are then assigned to the cluster of the nearest mean. The mean vectors are then re-computed in Step 4. Then a new assignment of pixel vectors is carried out based on the re-computed means. The means are then re-computed again. During this process, pixels will often move between clusters, iteration by iteration, because of the change in positions of the means. Ultimately, we expect that the stage will be reached where the pixels do not migrate any further between the clusters, say that the situation is stable and we conclude that the correct set of clusters has been identified. That is checked by seeing whether the full set of mean vectors no longer changes between iterations. In practice, we may not be able to wait until full convergence has been achieved and instead, we stopped clustering when a given high percentage of the pixel vectors no longer shifts between cluster centers with further iteration. Before we implement the K-Means technique in the next lecture, we hear text stock of what we are trying to achieve. Our ultimate goal in many remote sensing situations is unsupervised classification, which we pursued through the application of clustering techniques. Even though we haven't yet seen an example of the application of the k-means approach, we can still think about some practical matters concerning its operation and especially how we choose the initial cluster centers. That choice will affect the speed of convergence of the algorithm and the actual cluster set found. 

### Module 2 Lecture 20: Examples of k means clustering

In this lecture, we look at two examples of K-means clustering. The first uses a small dataset to show how the algorithm operates. The second is a remote sensing example to share what's operation in unsupervised classification. We now implement the K-means algorithm using the two-dimensional dataset shown here. We will see this dataset a couple of times in these lectures. Looking at the pixel locations in this data, it seems there are two or possibly three clusters. In practice, that might not be so obvious, so a guideline is needed in terms of the initial choice of how many clusters to find. Because we can merge clusters later, it is good to estimate on the high side, recognizing however, that the more clusters there are, the longer the algorithm is likely to take to converge. A guideline which has been used in remote sensing, is to estimate the number of information or ground cover classes in a scene and then search for about 2-3 times that number of clusters. The choice of the initial clusterizations is important, because it can influence speed of convergence and the ultimate set of clusters. It is important that the initial set be spread well across the data. Several guidelines are available. One is that the initial cluster centers be spaced uniformly along the multi-dimensional diagonal of the spectral space. That is a line from the origin to the point of maximum brightness on each axis. Better still, we could choose the multi-dimensional diagonal that joins the actual spectral extremities of the data, that requires a bit of [inaudible] processing to find the lower and upper spectral limits in each band, but that is quite straightforward. Another approach is to space the initial cluster centers uniformly along the first principal component of the data. That will work well with a highly correlated data sets, but might be less useful if the datasets show little correlation. Diagrammatically, this slide shows the evolution of the K-means method, iteration by iteration on a small data set. Here we're searching for two clusters. The bottom row shows four iterative assignments of the pixels to the clusters along with the corresponding SSA values. After the fourth iteration or assignment, there is no further migration of the pixels between the clusters, that incidentally those pixels which changed clusters in the first two steps. The top right-hand diagram shows how the means migrate with iteration. That tells us why the algorithm is sometimes called the method of migrating means. The ISODATA algorithm is a variation of the simple k-means approach. It adds two or three further possible steps as outlined in this slide. First, we can check to see whether any clusters contains so few pixels as to be meaningless. If the statistics of the clusters are important, say for use in a later maximum likelihood classification, then poor estimates will be obtained if the clusters do not contain a sufficient number of members. Secondly, we could see whether any pairs of clusters are so close that they should be merged. In Module 3, we will look at similarity measures for use in classification. They will give an indication of whether classes and clusters are too similar spectrally as to be useful and therefore should be merged. Thirdly, we can check whether some clusters are so elongated in some spectral dimensions that it would be sensible to split them. Elongated clusters are not necessarily a problem, but if they are, then comparison of the standard deviations of the clusters along each spectral dimension will help reveal the elongated nature. We now look at the application of clustering to unsupervised classification using a five channel dataset of an image recorded by the HyMap sensor near the city of Perth in Western Australia in January 2010. The table shows where the five bands are located in the spectrum. From our knowledge of spectral reflectance characteristics, that choice seem sensible in being able to differentiate among the cover tops we most expect to see. The image display uses just three of the bands selected to give the standard color infrared product in which vegetation is emphasized in rate. A remote sensing image analysis package called multi-spectral from Purdue University, was used for this exercise. Six clusters were specified with no provisions to include taken close or elongated clusters. However, if any cluster was found to have fewer than 125 pixels, it would have been eliminated. None were found in this exercise. Once clustering was complete, the unsupervised cluster map shown on the right-hand side of this slide was produced. The clusters represented by different colors by the visual patterns of the classes in the image. It is easy to associate the brown and orange clusters with highways, road pavements, and bare regions, the yellows with buildings, and the shades of green with various types of vegetation. Clearly the dark blue cluster is water. We strengthen this interpretation further in the next slide. In the table here, we see the mean vectors of the final set of clusters, which with the spatial clues in the map itself, allow us to associate the cluster colors with ground cover classes as indicated in the K to the cluster map, which of course is now a thematic map. Here we plot the cluster means by wavelength and see that they follow the spectral reflectance curves of the ground cover class labels assigned to the clusters. This is further information that has been used to identify the clusters. Insofar as it is possible, it is always good to look at unsorted representations such as this to help understand the identity of the clusters that had been found by the algorithm. It is instructive to see where the cluster centers lie in spectral space. While we can't envisage the third-dimensional space, we can look at two-dimensional scatter plots using two of the band's most significant to vegetation, they need infrared and the visible red bands as seen here. The final cluster centers represent well the scatter of the pixel vectors. Here we summarize the essential elements of the ISODATA algorithm and how clustering is used for unsupervised classification. The first question, here should allow you to develop a feeling for the importance of the placement of the initial cluster centers. 

### Module 2 Lecture 21: Other clustering methods

In this lecture, we will summarize other methods for clustering and illustrate one of them, so that we can compare its performance with that of the k-means algorithm. Although the k-means algorithm is one of the most widely used methods for clustering, there are other approaches that have been used with remote sensing data. In the next lecture, we will explore a recent clustering method that has been applied to hop spectral data and to big data sets. But here we will look at another long-standing technique so that we can see its performance relative to the k-means algorithm. By doing so, we will demonstrate that the results of clustering are not unique, affect that the use and needs to be aware of and handle carefully when undertaking unsupervised classification. As noted in the slide, other methods that have been used in remote sensing. First, hierarchical clustering, which when applied to the first example of the last lecture, tends to lead to three and not two clusters. It has been used as a basis for clustering in some big data applications and for details of this algorithm, please see my book. Secondly, histogram picks selection, math and climbing or density maximum selection is another technique used for clustering in remote sensing. Thirdly, we have the single pass clustering algorithm, which is the method we are now going to examine. The single pass method is an old technique and had its origins when remote sensing imagery or supplied on sequentially accessible storage media like magnetic type, which with iteration would be a particularly time consuming process because of the need to read and re-watch the type. Despite its formation, it is still sometimes used because of its simplicity and its speed. It starts by assuming that the data is arranged in the usual row and column format. If the image is very large, a random sample is taken with the pixels and the results arranged again by row and column. The algorithm proceeds in the following manner. The first row is used to obtain an initial set of cluster centers in the following way. The first sample is used as the center of the first cluster. If the second sample is further away from the first by more than a user specified critical distance, then it is used to start a second cluster. Otherwise, the two samples are assumed to be from the same cluster, in which case they emerged and their mean computed. This process is applied to all the samples or pixels in the first row. At the end of the first row, the multispectral standard deviations of the clusters we generated, are produced for use in the light of rows. Each sample in the second and subsequent rows is checked to see if it lies within a user specified number of standard deviations of one of the clusters from the first row. If it does, it is added to that cluster and the cluster statistics are recalculated. Otherwise it is used to start a new cluster and allocated a normal standard deviation in each band. In this slide, we show the single-pass method diagrammatically. The left-hand diagram shows how the first four samples are traded in this particular illustration. Only samples 2 and 3 are close enough to be merged. Clearly sample 2, was too far away from sample 1 and was used to start a new cluster. Also, sample 4, being too far away from the two existing clusters is used to start another cluster. The right-hand diagram shows how sample n plus 1 falls within the prescribed number of standard deviations of cluster 2 and becomes part of that cluster. Whereas sample n was too far away and is used to initiate another separate cluster. The single-pass method is fast and does not require the number of clusters to be pre-specified. It does, however, require the user to specify two parameters, the critical distance used with the first line of samples and the standard deviation multiply used in the remaining lines. Also, since it initiates clustering on the first line of samples, it can be biased by the samples in that line. There is no way to moderate that choice. Variations on a single pass algorithm exist, some let the users specify the actual initial cluster centers, while others use a critical distance measure for all rows. The multi-state package where they're going to use operates that way. We're now going to apply the single pass algorithm to the dataset we traded in the last lecture with the k-means method. The MultiSpec package was again used for this. It doesn't use the standard deviation method for the second and subsequent lines but applies another critical distance. The critical distance is used here, were 2,500 and 2,800 respectively for the first and subsequent lines of data. These numbers seem large, but remember, this sensor has 16 bit radiometric resolution. As noted in the previous slide, the algorithm uses a first line of pixels or samples if the image is large to initiate the cluster centers. In this case, the first line is actually the right-hand column of pixels in the displayed image, say in the next slide. Because after clustering the image and custom apps were rotated 90 degrees clockwise to bring them into a North South orientation. Here we see the results of the application of the single passed method to the image we analyzed earlier. As with the k-means algorithm, we can say that the clusters represented by different colors follow the visual patents of the classes in the image. The colors here are different from before, and this time they were just confusion between roughage and trays. In this slide, we say the cluster center is created by the single pass algorithm. How do they compare with the clusters generated by the k-means method? Well, let's say we compare the results of the two algorithms using bi-spectral plots as shown here. That the sparse vegetation, water, and building classes are about the same for both algorithms. Whereas the two approaches have picked up different combinations of bare surfaces, roads, and trays. In practice, clustering may need to be refined by re-running the algorithm with different sets of parameters until a cluster set is obtained that matches the information classes of interests. Here we summarize the essential elements of the single pass algorithm and the fact that unique results are unlikely to occur. The third question here is particularly important. Often in remote sensing, we have the notion that the pixels tend to clump into groups that align well with groundcover classes. That is often not the case. Instead, the spectral domain can look like a continuum with a few density maxima associated with definite classes like water. We will have more to say about that in Module 3. 

### Module 2 Lecture 22: Clustering "big data"

In this lecture, we look at how to do clustering with very big data sets, including hyperspectral images. We are now in the era of big data, particularly with the recording and storage of many high-volume image datasets. In 2014, NASA was managing more than nine petabytes of data, with about 6.4 terabytes a day being added. This accounts for an unbelievably large number of images. How do clustering techniques cope with such large amounts of data? If we want to apply clustering techniques to large images for unsupervised classification, or use clustering to extract information from archived datasets, a process called data mining, then the methods for clustering we developed in the last couple of lectures are limited in value. In this lecture, we will look at a recent clustering approach that is suitable for big datasets. There are others as well, but the one we look at here illustrates the types of method now being explored for use on so-called big data. Just before doing that, consider the time demand of the k-means algorithm. For P pixels, C clusters and I iterations, the k-means algorithm requires PCI distance calculations. For N bands, the distance calculations involve N multiplications each, giving a total of PCIN multiplication to complete a k-means clustering exercise. For 1000 by 1000 pixel image segment, involving 200 bands and searching for 15 clusters, then if 100 iterations were required, 30 by 10 to the tenth multiplications are needed. How can we devise an approach to clustering that is much faster, and is able to cope effectively with large images? But in searching for an improved technique, the k-means technique should not be abandoned. Its simplicity means it is still used with big datasets. But to make it more suitable, there are several alternatives that should be examined. First, the simplest is to use a more powerful computer. But from an operational point of view, it is important to note that most remote sensing practitioners would want to use readily available and not specialized computer hardware. Secondly, a better method for initiating the cluster centers might be found that helps speed up convergence by reducing the number of iterations needed. Several methods have been suggested for that in the big data context. Thirdly, a multiprocessor or multicore machine could be used to speed up the computation by taking advantage of parallel calculations. However, steps need to be taken to parallelize the k-means algorithm, which because of its iterative nature, requires some innovative modifications. Finally, a more efficient version of the k-means algorithm might be possible. We will examine one such technique here. It speeds up significantly the time required to undertake clustering and to allocate a pixel to a cluster class. In a sense, it is a particular case of the third dot point. The method we will look at for fast clustering is called k-trees. We met decision trees in the context of the support vector machine. But now, we want to look at them more generally. We start with some nomenclature. Trees consists of nodes, linked by branches. The uppermost node is called the root, and the lower most nodes are called leaf nodes. In between, there are internal nodes. The nodes are arranged in layers as shown. Progression of a pixel down the tree is based on decisions at the nodes. Those decisions direct the pixel into one of the available branches. In the k-trees algorithm, we allocate leaf nodes to the clusters that we are trying to find. Both in number and position in the spectral space. Although we don't know how many there will be beforehand. Some authors use the leaf nodes to represent the individual pixels within the clusters. With the clusters themselves being the internal nodes in the layer directly above. That does not help in developing the algorithm and just adds an additional unnecessary complication. The k-trees algorithm has one parameter that the user has to specify beforehand. It is called the tree order. Which specifies the maximum number of pixels in a cluster and as we will see in the following slides, the maximum population of any node in the tree. Full details of the K-trees algorithm will be found in this paper by Geva. It is a little hard to understand in the remote sensing contexts since it is written in the language of computer science. We will develop the algorithm for example, using a simple two-dimensional set of data, and using remote sensing terminology. We will use the set of eight vector samples shown here in vector and diagram form, and choose a tree order of three. Specification of the order controls the structure of the tree, as we will see. The tree starts with a single root node and a single leaf node. We then feed in the first sample, say sample C. Feeding in is also called insertion. Since we have no other information, the sample simply flows down to the leaf node, as does the second sample A, shown on the right-hand side of the slide. We will use black letters to indicate samples of current interest and red letters to indicate samples which have already been fed into the tree. A third sample, say G, can be accommodated, but it fills the leaf node since we have specified a tree order of three. A fourth sample, say D, cannot be accommodated in the current tree because the leaf node cannot contain more than three samples by design. That leaf node has to be split. The K-trees algorithm does the split by doing a K-means clustering of the four samples as on the next slide. The K-means clustering in the K-trees approach always looks for two classes, so that the over full leaf node is split into two new leaf nodes. In this example, the vectors A, B, C, and G have to be allocated to two clusters. We show that process in the slide. Because of the initial choice of cluster centers, the algorithm converges in one iteration for this very simple example. When complete, the two clusters have the main vectors indicated on the slide. The main vectors from the clustering step of the previous slide become the attributes or members of the root node, and the internal leaf is split into the two clusters, as shown in the left-hand diagram. The tree now has capacity to absorb more pixels, so our 5th sample F, can be inserted as shown on the right-hand side of the slide. That pixel is checked against the two main vectors in the root node, and it seemed to be closest to M-C-D, so it is allocated to the left-hand leaf node. As in the previous slide, when pixel B is inserted, it is checked against the main vectors held in the root node and found to be closer to M-A-G, as a result of which it is allocated to the right-hand leaf node. The main A-B-G is calculated and used in place of M-A-G in the root node. But when we try to insert the 7th sample H, the capacity of the left-hand leaf node is exceeded and that node has to be split again, using the K-means algorithm. We are looking to separate the pixels C, D, F, and H into two clusters using the K-means approach. Rather than go through the exercise, we assume for simplicity that the solution shown in the diagram here has been found. The means of the new clusters are shown. This leads to the tree now having three leaf nodes, as seen on the next slide. The root node now contains three main vectors and has reached its capacity. If any further leaf nodes are added, then the root node will have to be split. Consider the insertion of the final pattern E to the tree. When entering the root node, it was seen to be closest to the M-A-B-G mean, so it should be placed in the bottom right-hand leaf as indicated. But since that exceeds that leaf's capacity, the leaf has to be broken into two separate leaves using K-means clustering as on the next slide. For simplicity, we assume that the K-means algorithm has found the clusters illustrated here in the diagram to the left with the main vectors indicated. The tree now has four leaf nodes as shown on the next slide. But the root node now needs to be split into two because its capacity has been exceeded. The two new nodes resulting from the split will be internal nodes, and a new root node is created. We split the root node using the k means approach, that the elements now to be clustered are the main vectors stored in the root node. Again, assume the results of the k means clustering are shown here. The new means mcdfh and mabeg are now computed. Since all pixels have now been fit into the tree, we have its final version as seen here. It has three layers with two internal nodes and four leaf nodes. Any pixel vector fit into the top of the trail will make its way down to one of the clusters via the decisions. That is distance comparisons at the root and internal nodes. For example, the vector 33 will fly through as shown by the dotted green line into the cluster, say H. Apart from [inaudible] there cluster. There are two things we want to know about clustering algorithms. First, how long does it take to build the tree? Secondly, especially with unsupervised classification mind, how quick is it at allocating unseen data to a cluster? If we look at the speed of allocation first, we can do so by counting the number of distance comparisons. In the simple case here, that the k trees and the equivalent k means approach require the same number of comparisons. But what about with bigger data sets? If we take the simplest case of H node in a k tree requiring two distance comparisons, the number of comparisons increases by two for each newly added. Which in this case also doubles the number of clusters. By contrast, the number of distance comparisons for the k means the algorithm goes up as pairs of two say for, large numbers of clusters. K trees algorithm is much faster when allocating an unseen sample to an existing cluster. Getting a meaningful comparison of the times to build the k tree, and the k means approach is not straight forward. We can make comments on the numbers of nodes to be built and the checks within them. But the complexity introduced by the effect of different tree orders makes meaningful theoretical comparisons difficult. We will use an example instead, as in the next slide. This example is taken from the paper cited here. Also a by given. The data to be clustered, consisted of vector samples from a three dimensional normal distribution with zero mean and unity variance. A range of sample sizes was used, starting at 1,000 and progressing to 256,000 by successive doublings. The k tree order was chosen as m equals 50. For comparison, a k means clustering was run on the same set of samples. It was initiated with the same number of clusters as found by the k trees approached on the same data set. Ten sets of a tests were done, with the results averaged. Here we see the comparison, which is quite compelling for a given number of samples to be clustered. The k trees approach is much faster than the k means algorithm as seen. As always, there is a trade off. Geva found that the k trees clustering was not quite as accurate as the k means approach. Given that k-means is an iterative procedure which involves all pixels at all stages while k trees is a single pass per sample and segments that are dispatched during learning by its branching structure, that is not surprising. But in general, Geva found the difference not to be a problem in most practical situations, especially given the spade benefit of k trees. Another significant factor in favor of the k trees approach is that it can be adapted to run a multi-core processes and not to require all samples to be holding core memory during clustering. Those additional developments are new and contained in the paper by Woodley and others sought at in this slide. We are now at the end of our lectures on clustering. It would be good to compare this summary with those of the previous three lectures in order to reinforce overall the important aspects of clustering, especially when used as a tool for unsupervised classification. The first two questions here are important to the development of classification methodologies, which we will pursue in module three. The last question, hearts a particular benefit of clustering with the k trees approach. 

## Module 3

### Welcome to Module 3

We now come to the third and final part of our course. Our last module is in two parts. The first covers some practical matters we need to be aware of when analyzing remote sensing images with classification techniques. One relates to how we cope with the number of recorded bands, especially when that number is large. We also look at the very important topic of how to assess errors in the thematic maps generated by a classification exercise. We then look at methodologies for the application of classification methods to generate thematic maps, noting particularly the benefits of mixing techniques such as unsupervised and supervised learning. The second part of the module is a coverage of image in radar as a remote sensing tool. Radar is one of the key imaging technologies along with visible and infrared imaging that is most commonly encountered. That material introduces some of the innovative aspects of radar, such as its ability to produce topographic maps, monitor changes in topography with time, and the prospect of resolving detail within vertical features such as forests. The module and indeed the whole course concludes with a brief look at what is possible in landscape analysis if different imaging modalities such as radar, optical, and thermal imaging are used together. So at the end of this module, you should understand the need for feature reduction, feature selection as one feature reduction tool. How we go about hyperspectral analysis by library searching the very important topic of classifier performance and map accuracy. How we sample format accuracy assessment, classification methodologies, the fundamentals of imaging radar, then radar interferometry and radar tomography, finalizing with the concept of information fusion. 

### Module 3 Lecture 1: Feature reduction

We now come to two practical aspects of machine learning in remote sensing. The first looks at the features we use in performing a classification, while the second addresses the very important topic of how to assess the accuracy of a thematic map. As part of this work, we will look at methodologies for use in the various machine learning techniques we developed in module two. But we start now with looking at the topic called feature reduction. Feature reduction techniques allow us to limit the number of features to be used in a classification. Most often, those features will be the spectral recordings or bands from the remote sensing instrument. But sometimes, there will be special features, particularly if the image data has high spatial resolution. We will not look at methods for reducing the number of special features, noting that convolutional neural networks do that well. Instead, here, we will develop techniques for reducing the number of spectral features. Why would we want to limit the number of features or bands? The answer is simple, it is because the number of training samples required per class to train a machine learning algorithm is lower for fewer features. With more features, more samples per class are required for accurate training. That is a major consideration for hyperspectral image data sets. Feature reduction is carried out to avoid the so called curse of dimensionality, or the Hughes phenomenon, when we train a classifier. There is a second consideration, with more features, training and classification times increase, in some cases more than linearly, so that classification cost also increases. Reducing features therefore reduces costs, provided the cost of the feature reduction step does not outweigh the savings gained in classification. This raises an intriguing question. If we have to go to all the trouble of reducing the feature subset in order to develop a reliable classifier, why not just record a small number of bands in the first place? To answer that question, we need to examine the rationale behind hyperspectral imaging. Hyperspectral imaging is often referred to as imaging spectroscopy. Indeed, that was its original name. As you probably know from your studies in science, spectroscopy means being able to identify something by reason of its spectral characteristics. Examples with which you might be familiar include mass spectroscopy, visible spectroscopy, and electron spectroscopy. Sometimes a spectrum results from the absorption of radiation, and sometimes it is a result of reflection or emission. Imaging spectroscopy is a development of the familiar field of visible absorption spectroscopy at the pixel level. For each pixel, we record the full reflectance spectrum of the substance using reflected sunlight, usually over the wavelength range of about 0.4 to 2.5 micrometers. We measure the sunlight reflected to the sensor on a remote sensing platform after a component of the sunlight has been absorbed by the surface material. The reason our field is called imaging spectroscopy is because such a complete reflectance spectrum is recorded for every pixel in an image. This slide summarizes the essential nature of imaging spectroscopy. On the left, we see an image which has been recorded in a very large number of wave bands, typically 200 or so. For a particular pixel, those bands represent samples of the reflectance spectrum of the corresponding region on the ground. Or more correctly, the reflectance spectrum of the surface material corresponding to the pixel. From those samples we can reconstruct the reflectance spectrum of the pixel as shown on the right hand side of the slide. Notice that the reconstructed spectrum, particularly if there are sufficient samples, shows various important diagnostic features such as the dip corresponding to chlorophyll absorption in the red region and the water absorption bands in the mid infrared. There are other finer absorption features too, especially for soils and minerals. They do not show up on this vegetation example. One of the great benefits of recording the full reflectance spectrum for a pixel is that we can use Scientific knowledge to identify the pixel rather than depend upon supervised learning and machine classification. Expert spectroscopists can undertake that analysis from their knowledge of the spectral response properties of materials. Well, that is a viable approach, most often in hyperspectral remote sensing. The recorded reflectance spectrum of the pixel is compared with the library or prerecorded spectra, as in the next slide. This slide illustrates the approach to image interpretation based on library searching. The image captured by the sensor consists of pixels for each of which are full reflectance spectrum has been recorded. That spectrum is in compared against the library of spectra previously recorded in the laboratory, allowing a label to be attached to the pixel. Often that can be facilitated by using expert system techniques in which the knowledge of an expert analyst is encoded in sets of rules that are applied to the data. Although spectroscopic analysis is regularly applied in the earth sciences using recorded hyperspectral image data in many applications, we still wish to use machine learning methods for labeling a pixel. This approach is convenient and does not rely on expert knowledge or recorded spectral libraries. But we then have to face the problem that there are too many bands recorded to allow all of them to be used as features in a supervised classification exercise. That is, be cause it is too difficult to obtain enough training data per class to allow our classifier parameters to be estimated reliably. We now therefore need to find methods that will allow us to identify a subset of the recorder bands that are still sufficient for the development of an accurate classifier. Before proceeding, there are some dimming clutcher that we should keep in mind. First features is the name given to the input measurements to our various classification algorithms. Most usually they're just the recorded bands or some transformed version of the bands after, say, the application of the principle components transform. In object detection, there may be special descriptors. In some applications, the feature set may include by spectral and spatial descriptors. We are now embarking on a search for methods to reduce the number of features to use, not unreasonably called feature reduction in general. One approach to feature reduction is to select subsets which still function effectively that is called feature selection. Feature selection is the most common form of feature reduction, but other approaches also exist, as we will see in the following material. Feature reduction has been a focus in the machine learning world ever since the first algorithms were developed. Many techniques have been proposed over the years in remote sensing, especially since hyperspectral image there they became available. In these lectures were going to look at some of the more common feature reduction techniques. For those of you who wish to take this study further, the paper referenced on the slide is a comprehensive overview of the field. Essentially, we are going to look at three different approaches. The first will depend upon an analysis of the correlations among the recorded bands of data. We will see that those correlations allow us to consider groups of bands and will let us ignore correlations from group to group. The second approach we will look at involves transforming the spectral data in such a way that we can ignore the least significant transformed bands. The third approach we will look at involves measures that got us in discarding some of the original bands of data without significantly compromising the quality of classification. These techniques fall into two groups. One assumes that the classes can be described by probability distributions, while the other does not require any such assumption. Let's take stock of what we have said so far before we embark upon the actual procedures for feature reduction. Remember hyperspectral data can have as many as 200 or so bands. The field of imaging spectroscopy depends on reconstructing their reflectance spectrum for a given pixel from the set of hyperspectral measurements. Often the reflectance spectrum is identified by reference to a library. A prototype spectra. It is important for that to be effective that the recorded spectrum is radiometrically calibrated. Most of our focus will be on analysis by machine learning methods. To be effective, they require the number of features to be reduced. In the second question here, pay particular attention to the correlations of each of the four bands, with all three of the other bands. 

### Module 3 Lecture 2: Exploiting the structure of the covariance matrix

In this lecture, we are going to examine the structure of the covariance matrix and thus the correlation matrix to see how their properties help us undertake feature reduction. We have met the covariance metrics several times in the past. It is the starting point for the principal components transformation and along with the main vector, defines the class signature when we undertake maximum likelihood classification. In the latter context, we know that the accurate computation of the class covariance matrix can be a problem for hyperspectral data if we do not have enough training samples per class. Remember, we have to have enough independent training samples in each class in order to estimate reliably the elements of the covariance matrix. Generally, that is not a problem with the principal components transformation because then the covariance matrix is computed using all of the available training samples and not just those for an individual class. We can examine the covariance matrix for the data as a whole. We will do that now and note that it has some interesting structural properties. Rather than the covariance matrix itself though, we will look at the correlation matrix instead. Which remember, is derived from the elements of the covariance matrix. Let's look at an example of a real correlation matrix. Here we will examine the metrics for the Jasper Ridge, USA image, which has 196 bands. The correlation matrix for this image will be of dimensions 196 by 196. This is far too large for us to write down. We can, however, represent it in an image form as shown to the right. Remember, correlations are in the range of minus one to plus one. A correlation of zero means just that. There is no correlation between the pair of bands. Whereas a correlation of plus one means total positive correlation. While minus one means total negative correlation. If we choose a gray scale where black represents zero correlation and white means total positive or negative correlation, we get a very interesting representation of the correlation matrix. Note that there are regions of high correlation distributed down the principal diagonal of the matrix. That means that the bands in those blocks are highly correlated. Where there are black box, the associated bands have little correlation among them. There are some groups of bands which leave the correlations off the diagonal, suggesting some interrelationships of bands in the middle infrared and between the middle infrared and visible regions. In general, though, we can assume that most of the correlation activity is down the principal diagonal. What does that now suggest for feature reduction? Here we neglect the off diagonal correlations and represent the correlation matrix just by the blocks that form down the diagonal. That matrix now becomes a block diagonal matrix which has a number of very interesting properties. If we write the block diagonal matrix in the form shown in the center of this slide, composed of H separate blocks down the diagonal, a number of simple and useful properties arise as we now see on the next slide. First, the determinant of a block diagonal matrix is the product of the determinants of the individual blocks. That means the logarithm of the determinant is the sum of the logarithms of the individual determinants. Secondly, the trace of the matrix is the sum of the traces of the individual blocks. Finally, the inverse of the correlation matrix can be computed using the inverses of the individual blocks as shown on the bottom of this slide. Now what happens to a column vector which has the same dimensions as the covariance or correlation matrices? This could be, for example, the main vector which corresponds to the covariance matrix. Clearly, it can be represented as the concatenation vertically of a set of smaller vectors corresponding to the blocks of the covariance matrix. That leads to a particularly important matrix identity, which says that the vector, matrix vector construct that we use regularly in image processing can be decomposed into the sum of the same expression computed over each of the blocks. We now have all the material needed to show how block diagonalizing the covariance matrix simplifies the principal components transformation and the maximum likelihood procedure. Just before we do that, we can note a further simplification. Rather than utilizing the actual blocks of high correlation down the diagonal, we could just restrict our attention to the correlations that exist among immediately adjacent bands. For example, as shown in this slide, we could just use the two by two or three by three blocks indicated. While probably not as accurate as retaining the true block diagonal structure of the matrix. Such a simplification reduces dramatically the number of bands that had to be considered when training a classifier, as we will see shortly. Irrespective of whether we use that maturation simplification or stay with the original block diagonal form of the matrix, we now look at how the block structuring of the covariance matrix affects the formula for the principal components transformation. Remember, to compute the eigenvalues of the covariance matrix, we have to solve the characteristic equation shown here in determinant form. When the covariance matrix is blocked diagonalized, the characteristic equation is a product of a set of equations of smaller dimensions. Thus the roots of the equation are the eigenvalues of the component block matrices. Likewise, we can find the eigenvectors and the principal components transformation matrix by computing everything on a block basis and then combining results. Now let's see how blocked diagonalizing the covariance matrix and equivalently, petitioning the main vector affects the maximum likelihood classifier using the rules above for block diagonal matrices. The discriminant function for the Gaussian maximum likelihood classifier can be shown to reduce to the expression in the middle of this slide. Although using the block diagonal approximation to the covariance matrix ignores correlations among bands that are widely separated, it does have significant benefits in terms of reducing the dimensionality that has to be considered when applying the maximum likelihood classification rule. The largest matrix for which the elements need to be estimated using training data is that of the largest block. In general, that will be significantly smaller than the original matrix, meaning that many fewer training pixels are needed for reliable estimates, rendering the maximum likelihood rule useful for hyperspectral thematic mapping. We now look at an example of the performance of the maximum likelihood classifier with block diagonalization taken from the paper listed here. Two AVIRIS images were used based on the five different tests listed at the bottom of the slide that the last test applies just the minimum distance rule. Full details of these tests will be seen in the paper that I have referenced. The results of the tests are shown here in graphical form, so they can be compared. The classification accuracy shown is the average performance on training and testing data. Note how good the results are, particularly when the actual block diagonal form is used. What these results don't show, but which is included in the original paper is that there is a significant improvement in classification time with the smaller blocks. That is because the classification time for the maximum likelihood rule is quadratically dependent on the number of bands used. There are four key messages from this lecture. Covariance and correlation matrices can be represented as images. Strong band-to-band correlations appear in blocks, largely down the principal diagonal of the covariance or correlation matrix. The block diagonal structure can be exploited to simplify image processing operations that require computation of a covariance matrix. The block diagonal form of the covariance matrix allows the maximum likelihood classifier to be applied to hyperspectral image data. The last two questions here focus your attention on understanding what the image representation of the correlation or covariance matrix tells us. 

### Module 3 Lecture 3: Feature reduction by transformation

In this lecture, we are going to look at what can be done when using data transformation as I feature reduction tool. This has been a common approach for many years, particularly using the principle components transform. Remember that the principal components transformation maps image data into a new, uncorrelated coordinate system or vector space, it produces a data representation in which the most variance is along the first axis. The next largest variance is along the second mutually orthogonal axis, and so on. The later principle components would be expected, in general, to show little variance. They could be assumed to contribute little value to separability and thus could be ignored, thereby reducing the essential dimensionality of the vector space, and leading to more effective classification. As noted earlier, this is a widely used approach. It requires a computation of the covariance matrix, but using the data set as a whole. So there is generally sufficient training data available to be able to estimate the global covariance matrics accurately. We will illustrate the approach, but working through an example. Consider the two class, two- dimensional example shown here. If you look at the distribution of the data by class, you can see even now that the two classes should be separable in an axis aligned at about 50 degrees to the original axis. In other words, the first principle component. Now, let's verify that. Using the standard formulas, the global mean vector and covariance matrix are as shown here. We then compute the eigenvalues of the covariance matrix and thus the corresponding eigenvectors. Note that the eigenvalues suggest that a large part of the data variance will be along the first principle component. From the eigenvectors, we get the principle components transformation matrix from which we generate the formula for the first principle component as a function of the original axis. That is shown plotted in the diagram along with the projections or components of the data onto that axis. As expected, the classes can't be separated in the first principle axis, meaning that the second axis can be ignored. Subsequent classification can therefore be based just on the first PC alone. Thus, in this simple example, the number of features has been reduced from two to one. There are some situations, however, where the principal components transformation will not work as a feature reduction tool, such as that illustrated here. Feature reduction using the principle components transform, only works if the classes are separable along a reduced set of principle axes, as in the example just considered. In the example here, we could not use principal components analysis for feature reduction because of the manner in which the classes are distributed. There is, however, a transformation that can be used in such a situation. It is called Canonical analysis. If we could find an axis, such as that shown in the bottom diagram then, again, we could represent the data with a single feature, rather than the two original bands. That axis is defined such that when the classes are projected onto it, they appear as far apart from each other as possible, while having the smallest spread. If, along that preferred axis, we define the '' among class variance"" and the average "within class variance" as shown in this slide. Then we want to find the axis rotation such that we maximize the ratio of those two variances. In order to find this expression and the actual transformation, we have to express the problem in matrix form, so that it applies to any number of features, we need also to express it in terms of the variances in the original features. In this slide, we define the key properties of importance when finding the so called Canonical axis. They include the global mean and the two variances expressed in matrix form as required. Now this looks a bit complicated, but it follows an approach almost identical to that of the principal components transformation. Let y equals day transpose x be the required transform that generates a new set of axis y. In which the classes have optimal separation. By the same procedure that was used for the principal components transformation, we can show that there within class and among class covariance matrices in the new y coordinate system can be expressed in terms of those in the original x coordinates as shown in the center of the slide. As with principle components, the row vectors of the transpose define the axis directions in y space. Let's small day transpose. Be one particular vector. Say that one that defines the first axis along which the classes will be optimally separated. Then the corresponding with in class and among class variances will be as shown on the bottom of the slide. We now set up the measure shown as equation A, which we have to maximize, that is, differentiate with respect to the axis direction D. We then solve the generalized eigenvalue equation shown subject to the normalizing constraint at the bottom. That solution generates the Canonical axis for us as we see in the analysis on the next slide. Here we see a two class data set which is not separable in the first principle component access. But will be soon to be separable in the first Canonical axis using the computations shown on this slide for the covariances. Substituting the covariants calculations into their characteristic equation at the top of the slide yields the eigenvalues of 2.5, four and zero. Having a zero eigenvalue tells us that there is only one Canonical axis in this example. Using the non zero eigenvalue in the characteristic equation at the top along with the normalizing condition leads us to the new axis shown in the equation at the bottom of the slide. That access is plotted on the original diagram in which it is saying that the two glasses are now separable using just one transformed feature. Feature reduction can be carried out by transforming the original spectral bands to a new coordinate system in which some transform bands can be discarded without affecting the separability of the classes. The principle components transform is the most popular method to use. Even though it is based on the whole data set, that is the global mean vector in global covariance matrix, provided the classes themselves are approximately distributed along the first few principal axis, it shows good results. If the classes do not distribute along the principal axis, then principle components analysis is not a good technique to use for feature reduction. It is better than to use a transformation associated with Canonical analysis. But principle components and Canonical analysis depend on covariance information. In the former, it is a global measure and thus easier to estimate accurately with limited training data. Whereas for Canonical analysis, class specific covariances are required and thus the method has limitations with high dimensional data such as hyperspectral imagery. These questions direct your attention to an alternative measure for computing class separability. 

### Module 3 Lecture 4: Separability measures

This lecture traits the third approach to feature selection, in which we set up measures to assess the importance to a classification exercise of the original sets of bands. This is the more classical approach. It entails setting up measures that help us to evaluate the relative importance of each of the bands, allowing us to discard some when we want to undertake thematic mapping. Generally, these approaches depend upon a measure known as separability. It can be based on an assumption that the classes of interests, can be described by probability distributions, or it can avoid the use of distributions. Obviously, the first approach is restricted to image datasets with small numbers of bands. Because of the need to estimate the statistics of the probability distributions, such as the Gaussian model that is almost always use. It is the old problem of not having enough samples per class, in order to estimate the distribution parameters reliably, provided the number of bands does not exceed about 10, those methods should be suitable. The second set of methods was developed to help us and detect feature reduction with high dimensionality datasets, such as hyperspectral imagery. That are generally more complicated to develop but are more widely applicable. Measures of separability tell us how distinct or spectrally different two thematic classes are, remember, classes are defined by sets of features. Our objective here is to see whether we can use fewer features and yet still carry out an acceptable classification. We use separability, to tell us where the class is based on choice and subsets of features, are distinct enough to allow good classification results to be achieved. Generally, we adopt a process such as this. First, we start with the full feature subset and a separability metric, compute how distinct the classes are. Next, we then remove a feature or sometimes sets of features and say by how much separability has been reduced. If separability is not badly affected, we could live at that feature. Otherwise we keep it and test other features. We then benefit from reduced classification costs and avoidance of this phenomenon. But the search is an exhaustive one, requiring all features to be assessed for a full evaluation. Divergence is one of the earliest and simplest measures of separability. It is based on quantifying the overlap of two class distribution functions, as illustrated in the diagram on this slide. Suppose we have two classes, omega1 and omega2, that are separable in the two features, x_1 and x_2. Being separable means they can be easily mapped as different classes by a classification algorithm, leading to high classification accuracy. If we discard feature x_2 and return only x_1 as shown on the bottom of the slide, we see that the class is then overlap and thus cannot easily be separated with high-accuracy by our classifier using just that feature alone rather than the two original features. Divergence tells us the penalty we pay by way of loss of separation, if we reduce the number of features by discarding one or more. Without going into the derivation, the formula here shows the computation of diversions for a pair of classes, are shown to be modeled by Gaussian or multidimensional normal distributions. As noted on the previous slide, we use divergence to see how the similarity of classes is affected, if we remove bands. What we're looking for is whether the separability of the two distributions is made never simply worse by the removal of features. If it is, then those features are important when running a classification and should not be ignored. On the other hand, if removing a feature doesn't drop the separability by very much, then it could be removed when doing a classification without significantly affecting accuracy. Divergence has a number of important properties. First, it is always positive. Secondly, it is zero if the distributions are identical. Thirdly, it never decreases with the addition of features. That means it is never high, if features are removed. Another measure of separability, is the Jeffries-Matusita distance, which is again a measure of the average distance between two distributions, i and j, for normally distributed classes. It has the form shown, involving an exponential function of a distance measure called the Bhattacharyya distance. The exponential term will be seen soon to be very important. While JM distance bears some similarity to divergence, it's significant difference is the exponential term. To see the effect of that, we sketch on the next slide, how divergence and JM distance would vary as a function of the distance between two distributions measured by the differences in the main vectors. By looking at the formulas for divergence, and JM distance, the behaviors shown on the two diagrams here can be seen. Divergence increases without bound as the distance between the classes increases. Whereas the JM distance curve saturates. Which behavior is the more useful. If we think about how the probability of correct classification might change as the two classes move further apart, it is more likely they behave like the JM curve, as we will see on the next slide. That is because the overlap between distributions drops quickly at first and then approach zero. Once the distributions are well separated, and thus classification accuracy is close to 100 percent, further separation will have little effect on the classification result. Here we see the probability of correct classification compared with the behaviors of divergence and JM distance. This suggests that the JM curve is more realistic all because of the saturating behavior of the exponent in its formula. However, because JM distance requires a computation of matrix inverses of the sums of all pairs of classes, it is time consuming if it is to be used to check the average similarity of many spectral or information classes. By comparison, divergence is much quicker to compute because the inverse of each class covariance matrix is computed only once, even when checking the average similarity of a set of classes. A third separability measure was developed heuristically to take advantage of the inherent benefits of both divergence and JM distance called transformed diversions. It embeds the usual divergence expression into a saturating exponential function, so that its overall behavior emulates that of the probability of correct classification. It is also faster to compute than the JM distance. Transformed divergence is perhaps the most commonly used measure of separability in remote sensing when classes can be described by the normal probability distribution. As we noted earlier, that generally applies when the number of bands or features does not exceed about ten. One of the important uses of separability measures is to be able to assess beforehand whether the spectral classes are well enough separated to ensure a certain level of classification accuracy. While the actual performance of a classifier depends on many factors, including how well the analyst has defined the spectral classes, collected reference data and trained the algorithm. If the classes are not well separated then there is an upper limit on the classifier accuracy that can't be achieved. The formula shown on this slide provides an upper limit on the probability of correct classification in our two class case as a function of transformed divergence, and this is plotted on the next slide. Here we see the theoretical upper bound of classification accuracy versus transformed divergence, along with some empirical classification results computed from 2,790 trials. As seen, unless transformed divergence is close to its maximum, binary classification accuracy falls well short of the theoretical upper limit. We met clustering algorithms in the last module. They are the basis of unsupervised classification and form a component of a combined unsupervised or supervised classification methodology that we will visit shortly. One of the last stages in clustering is to evaluate the size and relative locations of the clusters produced. If the clusters are too close to each other in spectral space, they should be merged. Well, what does too close mean? If cluster mean and covariance data is available, we can make merging decisions based on a pre-specified transformed divergence. By establishing a desired accuracy level for the subsequent classification, from which the corresponding value of transformed divergence can be specified, cluster pairs with separabilities below that value should be merged. That completes our treatment of measures of separability based on distribution functions. In the next lecture, we will relax that requirement and consider separability measures that can be used when it is not possible to estimate second order parameters, such as covariances, that is usually the case with hyperspectral data. For any classifier, it is good to use as few of features as possible in order to constrain the classification time and thus costs. Feature reduction is important in maximum likelihood classification in order to avoid the Hughes phenomenon, in which not enough training samples are available to develop reliable estimates of the elements of the covariance matrix. Divergence, Jeffries-Matusita Distance and Transformed Divergence are three methods commonly used when classes can be modeled by probability distributions. JM distance performs well but it is time-consuming to evaluate, whereas divergence is faster to use but doesn't work well when classes are widely separated in spectral space. Transformed Divergence combines the benefits of both approaches. Separability measures allow us to place an upper bound on classification accuracy. Remember to consult the solutions if you are having problems with any of the quiz questions at the end of each lecture. 

### Module 3 Lecture 5: Distribution-free separability measures

We now look at separability measures that can be used when it is not possible to represent classes by probability distributions. We will look at two different measures. The reason we need to develop separability measures that can be used with [inaudible] class models is so it can handle data of hyperspectral dimensionality. In such a case, there are insufficient training samples for the generation of reliable covariance matrices. Separability measures are still possible in these circumstances, but they are a bit more complicated to develop since we can't rely on measures that look at the difference between say, two normal distributions. ReliefF is one of the commonly used measures for feature selection, that is, for evaluating existing features, and seeing which of them could be discarded. This measure gives features a weight, which is adjusted by reference to the classes of data being used. Those features with weights below a user-specified threshold are discarded. We will develop the technique by reference to the two class, two-dimensional dataset shown on this slide. The classes have been drawn intentionally so that one feature x_1, does not aid separation while the other x_2, does. The process commences by selecting a pixel at random from one of the classes. We then find its nearest neighbors in the same and in the other class as shown. We now want to derive a measure that gives more weight to feature x_2, than feature x_1 with respect to those chosen pixels. Remember, we have two features, x_1 and x_2. We now define a weight for each feature that tells us how important it is with regard to class separation. Call these weights Omega_1 and Omega_2, respectively, but please don't confuse those symbols with the same symbols we use for classes. Here they refer to weights. The weights are initially set at zero and then updated using the pixel we have chosen randomly. We will shortly choose further random pixels to give us a better measure of the weights, but for the moment, just concentrate on the one in the diagram. For the weight corresponding to the i_th feature, we use the updating rule shown. In the rule, x_i is a feature of the randomly chosen pixel, x_ i superscript s is the corresponding feature of the nearest neighbor from the same class, and the x_i superscript o is the corresponding feature of the nearest neighbor from the other class. D is a distance measure. Applying the updating rule to the diagram shown, we see that the adjustment will be small and negative for feature x_1, but large and positive for feature x_2. That means the weight for the second feature increases, indicating its relative importance, while that for the first feature drops. The same process is carried out several times using a set of m randomly chosen sample pixels and updating the weights each time. The distance values are normalized by the number of samples m. If there is a reasonable distribution of pixels in each class. A little thought will show that the weight for the feature x_2 will go on increasing relative to that for x_1. A helpful modification is to use a set of the K nearest neighbors to the randomly chosen pixels as indicated in the diagram. In this case, the updating rule becomes as shown by the formula which indicates normalization by both the number of trials and the number of nearest neighbors. Again, the process is initialized with all weights Omega_i set to zero. Each weight is then updated by selecting m random samples using the rule above. At the completion of the process, those features x_i with weight values above a threshold are kept for subsequent classification. The ReliefF method, as originally formulated, applies to just two classes. It is readily extended to the more usual multi-class situation by adding in other class term, for each of the classes other than that from which the random sample is taken, leading to the more complete formula shown on the slide. The x_i superscript ok and they are features of the kth nearest neighbors of the current random pixel in each of the other classes. The probability expression p(o) over 1 minus p(s) whites the contributions from the other classes in proportion to their prior probabilities. In all these formulas, the distance measure can be any convenient metric. Euclidean and city block distances are the most commonly used. Another feature selection technique that avoids the need to use probability distribution models for classes, is non-parametric discriminant analysis or in the, like canonical analysis, it sets up measures of the distributions of pixel vectors within classes and the dispersion of the classes themselves in spectral space. It then sets and access transformation, which provides maximum separation between the classes based on those measures. Instead of using covariance matrices, though, it employs the concept of scattering matrices in the manner now to be developed. This is a bit complicated conceptually, but not hard mathematically. If you found the concepts difficult, it will affect your understanding of the equations. Since this procedure is only one of a number of distribution free methods, you may wish to pass over the material if you find it too complicated. But if you wish to employ it in practice, software is available that will guide you through issues. In its simplest form, NDA examines the relationship between the training pixels of one class and the nearest neighbor training pixels from another class. At the bottom of the slide, we show an expression for x, which represents pixel drive from classes, that is the nearest neighbor of pixel i from class r as shown in the diagram. Pay particular attention to how the subscript has been set up. It says j is a member of class s, which is a nearest neighbor of i, which is a member of class r. We can describe the distribution of class r pixels with respect to their nearest neighbors in class s by a covariance like calculation. However, because we're now not describing the distribution of pixels about a class main, which is a parametric description, it is better to use a different term than covariance matrix. To talk about the scatter of pixels with respect to each other, we use the term scatter matrix. The scatter of all of the training pixels from class r about their nearest neighbors in class s, is defined by the scattering matrix definition shown in the equation, which actually computes the expected value of the distance between those pixels times the transpose of that distance, giving a matrix effectively of the distance squared. In this expression, x subscript i is, remember r is the i pixel from class r. The amica r conditionality reminds us that the calculation is determined by pixels from class r. We then do a similar calculation for the scatter of the training pixels from class s about their class r nearest neighbors and then average the two measures. Usually, the average is weighted by the prior probabilities or relative abundances of the classes as shown in the first formula on the bottom of the slide. Often in the a is not just the nearest neighbor, but instead a set of k class is training pixels as the nearest neighborhood for each class r training pixel. The local mean over that neighborhood is then use in the calculation of the between class scattering matrix, giving rise to the second formula for the between class scatter matrix which includes a main vector term for the k nearest neighbors in class s shown on the next slide. The main vector in the previous calculations is given by the expression shown here. Note that if K, the size of the neighborhood, is the same as the total number of training pixels available in class s, the local mean becomes the class mean, and the between class scatter matrices resemble covariance matrices, although taken about the mean of the opposite class rather than the mean of the same class. This approach has to be made applicable to a multi-class situation. In doing so, we note that there are as many weighted means of the pixels from the other class as there are other classes. This is illustrated in the diagram for the case of three classes: r, s, and t. It is easier to express the expectations in algebraic form so that for c total classes the among-class scatter matrix is as in the formula given. In this expression, the inner sum computes the expected scatter between the N_r training pixels from class r and the mean of the nearest neighbors in class c different for each training pixel. The middle sum changes the class c, still relating to the training pixels from class r. The outer sum changes the class r for which the training pixels are being considered. The latter computation is weighted by the prior probability for the class. Having derived an expression for how the classes are scattered with respect to each other, we need to look at how the pixels scatter within the classes. We define the mean vector of a class based on the k nearest neighbors of the i_th pixel in class r from the same class as shown. This leads to the within-class scatter matrix in the second expression. How do we use the two scatter matrices: S_W and S_A to carry out feature reduction? We are looking for a new set of axes in which S_A looks as large as possible while at the same time, S_W looks as small as possible. We define the axis transformation in the usual way by the matrix equation Y equals D transpose X, and look for a y coordinate system in which we can maximize the joint measure, which we'll call J. That's defined as the trace of the within-class matrix inversed times the among-class matrix. Once the new axes have been found, we will have a maximum class separation in the first, followed by the next best separation in second, and so on. We then retain a subset of those transformed bands or axes as the feature-reduced representation of the data that we wish to classify. For high dimensional imagery, we need measures of separability that do not depend on the need to compute a covariance matrix. The method known as ReliefF devises a weighting scheme. Each feature or band is allocated a weight to indicate its significance. Bands with weights less than a user-specified threshold can be discarded when undertaking a classification. Non-parametric Discriminant Analysis, NDA, parallels canonical analysis in that it seeks a new set of axes in which to represent the pixel vectors, such that classes have maximum separation in the first new axis, next best separation in the next axis, and so on. Non-parametric scatter matrices are used in the calculations. After transformation, those low order features which do not help class separation are discarded, resulting in a data-set with smaller dimensionality. Please consult the solutions if you need to in order to help you consolidate your understanding of distribution free feature reduction techniques. 

### Module 3 Lecture 6: Assessing classifier performance and map errors

With this set of lectures on accuracy assessment, we come to the end of our material on general remote sensing before then completing the course by set of lectures on remote sensing with imaging radar. Assessing how I classifier performs and the accuracy of the thematic map it generates is one of the most important topics in operational thematic mapping in remote sensing. We will see that classifier performance and map accuracy are not the same thing. This is a crucial matter in practical remote sensing. If significant economic decisions are going to be based on the thematic maps in class hemorrhages generated by the machine learning techniques we have covered in this course, then we have to be as sure as we can that we are interpreting the results correctly. In this series of lectures, we will look at how we assess the performance of a classifier. The relationship between classifier performance and the accuracy of a thematic map. The number of testing samples that should be used to test thematic map accuracy and the number of testing samples needed to estimate class areas accurately. Recall from a previous work on machine learning methods that classifier performance is assessed by selecting a sample of testing pixels previously unseen by the classifier, the algorithm is applied to that set to check the performance of the classifier. In selecting the set of testing pixels, the analysts has to ensure that biases are not introduced by the fact that the aerial sizes of the classes might be quite different. If care is not taken, more testing pixels in a random sample will come from large classes whereas smaller classes will have fewer pixels with which to test the operation of the classifier. We will assume we have avoided such a bars in what we are now going to develop. The set of testing pixels is often referred to as reference data or less frequently nowadays as ground truth. The same terms are generally applied to training, either. It is common to express the results of testing the classifier by setting up an error metrics as on the next slide. In the past, it was sometimes called a contingency matrix or a confusion matrix. Terms you may still find in use. Here we see an error matrix which has been compiled from a thematic mapping exercise involving just three classes, A, B and C. The cells are populated according to whether the pixels have been placed into their correct class, assessed by comparing the classifier output with the reference data or whether they have been put into the wrong class. That is a classification error has been made. The rows represent the classifier. For example, it put 39 pixels in the class A. But when we look at what the referenced data tells us for each of those pixels, we see that somewhere really from class B and some from class C. The same thing happens for its labeling or pixels as class B pixels and class C pixels. Down the columns, we see how each of the pixels in the reference say there has been handled by class. For example, in the first column, the 50 pixels that are class A in the reference later have not all been put into that class by the classifier. Some have been put into class B and others into class C. If there were no classification errors at all, the matrix would be diagonal with zeros for all the off diagonal entries. In such a case, the classifier and reference data agree on the label for every pixel. In this slide, we just emphasize that the rows tell us about the classifier outputs. Remember, it has liable 39 pixels as class A. 50 as class B and 47 as clas s C pixels. So that is how many pixels for each class we will find on the thematic map. In contrast The column sums, as we noted on the previous slide, are the numbers of pixels in the reference data set for each class. We now introduce some new nomenclature. In placing some class A pixels into the other classes. The classifier has committed errors. So we refer to the off diagonal entries along the same row as errors of commission. The errors down a column, represent pixels from the reference cider that the classifier has failed to label property. We call those errors of omission. We now need to ask ourselves the question of how to determine the accuracy of a classification exercise using the entries in the error metrics. Well, the answer actually depends on whether you are the user of the thematic map produced by the classifier. Or the analyst who ran the classifier to label the pixels which led to the map. They are not the same. A measure of how well a classifier has performed is, how well it has labeled the reference pixels. If we choose class B as an example, the classifier was presented with 40 class B testing pixels and got 37 of them right. That is an accuracy of 92.5%. We call this the producer's accuracy, since it is the accuracy with which the results were produced. However, the user of the thematic map is interested in how many pixels labeled B on the map are correct. There are 50 class B pixels on the map. Only 37 of which are correct. The others are the result of the classifier committing errors on class A and class C pixels. We call this the user's accuracy. For this example, it is 74%. Here we see the full set of user's and producer's accuracies. Along with a commonly used measure of classifier performance, that is the overall accuracy which is computed as the total number of pixels labeled correctly by the classifier. As a fraction of the total number of pixels in the image. Note the range of producer's and user's accuracies compared with the average accuracy. There is an alternative technique used for assessing the accuracy of a classifier, which does not use a separate set of testing or reference pixels. It is called cross validation and involves the following steps. First, a single label set of reference pixels is used for both training and testing. It is divided into k separate equally sized subsets. Next, one subset is put aside for accuracy testing, and the classifier is trained on the pixels in the remaining k- 1 sets. The process is then repeated k times with each of the k subsets excluded in rotation. At the end of those k trials, k different checks of classification accuracy have been generated. The final classification accuracy is the average of the k trial outcomes. A variation to this approach is to use subsets of size one. It is then called the leave one out approach. There will be as many trials as there are reference pixels. Again, after all the trials have been performed, we will have an estimate of accuracy. Our way of summary, note that first correctly assessing the accuracy of a thematic map is an essential step in operational remote sensing. Next, accuracy assessment is carried out using labeled testing pixels but better referred to as reference data. The error matrix summarizes the performance information needed to assess class accuracy. There is a difference between producer's accuracy, that is how well the classifier has performed. And the user's accuracy, that is the accuracy of the thematic map produced by the classifier. Cross validation is an alternative means for assessing classifier performance. And finally, they leave one out approach is a special case of cross validation often found in practice. These questions direct your attention to understanding fully the concepts involved in assessing classifiers and thematic maps. Pay particular attention to the third question about cost overheads of the cross validation approach. 

### Module 3 Lecture 7: Classifier performance and map accuracy

We now wish to extend our analysis of the accuracy of a thematic map generated by a machine learning classifier. In the last lecture we looked at the difference between measures of the performance of a classifier class-by-class. And users accuracy which is related to the accuracy of the thematic map produced by the classifier. We now want to take this analysis further. Are focusing on the true accuracy of the thematic map. We will show that users accuracy only represents the accuracy of the map. If the selection of reference or testing pixels by class reflects the actual ground proportions of the classes. In other words, there prior probabilities. To do this, we need to use some simple probability theory. Let the variance t r and m represent the true labels for the pixels in terms of what is actually on the ground. They variables in the reference stay there. That is the data we have chosen to measure map accuracy. And their labels on the thematic map produced by the classifier. Hopefully the first two t and r are the same. But we will treat them separately for a moment. Let the two symbols Y&Z represent any of the available class labels for our three class example. They can be any of A B or C. Suppose we sampled the thematic map that has been produced by the classifier. If we check the accuracy of those samples against their true labels on the ground, then effectively we are estimating the map accuracy conditional probabilities which are expressed. The probability that the true label is said given that the map says it is it. That is what the user of a thematic map is interested in. More generally, the probability that the true label is, Y given at the map label is Z. Is the probability that Y is a correct class if the map shows it. This allows us to assess incorrectly as well as correctly label the pixels. If we use the reference pixels we have selected to assess accuracy and check the corresponding that labels, then we are in fact computing the classifier performance conditional probabilities. That is, the probability that the map sensor pixels Z. Whereas in fact the reference say that says it is Z. Or more generally, the probability that the map says it is said when the reference say that says it is, Y. If the pixels in the reference data set are chosen in the same class proportions as on the ground. We can assume that the variants r and t are the same. That means we have chosen how reference data set carefully so that the proportions of pixels in each of the classes are the same as, say, proportions in reality. We can then use r in place of 2 in our equations so that for example. The accuracy of the map from the previous slide is the probability the reference says Y. If the map says said, which is equivalent to the probability that the true label is, Y if the map so said? This is still a probability that the true label for the pixel on the ground is Y. If the classifier, then it's a thematic map, says it is in. Because we are assuming r&t are the same. We can now use Bayes theorem to relate the accuracy of the map to the performance of the classifier. So in other words, the probability that the reference datasets, Y given the map, says it is equal to the probability that the map says said if the reference letter says Y times probability that the reference say that is Y divided by the probability that the map label is Z. Now in this expression Pr = y. Represents the Lockley Hood that class why pixels exists in the region being image. This is in fact a property of the scene on the ground. In contrast he of =D. Is the probability that class Z pixels appear on the thematic map? And it is given by the. Equation shown at the bottom of the slide where we sum joint probabilities. Well, let's take stock of where we are at this stage of the analysis. The classifier labels pixels as m, whereas that labels in the reference later are r. Thus, a classifier generates the probabilities that the classifier output will be Y if the reference data is Y, shown as the conditional probability p m equals Y given r equals Y. Whereas we as users are interested in the probability that the reference data is a Y pixel if the map or the classifier says it is. That is a conditional probability, p r equals Y given m equals Y. We will now apply those concepts using our three class example from the previous lecture. Here, we'll show the error matrix we are working with on the upper left hand side of the slide. The table on the right shows a classifier performance or produces accuracies. The users accuracies and the map accuracies computed from the expressions from two slides back, involving Bayes theorem. If you wish to check the calculations, you will need to compute the full set of classifier performance probabilities. That is, the probability that the map says Z if the reference data says Y using the entries from the error matrix. Note, the effect of the class prior probabilities that is, the real proportions of the classes on the ground on the results. The map accuracies are only the same as they uses accuracies when the set of label pixels used for reference data is in the same proportion by class as the real situation on the ground. The second and third examples show the differences if that is not the case. The last is an extreme by the instructive example. The average map accuracies given here were computed by the formulas developed on the next slide. We now consider the average accuracy of the thematic map generated by the classifier. Based on what we have done so far, the accuracy by class is expressed as the probability that the true class is Y if the thematic map says it is, that is shown by this conditional probability. In computing an average accuracy of all classes, we could be tempted just to average the above expression over all values of Z. However, that gives too much weight to the smaller classes and diminishes the contribution to the average of the performance on the larger classes. It is better, therefore, to compute the map accuracy as the average but weighted by the probabilities of occurrences in a thematic map. That leads to the map accuracy expression shown here in two forms. The last expression is important. It tells such that the true average map accuracy can be obtained from the classifier performance provided the classifier performance probabilities are weighted by the class prior probabilities. The average map accuracies noted in the final column of the previous slide have been computed that way. Some analysts prefer to express classifier performance in terms of the Kappa coefficient. It is a measure intended to avoid any chance agreement between the performance of the classifier and the reference data. If we again use the results of the example from the last lecture, we can estimate the chance agreement. The two sets of calculations on the right hand side of the slide show the performance of the classifier by class and the allocation of pixels by class in the reference data. The probability that they both place a pixel in the same class is given by the products of their individual performances. The probability that both the classifier and the reference data place a pixel at random in the same class, overall classes is the sum shown her0,e 0.106 plus 0.108 plus 0.117 Which is 0.331. This is the random chance of their agreeing on the label for a pixel. On the other hand, the probability of a correct classification determined from the agreement of the classifier output and the reference data is 35 plus 37 plus 41 divided by 136, which is 0.831. In terms of those calculations, the Kappa coefficient is defined as shown here, and for our example has a value of 0.747. In order to give meaning to that number, we use the ranges shown in the table. Kappa is not strongly supported by all analysts because its value is not a unique function of its arguments as shown here. By way of summary note. There is a difference between the performance of a classifier and the accuracy of the resulting thematic map. Map accuracy can be obtained from classifier performance if the class prior probabilities are used. Classification errors on large classes can give significant errors of commission into smaller classes. The Kappa coefficient is a common alternative measure for expressing classifier performance. And finally Kappa suffers because its range of values does not naturally imply levels of performance. Nevertheless, it is often used as a comparative measure when jointly assessing different classification algorithms. In answering the first question here, you may wish to look at an extreme example. For example, consider a classification exercise with just two classes. Suppose in reality class A occupies 90% of the scene on the ground and class B the other 10%. Suppose your classifier is 100% accurate on class B, but has 50% error for class A pixels. What does that imply for the thematic map? 

### Module 3 Lecture 8: Choosing testing pixels for assessing map accuracy

We need now to consider one last topic on accuracy assessment, to complete the theoretical aspects of remote sensing before we move onto radar. When we looked at training a classifier algorithm, we had to have enough training pixels per class to estimate reliably any parameters in the technique we chose. We now turn to the related matter of choosing independent samples from a thematic map, that is referenced data or testing data, in order to get reliable estimates of the maps accuracy. If we don't choose enough, then we will end up with a poor estimate of map accuracy. We will undertake this analysis by looking at how well the testing samples we choose actually sample the thematic map. In this slide, the top diagram represents the thematic map itself. We are going to work out this theory with just two classes, but we will comment on the multi-class situation later. In a sense, we don't have to know what happens with each of the two classes. All we are interested in is whether a pixel in the map is correctly labeled irrespective of its class. By knowing how many pixels are accurate overall, we can get the map accuracy. In this top diagram, the shaded pixels are those which have been labeled incorrectly by the classifier. We can describe the situation by a binary period, which takes the value one if a pixel has the correct label, and zero if it is in error. The second diagram shows a random sample of testing pixels. Because they are chosen randomly and because we do not know which pixels in the map above are correct and which ones are in error, we have to choose enough testing pixels to be sure we sample the map so that our accuracy estimate is reliable. The bottom diagram shows the testing samples layered over the thematic map. We now define a new boundary period which takes the value one if a correctly labeled pixel is sampled, and zero otherwise. First, we look at the distribution of errors described by the variable y_i as shown. That theorist has a binomial distribution. If we take the sum of all y_i, we get the total number of correctly labeled pixels because the incorrect ones contributes zero to the value of the sum. That means the accuracy of the map can be expressed as shown here by the formula P equals one over N times the sum of the y_i periods. N is the total number of pixels in the map. What we are interested in is getting an accurate estimate of P, the map accuracy. Now consider the properties of g_i. It also has a binomial distribution. By definition, little n is this begin. Otherwise, we will be sampling every pixel in the thematic map. The sum of all the g_i is the number of correctly labeled pixels, but in this case as found in the testing set. As before g_i is zero for testing pixels which are incorrectly labeled so that they do not contribute to the value of the sum. The proportion of correctly labeled testing set pixels, which is always what we use as a measure of accuracy, is given by the expression for p. That is, one over n times the sum of the g_i. We want p found from the testing set to be a good estimate of the actual accuracy of the thematic map, P. The question is, how many testing set pixels n, are needed to ensure that? Since the sum of binomials is itself a binomial distribution, the map accuracy estimate p, is binomially distributed. We can assume that g_i and y_i come from the same underlying distribution. They then had the same means so that the expected value of p is equal to the actual map accuracy P. Although the expected value of the map accuracy found from the testing pixels is the main P, the actual value found can be different. Hopefully, it would be in the vicinity of P, but depending on the number of testing pixels taken, it might be quite different. But how different might that be? To answer that, we look at the variance of p about the main P, and that variance is given by the expression shown on the slide. Usually, the number of pixels in a remote sensing image far exceeds the number of testing pixels so that the variance takes the form of the bottom equation on this slide. It is inversely proportional to the number of testing pixels n so that N reduces severance and makes the estimate p, closer to the actual map accuracy P. Repeating the message from the previous slide, the variance reduces with more testing pixels. Thus, more testing pixels gets us closer to the true value of the accuracy of the thematic map. To a good approximation, little p can be assumed to be normally distributed about the mean big P, as illustrated in the diagram. Two standard deviations about the mean contain 95 percent of the population. Thus, with 95 percent confidence, we can say that our estimate of the accuracy of the thematic map lies within about two standard deviations of the true value. Let's say now how it can use that information. With 95 percent confidence, we know our estimate of the accuracy of the thematic map, lies within about two standard deviations of the true value. Now suppose we are happy for our estimate of the accuracy to be within plus or minus epsilon of its true value. That is, we're looking for little p, which is equal to big P plus or minus epsilon. With 95 percent confidence, we know it will be within the range if epsilon represents two standard deviations. Thus, we have the formula for epsilon equal to 2 times the square root of p times 1 minus p divided by n, which gives the number of testing pixels shown on the bottom of the slide. Interestingly, note that it is a function of the true accuracy p. Is that unusual? We'll know. If p was equal to 1. In other words, the map was 100 percent correct. We really don't need to center it at all. Whereas if P were low, the numerator, P minus p squared is large, meaning that more samples are required if the accuracy is relatively poor. As an example, suppose we are happy for an estimate to be within plus or minus 0.04 of a true proportion, which is thought to be about 0.85. That is, we are happy with accuracies in the range of 0.81-0.89. Then at the 95 percent confidence level, n equals 319. That means randomly sampling or randomly selecting 319 testing pixels will allow a thematic map accuracy of about 85 percent to be checked with an uncertainty of plus or minus 4 percent, with 95 percent confidence. Further examples are in the table on the next slide. Here we see explicitly that the required number of testing pixels decreases with the actual accuracy of the thematic map. Our previous analysis was focused on the need to establish a sufficient number of testing pixels so that the overall accuracy of a thematic map could be assessed. What we want to know now is how to ensure we have enough testing pixels per class, to know that each class is accurately labeled. One approach upon the paper referenced here, leads to the numbers shown in the table below. In this case, with an uncertainty in the estimate plus or minus 10 percent at the 95 percent confidence level. When looking at overall map accuracy, we were able to use binomial statistics since only two outcomes are possible, a correctly labeled pixel or an incorrectly labeled pixel. If we wish to look at class wise accuracies for each pixel in a thematic map, a multinomial probability distribution is needed. We will not go into the development here. But note that it leads to the following conservatively high estimate for the number of testing pixels required per class. When we are interested in whether we have the correct class for a pixel. Not just that it's this correctly labeled. We can use the expression shown on the slide. Epsilon is again the level of tolerance we need around the estimate of the class accuracy. B is the upper beta percentile for the course grade distribution. With one degree of freedom. Beta is the overall precision needed divided by the total number of classes. For the background theory here, see that paper by Tortora. Consider this example from the book by Congalton and Green. Suppose we have eight classes in a thematic map. We want to find the estimate of the accuracies within plus or minus 5 percent, and we want our results to be at the 95 percent confidence level. In this case, beta equals 0.05. divided by 8, which is the 0.625 percentile of the distribution, giving b the value of 7.568. That's the total number of testing pixels needed, is 757 as shown. Therefore, we need just under 100 per class. By way of summary, note, just as in training, where we need enough labeled reference pixels to get good estimates of the parameters of a classifier, in testing, we must have enough labeled reference or testing pixels to ensure you can get good estimates of the accuracy of a thematic map. We need enough testing pixels to ensure that the estimate of the map accuracy obtained is as close as possible to the actual accuracy of the thematic map generated by the classifier. We can look for estimates of the overall map accuracy or the accuracies with which each of the individual classes have been mapped. The latter requires more training pixels. It is important to understand the messages behind each of these three questions, if you are to be sure you understand the guidelines for selecting the number of testing pixels. 

### Module 3 Lecture 9: Classification methodologies

Often, we say analysts, sometimes naively using machine learning techniques for thematic mapping. Without thinking carefully about the objectives of the exercise and whether a careful mix-up procedures can produce better results. Here, we want to look at some mixed algorithm methodologies. Too often, analysts just apply a chosen classifier to an image, using labeled training data for each class and expect good practical outcomes. Such an approach works okay for highly stylized images, such as the labeled data sets regularly used to test new algorithms, but they are not optimal when looking at real and heterogeneous image data. In this lecture, we will look at practical methodologies that work well in practice and yet are often overlooked. We start with a review of the properties of supervised and unsupervised classification. Remember, in supervised classification, the analyst acquires beforehand a labeled set of reference data, that is, training pixels, for each of the classes of interests and desirably for all apparent classes in the image. That data is used to estimate the parameters or other constants required to operate the chosen classification algorithm. The algorithm can then be applied to the full image of interest, and testing pixels can be used to assess how well it has performed. The amount of training data will often be less than 1-5 percent of the image pixels. The learning phase, therefore, in which the analyst plays an important part in the labeling of pixels beforehand, is actually performed on a very small part of the image. Once trained, the classifier can then attach labels to all the image pixels, and that is where a significant benefit occurs in thematic mapping. The output from supervised classification consists of a thematic map of class labels, often accompanied by a table of area estimates, and importantly, an error matrix which indicates by class, the residual error, or accuracy of the final product. In unsupervised classification, clustering algorithms are used, typically on a sample of an image to petition the spectral space into a number of discrete clusters or spectral classes. It then labels all image pixels as belonging to one of the spectral classes found, typically using a minimum distance assignment. A cluster map can be produced in which the pixels are given labels indicating which cluster they belong to. Unsupervised classification is a segmentation of the spectral space in the absence of any information fed in by the analysts. That's why it's called unsupervised. Analyst knowledge is used afterwards to attach information class labels to the map found by clustering. This is often gathered by the spatial distribution of the labels shown in the cluster map. Clearly, this is an advantage of unsupervised classification, as we will see in an example. In general, clustering algorithms are computationally expensive to run, compared with most supervised classification methods. We can benefit thematic mapping by bringing together the strengths of supervised and unsupervised methods into a classification methodology. Even though very old and devised when the maximum likelihood rule was the key machine learning classifier used in remote sensing, it teaches us a lot about operational thematic of mapping. We will demonstrate the method with a simple example. It uses the best aspects of the unsupervised and supervised approaches. Unsupervised clustering is used to discover the spectral classes using an image. Supervised classification after having associated those spectral classes with the information classes of interest, produces the thematic map. It is outlined in five steps on the next slide. The unsupervised supervised methodology developed by Fleming and his colleagues works as follows. Step one, we use clustering to determine a set of clusters or spectral classes into which the image results. This is performed on a representative subset of data. Spectral class properties, that is, statistics can be produced from this step. Small classes that might be overlooked by the analysts, such as those consisting of mixtures along class boundaries, and elongated classes like rivers and stream systems, will be picked up. Step two, using available reference data, that is maps, air photos, the image itself, and even the special distribution of clusters in the cluster map, will associate the spectral classes or clusters with information classes. Frequently there will be more than one spectral class for some information classes because they might look different spectrally to the sensor, even though the analysts calls them by the same name. This is an important consideration. Step 3, we can then perform a feature selection to see whether all the features or bands need to be retained for reliable classification. Depending on the classifier algorithm used and the data set. This step is not always necessary. Step 4, we use the supervised algorithm to classify the entire image into the set of spectral classes. The hybrid approach was used initially with maximum likelihood classification, but it can be used with most classifier algorithms. Step 5, label each pixel in the classification with the information class corresponding to its spectral class and use independent testing data to determine the accuracy of the classification product. We now use two simple examples to highlight the value of using unsupervised classification as a means for identifying spectral classes and for generating the signatures of classes for which the acquisition of training data would be difficult. Those results are just then used in a supervised algorithm. To demonstrate the hybrid approach, we use an image segment recorded by the HyVista HyMap sensor over the city of Perth in Western Australia. It is centered on a golf course. The obvious cover types are water, grass that is the fairways in the golf coarse, trees, bare ground including bunkers, a clubhouse, tracks and roads. Apart from a few cases, the distribution of cover types suggests that it might be hard to generate training fields for all classes of interest. Five bands were used, band 7 which is visible green, band 15 which is visible red, band 29 which is in the near infrared and band 80 and band 108 both in the mid infrared range. These last two are the infrared maxima of the vegetation and soil curves midway between the water absorption regions. It was felt that they would assist in discriminating among the bare ground and roadway classes. The three heterogeneous fields shown on the image were used for clustering. Among them, they seem to include all cover types. The three fields were aggregated and the Isodata clustering algorithm was supplied to the combined dataset with the goal of finding 10 spectral classes. Note there are seven information classes in the image. The results for the three cluster regions are shown on the top right-hand cluster map. We can say that the spatial distribution of the clusters matches the spatial distribution of the information classes in the image which is as to be expected although there may be more than one cluster corresponding to each information class. In this slide, we showed the main vectors of each of the clusters found by looking at where the clusters lie in the image and by looking at the shapes of the spectral reflectance characteristics seen by the distribution of the elements of the cluster means, we attached the information class labels shown on the slide. Several important observations can be noted here. First, two of the information classes are each represented by two spectral classes or clusters. Secondly, one spectral class or cluster has been generated from the water vegetation mixed pixels along borders. Thirdly, elongated classes such as tracks and roads have been found as separate spectral classes. It would be a little hard to develop 20 pixels for those classes otherwise. For verification, this slide shows an infrared versus red bars spectral plot for the clusters found. The distribution of classes agrees with where they would normally lie in such a plot. Using the set of spectral classes found from clustering and their mean vectors and covariance matrices which are generated automatically by the Isodata algorithm, we get the symmetric map shown on the left-hand side of the slide. The right-hand, the medic map is colored. Sets of the clusters or spectral classes corresponding to each information class have the same color. Here we show the final thematic map alongside the image to allow a comparison. In this simple exercise, we did not choose testing data so it cannot report quantitatively on the map accuracy. We now look at another simple example of the combined unsupervised, supervised approach. It involves classifying an arid region of Australia employed for growing cotton by the use of irrigation from a nearby river. The task was to assess the area in hectares sown to cotton as a surrogate for the amount of water used. Field agronomists had assessed the hectarage of cotton crops in the region but required corroborative evidence. The image to be classified consists just of the visible red and the first of the two near infrared bands of a landsat multispectral scanner image, recorded in February 1991. Although the region is very dry at that time of the year, apart from the crops there is a gallery or riparian forest along the river, which provides another vegetation class. Supervised classification was carried out using the minimum distance classifier, a very simple algorithm. On the left of this slide, we see a near infrared image of the region to be analyzed. A test semi-image has been identified on which the results are to be evaluated. The Darling river can be seen in the image. The cotton fields are mostly in the test area, which is what in the left-hand image, indicating a high IR response as well and approximately triangular shaped crop is shown in the bottom southeastern corner of the image. This is cotton, as some other scattered fields. The full rectangular selections in the right-hand image subsets, along with our sample of the lower triangular crop were use to resolve the spectral space in the spectral classes by clustering. Here the simple single-pass clustering algorithm was used and each of the five of heterogeneous regions was clustered separately. The results are shown on the next slide. The results of clustering as shown here in bar spectral plot form. In terms of the means of the cluster centers found, there were 34 clusters in total, which were then rationalized down to the 10 shown. That was done by associating the clusters with information classes using black and white and color air photos along with photointerpretation of the image itself. Those 10 grouped spectral classes were considered to be adequate to differentiate the image into its main cover types and thereby avoid any errors of commission, which might lead to poor estimates of the area of the cotton crops. When the minimum distance classifier was applied to the test image, using the 10 rationalized spectral classes from the previous slide, it was found that the cotton crops accounted for 803 hectares. The field agronomists had estimated 800 hectares. In this exercise, it was not necessary to produce thematic map since the important result was the air of cotton in the test region. How can we ensure we have pixels in the training dataset representative of all the information classes in an image and does it matter? For example, consider the river and gallery forest class in the previous irrigated crop example. How can the user select beforehand a representative set of those pixels with which to develop one of the binary classifiers in a support vector machine? If the pixels for that class are not well differentiated and indeed that may be mixtures of water and trees, then hand selection of the training fields might be difficult. Again, the analyst may wish to consider using a hybrid clustering support vector machine approach. The cluster step as with the maximum likelihood and minimum distance classifier examples just presented, would be carried out on representative and heterogeneous parts of the image in order to generate a set of spectral classes with which to work. Those clusters could be aggregated into single information classes before application of the support vector machine in order to limit the number of binary classifiers needed, although in some cases there may be value in retaining some information class sub-sets that improves separability. By way of summary, unsupervised classification based on clustering can be a very effective way of generating training pixels and revealing spectral differences within information classes, that is, spectral classes. Often, more accurate supervised results will be obtained if sets of sub-classes are use for each information class. Rather than use reference data explicitly to train a supervised algorithm, it is used to attach information class labels to clusters. This is a traditional unsupervised classification approach. Even there the analysts might be interested in just a small number of classes, sometimes one, better results will often be obtained if all apparent cover types are represented in the classification, in order to avoid errors of commission caused by large classes spilling over into the small classes. Finally, because the convolutional neural network develops knowledge about a scene as a whole, and to particularly the special nature of information classes and implements exceedingly complex piecewise linear decision surfaces, it is unclear at this time whether a prior unsupervised clustering will help improve performance. Answering this question requires a careful assessment of the spectral reflectance characteristics of the cover types involved. 

### Module 3 Lecture 10: Other interpretation methods

We don't always have to use supervised or unsupervised techniques for pixel identification in remote sensing. Other approaches are used in practice. Most of our analytical focus in the course has been on Machine Learning. There are, however, a number of other approaches used regularly in practice, often founded on scientific as against data analysis principals. Lab searching using pixels spectra in hyperspectral images is one example of that, which we have already seen. One of the most longstanding and effective methods for assessing cover types recorded in a remotely sensed image involve indices, which are metrics computed from the brightness values of the pixels in sets of bands. For example, a simple vegetation index might just be the ratio of a near infrared measurement and the visible red measurement. The rationale for which becomes evident when we look at common spectral reflectance curves, such as shown on the bottom left of this slide. If we apply that vegetation index to all the pixels in an image, it will produce a map in which pixel brightness is correlated with a strong vegetation signature. The indices should be defined as functions of surface reflectance, which is implied by our reference to the spectral reflectance curves. But in practice, the formulas are often based on the digital numbers in the received image product. There are better vegetation indices, such as the two shown here. The most common is the Normalized Difference Vegetation Index or NDVI. Here we see an application of NDVI to know our AVHRR images for which the pixel size is one kilometer. This allows us to develop a continental scale vegetation index map, which demonstrates, in this case, the evolution of drought conditions over the past four years in Australia. The spring sequence is perhaps the most profound during the huge change in green biomass over the four year period. Many other indices are used in remote sensing, including those shown here, which allow compensation for the amount of soil reflectance in a pixel and those for monitoring water conditions, soil, green biomass and so on. Again, by way of summary, vegetation and other indices are the result of band arithmetic. That is, computing new values for the brightness value of a pixel from sums, differences, and questions of the originally recorded bands. Those new values emphasize properties of importance such as greenness. Secondly, used in time-series, indices can follow the development of important phenomena, such as the development of a crop or the evolution of a drought. Answering this question requires you to plot the equations of straight lines that result from the index definitions. For example, from the simple vegetation index, we have near infrared equals vegetation index times red, where vegetation index is the slope of a straight line through the origin. 

### Module 3 Lecture 11: Fundamentals of radar imaging

We are now going to change direction completely and spend the rest of this course looking at the technology of imaging radar as a remote sensing tool. In the first module of this course, we looked at the Planck radiation law. Remember, it shows how bodies at different temperatures emit radiation or energy. As seen on the diagram to the right of this slide, the earth at 300K is a low emitter by comparison to the sun or a burning fire. But let's focus on the curve for the earth itself in more detail in the next slide. Here we see the earth curve expanded. Look particularly at the right-hand side covering the microwave range and note that the emission from the earth at microwave frequencies is much lower than its emission in the visible and infrared range. Although there is a very small level of radiation from the earth at microwave frequencies, we can, for all intents and purposes, assume the earth is dark. We can therefore take advantage of that by irradiating the earth with an artificial source of microwave radiation, just as we use a torch or a flashlight at night when there is little natural light available. When we use radar to image the earth's surface, we do so to the side of the platform, whether that be a satellite, an aircraft, or a drone. The reasons for this will become clear soon. There are two things we then need to understand. First, we need to understand how the energy reflected back to the platform is representative of the properties of the landscape. Secondly, we need to understand how such an arrangement gives us spatial resolution in both the across track and along track directions. Just as with radio, television, and mobile or cell phones, clouds are not a problem with radar imaging. This is a huge advantage because clouds can completely stop remote sensing imaging at optical wavelengths. Also, since the platform carries its own source of irradiating energy, imaging can be carried out anytime of day or night. The energy radiated to the surface is not continuous, but consists of a regular sequence of pulses, as shown in this slide. Importantly, those pulses travel at the speed of light, which is 300 megameters per second or 300 million meters per second. Reflections from targets closer to the platform will appear back at the radar earlier than those from the targets further from the platform, as illustrated by A, B, and C in the diagram here. The pulses are actually the envelopes of a burst of microwave radiation at the frequency or wavelength of interest. Just as with optical imaging, in which the earth responds differently at different wavelengths, the same is true of radar. The frequency inside the pulse will determine the property of the earth that we are measuring. The pulse transmitted in the previous slide is actually repeated on a regular basis called the pulse repetition frequency. The rate at which the pulses are transmitted is synchronized with the forward velocity of the platform so that the strips of ground image by each of the pulses align with each other as shown in this diagram. This provides continuous coverage. It is actually the property of the antenna used to radiate the pulse that gives the so-called footprint on the ground. The angle with which the radiation is transmitted to the side of the platform is called the look angle. Its corresponding version on the ground is called the incidence angle. They will be the same for a low-flying platform and a flat surface. There will be slightly different, however, at spacecraft altitudes where the earth's curvature might be important and also for ambulating terrain. We now need to understand quantitatively, how the radar separates targets, or regions on the ground as different distances as from the platform. In the diagram here, we show two targets A and B separated in the direction and from the platform. We call that the ground range direction, in contrast to the slant direction which is parallel to the beam from the radar as shown. Suppose the two tablets are delta r apart, in the slant range direction. Given that the radiation travels at the speed of light c, and that it travels to and from the platform. The difference in time between the two echoes is given by delta t, which is twice the distance divided by the speed of light. If the pulse duration is tall, as in the previous slide, then the two targets can be resolved provided there are no closer together then c2 divided by 2. We call that limit the slant range resolution, because we cannot resolve between the pulses if they overlap. This is illustrated on the bottom of the slide. Of course, as remote sensing uses, we are interested in the resolution along the ground range direction, rather than that in the slant direction. From trigonometry we can see that to be the formula at the bottom of the slide, there is a sine theta in the denominator. From the last formula, we can make some important observations. First, there is no special resolution directly under the platform. That is, when theta equals zero. That is why the system has to be side looking. Some early aircraft radars of this type were called side looking airborne radars or SLARs. Secondly, the slant and ground range resolutions are independent of the altitude of the platform. That's an amazingly useful property. Thirdly, ground range resolution is a function of incidence angle, and thus will vary across the swath. It is best in the far swath where theta is largest and poorest in the near swaths where theta is smallest. This is opposite to that for optical sensors which have their best resolution closest to the platform. Just as we can see more detail in the near range when we look at the window of an aircraft. Finally, if the antenna radiated to both sides of the aircraft and a single receiver were used, then there would be a right-left ambiguity in the received signal. That can be circumvented using two antennas and receivers, but most often that is not the case. Most systems encountered in practice radiate to one side of the platform. Having looked at how we achieve resolution in the ground range direction, that's from the platform. We now need to see how we achieve spatial resolution along the track of the platform. Although a curious line it is called azimuth resolution. It is determined by the properties of the antenna used on the platform for transmitting and receiving the microwave pulses. In particular, it is the so-called beam width of the antenna in the azimuth direction that sets the azimuth resolution in the manner we will now describe. From antenna theory, the width of the beam transmitted by an antenna is directly proportional to the operating wave length and inversely proportional to the length of the antenna, as shown on the slide. That means that a longer antenna will give a narrow beam width on the ground and thus a smaller strip irradiated action. What we want to know now is the width of that strip because that defines the azimuth resolution. From the previous slide, we had the top equation as the beam width of the antenna expressed in radians, at a range of R subscript nought that projects onto the ground, l dimension of R subscript l meters. That is the width of the illuminated strip and thus the azimuth resolution. Note from this formula, the azimuth resolution gets worse with increasing slant range and better with increasing antenna length. At this point, it is worth doing a simple calculation as on the next slide. Consider an aircraft situation, the slant range is 2000 meters. The radar antenna is three meters long in the azimuth direction and the operating frequency is 10 gigahertz. That corresponds to a wavelength of three centimeters. Substituting those values into the formula, we see that an azimuth resolution of 20 meters results, which is acceptable. However, if the same radar is placed on a spacecraft with a slant ranges 1,000 kilometers, the azimuth resolution will be 10 kilometers, which is totally unacceptable. Clearly, a better method is needed to achieve good azimuth resolution at spacecraft altitudes. What if we use a short antenna and thus irradiated a broad region on the ground as shown in this diagram? Clearly, the azimuth resolution should be very poor. The length of the region illuminated in the along track direction is the slant range to the ground multiplied by the beam width of the small antenna, which is as shown in the equation on the slide. What happens, though, if there's more than one pulse transmitted while the point target is in view of that large right at beam? To answer that, consider the geometry on the next slide. Here we see the platform moving past the target. The radar just encounters the target when the target first comes into the beam and loses it when it leaves the beam. All the time the platform is transmitting pulses, which incidentally are called ranging pulses, and it's receiving echoes. By processing those echoes using a signal processing technique called matched filtering, we are able to make the length of the spacecraft travel look like a long antenna or aperture as shown in the figure. That travel distance is equal to the projected beam width on the ground of the real short antenna. The effective long antenna is called a synthetic aperture, leading to the name synthetic aperture radar or SAR for the technology. What beam width does a synthetic aperture create, and what footprint does that lead to on the ground? Remember the beam width in radians is the operating wavelength divided by the antenna length. For the synthetic antenna, that is as given by the formula in the center of the slide. The two in the denominator comes about because the transmission is from the antenna to the target and then back to the antenna. When projected onto the ground, this gives an along track or azimuth resolution as shown at the bottom of the slide. Truly an amazing result because it tells us that the azimuth resolution in synthetic aperture radar is directly proportional to the antenna length in the azimuth direction and not inversely proportional as before. We will look at the implications of that in the next lecture. By way of summary, we note so far, first that imaging at microwave frequencies makes use of the fact that very little natural microwave energy emanates from the Earth's surface. We can thus irradiate the surface ourselves with a source of microwave energy carried on a remote sensing platform. Secondly, as with optical remote sensing systems, the forward motion of the platform sweeps out a swath of recorded image data of the Earth's surface. Thirdly, imaging radars are side looking; range resolution is best at far swath, that is large incidence angles, and worse at near swath that is, with small incidence angles. Fourthly, good azimuth resolution is obtained by using a signal processing technique called matched filtering, that allows a small real antenna to be used on the platform but achieves a very high spatial resolution equal to half the size of the real antenna; this is called synthetic aperture radar. Finally, in SAR, both the ground range and azimuth resolutions are independent of the platform altitude and operating wavelength, which is the wavelength of the frequency burst modulated by the ranging pulse. Again, this is a significant practical advantage. In the first question here, your attention is drawn to the node to receive sufficient energy that the brightness of a pixel is of a measurable value and well above any noise that might be present in the system. 

### Module 3 lecture 12: Summary of SAR and its practical implications

Let's now summarize where we were in the last lecture and then note some practical implications of synthetic aperture radar. In this slide, we summarize the important geometric properties of radar imaging, including how the width of the recorded swath depends upon the system parameters. We know now how to resolve the landscape into pixels using radar. Our objective from this point on is to understand what the pixels tell us about the landscape. Along the way, we have to consider a number of related matters, including some further properties of electromagnetic radiation and sources on geometric and radiometric distortion in radar images. The first important point to take note of is that the wave form within the ranging pulse does not have a constant frequency, but chirps over a limited range about a center frequency. That chirp range is very small but nevertheless, it is a major determined for achieving high range resolution. Understanding that is beyond this series of lectures, but the book referenced provides the details for those interested. The chirp range is small enough, that we can regard the imaging wavelength as equivalent to that of the center frequency. A second practical matter and one which has significant implications for understanding the properties of radar images, is to do with the polarization of the incident and reflected waves. Power or power density is carried forward as a result of both an electric field and a magnetic field, that oscillates at right angles to each other and to the direction of propagation as illustrated by the field vectors in the left-hand diagram on this slide. There is a fixed relationship between the magnitude of the magnetic field and the magnitude of the electric field. They are always at right angles each other. Thus, we only need to consider electric field from now on. The direction in which the electric field vector points, defines the plane of polarization depending on the transmitting antenna used at the platform, the radiation incident at the earth's surface can be vertically or horizontally polarized as illustrated in this slide. The transmitted signal can either be vertically or horizontally polarized. Irrespective of the polarization of the incident radiation, the scattered signal can also have both horizontal and vertical components. Whether either or both of those components can be received, again, depends upon the properties of the receiving antenna. Generally, the same antenna is used for transmission and reception. Whether it can transmit and receive both polarizations will depend on its design. There are four possible radar configurations. Transmit horizontal and receive horizontal, which is called HH, transmit horizontal and receive vertical, which is called VH, transmit vertical and receive horizontal, which is called HV, or transmit vertical and receive vertical, which is called VV. Note that the first in the double letter convention is they received polarization, while the second is the transmitted polarization. Most systems are the HH or VV, and are called single polarization radars. Whereas more sophisticated radars accommodate all four configurations, and are set to be fully or quad-polarized radars. This slide shows a radar image example over the city of Darwin, Australia in two different polarizations, HH and VV. It was recorded by the ASAR C band imaging radar carried on the Envisat platform. Although much of the image seem similar in the two polarizations, there are some notable differences as indicated. In many cases, polarization is an important discriminator in radar imagery. Here is another Envisat ASAR example over the city of Kalgoorlie, in Western Australia. This time, one of the images is cross polarized as indicated. Again, note the differences between the lock and cross polarized responses. Note the comment on the bottom left-hand side of the slide. Monostatic radar, main choosing the same antenna for transmission and reception. Later we will relax that requirement, but for monostatic systems which are by far the most common, we assume the two cross polarized responses are identical. In this example, we see all four polarization configurations. The image was recorded by the TerraSAR-X Satellite Program. The color composite image was constructed by displaying the horizontally polarized image as red, the vertically polarized image as green, and the two cross polarized responses as blue after they have been added. Bringing all this together, we say that operational radars in remote sensing are defined by three important system parameters. The operating wavelength, the polarization, and the incidence angle, or range of angles. Ever since radar was developed during the Second World War, the range of wavelengths has been described by a letter designation. It is not unique, but the frequency and wavelength ranges shown here are the most usual in remote sensing. Of this set, L, C, and X are the most often encountered. You may wish to take note of the formula on the right-hand side of this slide, which relates operating wavelength in meters and frequency in megahertz. In this slide, we see the technical characteristics of a number of satellite and aircraft radar remote sensing missions. You can convert between wavelength and frequency, using the formula from the previous slide. We've seen that the radar resolution cell or pixel size in a displayed image product, is defined by the ground branch resolution and the azimuth resolution. The ranging pulses are chirps about the operating frequency or wavelength. Although that requires some sophisticated signal processing to resolve targets in the range direction, better sensitivity to weak targets is possible. The important parameters of an imaging radar are the wavelength, polarization, and look angle. Operating wavelengths or frequencies are described by letter designations, reputedly chosen as a means for discussing the operating frequencies of aircraft detection radars during the Second World War. These questions are designed to consolidate your understanding of the geometric properties of a radar image. 

### Module 3 Lecture 13: The scattereing coefficient

We now turn our attention to the properties of the earth itself that are measured by the various synthetic aperture radar programs. In focusing on the properties of the earth surface detected by imaging radars, we start by analyzing a more traditional radar situation in which we detect at point of target. We will then generalize that to distributed habits such as crops, forests, another ground cover types of interest in remote sensing. Consider the situation shown in the diagram in which we start with the concept of an isotropic radiator on the left hand side, which is a transmitter which radiates equally in all directions, a simple light bulb can be thought of as an approximation to an isotropic radiator. In the next couple of slides we will work through the details of this diagram. We look first at the transmission of radiation from the isotropic source to the target. The power transmitted. P sub script t spreads out over the surface area of a sphere at a distance are not the power density over that spherical surface is given by the first equation here, which is just the power transmitted divided by the surface area of the sphere. When a real antenna is used rather than an isotropic radiator, the power density towards the target is increased by the gain of the antenna. The gain is just a number that tells us how much better the antenna is in the preferred direction compared with isotropic behavior. It is dimensionless. Now we have to describe the properties of the target in such a way that we can use it in our calculations. We describe it by an artificial area Sigma, called its radar cross section, which is defined as a cross sectional area at right angles to the incoming beam such that the area intercepts power from the incoming power density and re radiates it isotropically. The power available for isotropic re radiation is given by the first equation on this slide. The power density then produced back at the platform as a result of isotropic radiation from the target is as given by the second equation. In order to describe the actual power received back at the right app platform after having been scattered from the target, we introduce a property of the receiving antenna called its aperture. Which is also an area at right angles to the incoming beam of power density and which extracts power from that theme as seen in the last equation n this slide, the aperture has dimensions of area. The aperture of an antenna is a property we've used when describing its ability to extract power from an incoming wave front of power density. That is, when receiving. An antenna as a receiver can also be described in terms of its receiver gain, G subscript are, which is related to the aperture of the antenna as per the top expression on this slide, if we use receive again rather than aperture, the equation for receive power from the last slide becomes as shown in the center of the slide here. This is a famous equation. It's called the radar range equation. Note that there is suit powered varies inversely with the fourth power of range. It shows explicitly as should be expected that the power level received at the radar is a linear function of the target property. The radar cross section. Note that the equation has been derived on the basis of a point target rather than the distributed landscape which we will consider shortly. By point targets do arise in radar remote sensing. Any isolated strong reflector smaller than the size of a pixel can behave that way. Examples include houses, isolated trees, and targets intentionally located in the scene to help calibrate the radar. We will see examples of those later. We didn't have to see how we can modify the radar equation to account for the imaging of distributed regions, such as natural grasslands, forests, crops, oceans and so on. To do that, we represent the landscape by a collection of infinitesimal elements, as shown in the figure on this slide, each has an effective area of DS and a radar cross section of the Sigma. On the average, the region. Is it really that cross section per unit area of the signature? Yes, this is called the scattering coefficient. It is given the symbol sigma superscript o and it has units of meters squared per meter squared. That means in principle, it is really dimensionless. But it is important in practice to maintain the seemingly strange units of middle square can be described as we will see later. We're now treat each of the infinitesimal resolution elements as though, it were a single isolated point scatterer and use the right our equation which were previously as in the equations shown here within integrate all those contributions over the resolution cell to get the received power from the whole element as saying. The received power is a linear function of the scattering coefficient as it was for the radar cross section before. This is important, because it means that the measured reserved power is in proportion to the scattering coefficient of a surface. The scattering coefficient is usually expressed in the log rhythmic units of decibels and its value is related to a reference level. In this case, the scattering coefficient of one meter squared per meter squared. Base 10 logarithm so used and it is common to refer to the scattering coefficient as sigma nought. Radar cross sections for point targets are also expressed in decidbels reference to one meter squared as seen in the second equation. The disability notation is very useful in practice not just because the scattering coefficient and radar cross section can vary over several orders of magnitude, but also because values can be computed easily as we see on the next slide. This slide shows a range of scattering coefficients expressed in normal form. And indeed, the form along with some sample calculations which shall have the day bay form can be used to compute knew values easily. Because we have radars with different polarization configurations, we add subscripts to the scattering coefficient to indicate which transmit receive configuration was used to measure it. For quad or fully polarized radars, the four scattering coefficients are usually brought together into a sigma nought matrix. In more advanced treatments of radar imaging, we like to refer to the transmit receive situation in terms of electric fields rather than powers and power densities. At derivations here, we're based on the concept of power transmission and reception. But as we saw when discussing polarization, the actual signal is carried as a combination of propagating electric and magnetic fields. Just as we expressed the power view in terms of scattering coefficients, we can do the same with fields and in particular with the electric fields involved rather than scattering coefficients within describe the scattering events in terms of the elements of the scattering matrix indicated here. Incidentally, this matrix equation tells us why the second subscript on the matrix element here and in all treatments is the transmitted or incident polarization and the first subscript is a scattered or received polarization. If they were the other way around, the matrix vector multiplication would not work. Summarizing this lecture. The transmitted right our signal is treated by assuming isotropic radiation multiplied by the gain of the transmitting antenna. Gain is just a number, which tells us how much better the antenna is compared with isotropic transmission in the direction of interest. The target is described by an area called its radar cross section, which is defined by assuming that the power scattered from the target is isotropically reradiated. Distributed targets are described by the radar cross section per unit area, which we call the scattering coefficient both radar cross section and the scattering coefficient are polarization dependent. Both radar cross section and scattering coefficient are usually expressed in decibels. Finally, we can also describe the scattering behavior of a target in terms of electric fields rather than powers. In which case, we define and use the scattering matrix of the target. The first two questions Yeah, are important in understanding why the scattering coefficient of the landscape image by remote sensing radars, is a function of look, or incident angle. The last question shows you that radar image in space, essentially on two inverse square laws applied at the same time. 

### Module 3 Lecture 14: Speckle and an introduction to scattering mechanisms

In this lecture, we want to look at a major consideration in terms of the perceived quality of radar images. They often look speckled in appearance and lack the crisp quality of optical images. In the next lecture, we are going to look at the nature of the scattering coefficient for various Earth surface cover tops. Before that though, we need to be aware of this unusual problem of speckle, how it arises, and how it can be traded. That's what we want to do in this lecture, along with an overview of what is to follow. As just noted, one of the most striking differences in the appearance of radar imagery compared to optical image data is its poor radiometric quality, that is caused by speckle. The figure here shows a portion from an image of a fairly homogeneous region in which speckle is obvious. Speckle is a direct result of the fact that the incident energy is coherent. It is assumed to have a single frequency and the wavefront arrives at a pixel with a single phase. What does that mean?. Incident sunlight that forms the basis of most of our optical remote sensing is not coherent. It is a collection of energy at the different wavelengths corresponding to the wavelength range of the image and band, and there is no relationship between the phases of the different components. The phase angle of a sinusoid, which represents a propagating electric field, is a relative quantity which describes its position in time. In the diagram on the left, the phase angle is described with respect to the time origin. More often though, we talk about the differences in phase between two or more sinusoids as shown in the diagram on the right. A direct consequence of the phase difference between two or more sinusoids is interference. When they add in phase, we get a sinusoid of twice the amplitude, as in the top right-hand diagram. When they add out of phase or have a phase difference of 190 degrees between them we get zero, as in the bottom right-hand diagram. In between, we get somewhere between those two extremes depending on the phase difference between the sinusoids as illustrated. How does that explain speckle? Typically, a pixel will be a set of a very large number of incremental scatterers. We saw that when we developed the radar equation. Their returns combine to give the resultant received signal for the pixel. Such a situation is illustrated in the diagram here. The next brightness of the resolution element or pixel will be the result of all the incremental reflections interfering with each other. An adjacent pixel will have a different set of interfering scatterers, but will have about the same average brightness value if both pixels are of the same cover type. If nothing is done to reduce the speckle in a recorded radar image, the speckle level will be too high to allow sensible interpretation of the image. Before radar imagery is released for use, it usually has undergone some form of speckle reduction. There are many ways to reduce the influence of speckle. Some quite sophisticated filters are used, but often just a simple averaging of several images of the same scene is used. But averaging will unfortunately reduce spatial resolution as well since averaging is smoothing. However, when the radar is designed, it is anticipated that averaging will be needed to reduce speckle, so the designed spatial resolution, usually in azimuth, is higher than needed. After averaging the azimuth and range resolutions approximately match as required by the user. The number of signals averaged in the terminology of radar imaging is called the number of looks. On the next slide, we see an example of speckle reduction by averaging. This is a simple made-up example, but it illustrates the point that averaging will reduce to notice periods or speckle while retaining the average value of the signal, that is the scattering coefficient. Before separate images have been generated from a distribution with a mean of 50.7 and a standard deviation of 15.05. When full looks are averaged, the mean is preserved but the standard deviation has been halved. The speckle variance, which is equivalent to its notice power contribution, has been reduced by a factor of four. In the next lecture, we start our trip with our radar scattering mechanisms. That is, how different Earth surface elements scatter energy in radar remote sensing leading to the formation of an image. Here we've summarized the situation so that we have an overview of what is to come. The most common scattering mechanisms that contribute to the formation of images in radar remote sensing are illustrated here. Unlike the case of optical remote sensing where, because of the very short wavelengths involved, scattering mostly occurs in surfaces. In radar, the situation is much more complex, including the possibility of scattering from elements within the surface. In summary, the actual radar signal is carried by electric and magnetic fields. We concentrate just on the electric fields since the accompanying magnetic fields are directly related to the electric fields. Electric fields are represented as sine waves, since that is how that propagate. As a result, sets of electric fields can experience constructive and destructive interference. The backscattered signal from a pixel is composed of a large set of reflected fields from the myriad of scatterers that compose the pixel. Those fields interfere with each other, the result of which is that adjacent pixels, even though from the same cover type, may have different brightnesses. That causes the image to have a speckle appearance, even though adjacent pixels might have quite different brightness values because of speckle, the average brightness of scattering coefficient over a set of pixels of the same cover type will be representative of that cover type. Speckle can be reduced by averaging, which doesn't affect the average value of pixel brightness but reduces the variance and thus the image will look less noisy. The last two questions here are included to add to your understanding about look averaging in radar remote sensing. 

### Module 3 Lecture 15: Radar scattering from the earth's surface

In this next set of four lectures, we will look at how radar energy scatters from the earth's surface. This treatment is important in understanding radar images and in many ways, is similar to the importance of spectral reflectance curves in optical remote sensing. We will look at surface scattering, volume scattering, strong or corner reflector scattering and scattering from the surface of the ocean. Along the way, we will discover some interesting and unusual facts about how microwave energy interacts with the surface. Consider a beam of microwave energy incident vertically on a surface as shown in the diagram. To analyze what happens, we have to work in terms of electric fields and we describe the properties of the surface in terms of electrical quantities, the dielectric constant and sometimes it's conductivity. Conductivity does not appear often, but dielectric constant is a material property that is central to radar imaging. The dielectric constant of air is one. Whereas most natural media have dielectric constants in the range of about one to 80 or so. The diagram shows three components of electric field, that incident on a surface, that component reflected from the surface and that component transmitted into the surface medium. Note the definition of reflection coefficient on the right-hand side of this slide for vertical incidents. It is determined exactly by the dielectric constant of the medium. We can also define a power reflection coefficient as shown and a transmission coefficient. We don't use transmission coefficient very often in remote sensing. If we consider dry soil at microwave frequencies, the dielectric constant is about four, said that the power reflection coefficient is about 0.11. That tells us that only about 11 percent of the incident power is reflected. The remainder is transmitted into the surface. The surface therefore, would look quite stark in an image. By comparison, for water at microwave frequencies, the dielectric constant is much larger, about 81, so that the power reflection coefficient is about 0.64, telling us that about 64 percent of the incident power is reflected. The surface therefore would have a light tone in a radar image at vertical incidence. Water is a strong determinant of radar scattering, because it has a major impact on the dielectric constant of a substance. The diagram on this slide shows its effect on the dielectric constant or soil. The imaginary part is related to the conductivity of the soil, which tells us how effectively it will conduct electricity. Now, consider oblique rather than vertical incidence, but still with a smooth surface. If a beam of microwave energy is obliquely incident to a surface, as shown in the diagram, the reflection coefficient becomes polarization dependent. The two formulas given here show that fact. One is the reflection coefficient for horizontal polarization and the other, the reflection coefficient for vertical polarization. Reflection from a smooth surface is called specular, since it is mirror-like, because the reflected field is away from the incident radiation, that is from the radar itself, the surface appears black in imagery. Examples of this include, calm water bodies such as lakes and smooth soil surfaces. Now, consider rough surfaces. Most real surfaces exhibit some form of roughness when being imaged by radar as indicated in the figures here. We now wish to understand the backscattering properties of rough surfaces. However, how do we know beforehand whether we should consider a surface as rough? Well, we use the Rayleigh criterion for that purpose. Note that the decision is related to wavelength, incidence angle and the variation in height of the surface. In the previous slides, we looked at scattering from smooth to approximately rough surfaces. Now, what about the other extreme? A surface that is very rough? Such a surface is called Lambertian for which the backscattering coefficient is as shown in the equation here. Notice that it is independent polarization. It is possible to derive scattering models for surfaces with roughness ranges between pure specular and Lambertian. But those derivations are beyond the scope of this course. We do, however, show results on the next slide. This slide shows three surface scattering models which indicate how the scattering coefficient varies as a function of incidence angle and surface roughness. It is important to have a feel for these types of behavior, particularly for how smooth and very rough surfaces behave. Surface scattering is sensitive to polarization. This graph illustrates the HH and VV lack responses of a moderately rough surface, and it's HV cross polarized response. Remember, this spins at the incident radiation was vertically polarized as it was in the VV case. But there were some horizontally polarized scattering, as well as the vertical component. When cross polarized responses occur, we sometimes say deep polarization has taken place. In this slide, we summarize in two diagrams, surface scattering behavior as a function of the common wavelengths found in radar remote sensing, as a function of surface roughness and as a function of surface dielectric constant. All wavelengths show strong dependence on the dielectric constant of the surface material. Given that dielectric constant is a direct indicator of moisture content. This shows the importance of surface moisture in effecting the tone of a radar image. Note that higher moisture contents give a bigger radar response. Variations in surface roughness show more strongly at the longer radar wavelengths. At X band, it might be hard to discern such changes. So the image will have a more uniform tone overall, irrespective of whether the roughness of the surface might vary across the same. But note that in general, a rougher surface will give a bigger radar response. This slide shows a classic radar image recorded by the short-lived Seasat satellite in 1978. The enhanced backscatter, that is, the lighter regions, resulted from increased soil moisture owing to the effect of a storm to the West and the subsequent storm cells that traveled to the North East both late on the day prior to image acquisition. In summary, the reflected field in radar for smooth surfaces is determined by the reflection coefficient of the air surface interface. Secondly, the reflection coefficient is a function of the surface material dielectric constant, which in turn is a strong function of moisture content. Thirdly, when viewed a obliquely as in a radar looking to the side, a smooth surface will appear black in imagery since there is little or no backscatter. Next, as surface roughness increases, the level of backscatter and thus the radar image tone increases. Backscatter from smooth surfaces is a strong function of incidence angle. Whereas backscatter from rough surfaces is a weak function of incidence angle. Apart from specular scattering, backscattering from rough surfaces will have a cross polarized, that is an HP or VH component, which is also a weak function of incidence angle. Finally, longer radar wavelengths are more affected by surface roughness than shorter wavelengths. The second question here, start thinking about the choice of Bragg law parameters for particular applications. 

### Module 3 Lecture 16: Sub-surface imaging and volume scattering

We now want to look at the intriguing possibility of imaging below the surface with radar, along with the mechanism we call volume scattering. Not all of the energy incident on a surface is scattered. As seen in the last lecture, some is transmitted across the boundary and into the surface medium. There it is absorbed by losses, usually very close to the surface. Sometimes though, the losses are small enough to allow significant propagation into the medium, so that the energy is then scattered by buried features, allowing those features to be imaged. The loss of power density with propagation into the surface medium can be described by a simple exponential equation in which the exponent is called the absorption coefficient. It is given by the second formula on the slide, in which we see it is a function of wavelength, the dielectric constant of the medium, and its conductivity via the so-called imaginary part of the dielectric constant. Based on those equations, we can make a number of important observations. If kappa, the absorption coefficient, is large then power density drops quickly with distance into the medium. Kappa is smaller for longer wavelengths, indicating that there is better transmission into the surface material at longer wavelengths. We now define the penetration depth delta as that value of r at which the power density has dropped to 1/e of its value just under the surface. The diagram in this slide shows the penetration depth at L band as a function of moisture content. Note that for very dry media, we can get penetrations of several meters. It is important to note that the signal doesn't stop after the penetration depth. Depth is just a convenient measure of how quickly it falls away. Objects several penetration depths below the surface can still return measurable signals to the surface, noting that absorption happens, of course, on both the forward, or incident, and backward, or reflected, paths. This is a wonderful example of the ability of spaceborne SIR to image below the surface of very dry sands. The top image is an optical color infrared photo, and the bottom is a shuttle imaging radar C composite radar images, with the color assignments shown. The radar energy has penetrated below the sand sheet to reveal an old paleo channel of the Nile River, as shown. Adjacent buried drainage patterns are also visible in the bed rock under the sand sheet. We now turn to the scattering of radar energy from volume media, such as tree canopies, shrubs, and sea ice. Those types of media contain many individual scattering sites that are harder to identify but which collectively contribute to a backscattered signal, as in the diagram shown on the slide. A simple model of this type of behavior has been devised by assuming that the medium consists of a cloud of water droplets. They are the scatterers. Known as the water cloud model, it gives the scattering coefficient of the volume medium as shown by the equation on the slide. Note that it is a function of the thickness of the canopy and the volume metric properties of scattering coefficient and extinction coefficient. Note also that the loss of energy from the signal is a result of scattering by the fine particles of water. We can use that model to simulate volume scattering behavior. The diagram here compares volume scattering with very rough surface scattering in which we see that volume scattering is even less dependent on incidence angle than the roughest of surfaces. Although many surfaces might show a small specular component near the origin for surface scattering, except for this very rough case, there is never any specular component with volume scattering. Here we make two important observations about volume scattering, one to do with polarization dependence, and the other concerned with the frequency or wavelength dependence of the volume extension coefficient. The simple water cloud model of volume scattering is based on a set of small spherical scatterers. It therefore generates no cross-polarized returns. In general, however, volume scattering is highly depolarizing, which means it causes a cross as well as a co-polarized signal. That is because the scatter is are not spherical but have geometric shapes, such as found with twigs and leaves. As with like or co-polarized behavior, that is HH or VV, the cross-polarized, HV or VH, backscatter signal is very insensitive to incidence angle, but is usually much lower in magnitude than the level of the co-polarized response. The extinction coefficient for volume scattering is strongly dependent on wavelength, meaning that energy loss through the canopy is also wavelength-dependent. If the canopy has low loss, then imaging can be performed of the underlying medium, such as the soil surface and trunks underneath the tree canopy, and the water surface in the case of sea ice. In general, tree canopy attenuation is almost negligible at P band, it's very low for L band, it can be moderate at C band, and is generally high for X band. Although not a very good image, this JERS-1 image of a forested region in Australia shows how much higher the volume response of the forest is compared with the surrounding grassland. Both are vegetated, but the grassland at L band is behaving more like a surface scatterer. So in summary for this lecture, radar energy can penetrate very dry surfaces at long wavelengths. The absorption coefficient and the depth of penetration are strong functions of moisture content. Volume scattering is a weak function of incidence angle. In general, there are both cross and co-polarized components of volume scattering. Volume scattering is in general stronger than surface scattering at longer wavelengths. The canopy extinction coefficient for volume scattering is a strong function of wavelength. And finally, canopies appear almost transparent at P band but look like strong opaque scatters at X band. The second question here concentrates on the relevance of penetration depth. Bowler first asks you to think about a scattering mechanism that we will meet in the next lecture. 

### Module 3 Lecture 17: Scattering from hard targets

We now look at some unusual scattering behaviors that nevertheless occur often in remote sensing. Because of the longer wavelengths involved, some materials we image with radar act as hard targets in that they are capable of reflecting a very large proportion of the incoming wave front and thus can appear very bright. Examples of hard targets include metallic elements such as wire fences. Recall the forest example in the last lecture with the adjacent pasture fields. Other hard targets are facets directly aligned to the incoming radar beam such as roof facets and corner reflectors formed by paired horizontal surfaces and vertical structures. Examples include houses, tree trunks, and ships at sea. Because they are such strong reflectors, they tend to dominate the response of an individual radar resolution cell or pixel so that we revert to radar cross section to describe their scattering behavior rather than the scattering coefficient. We will now look at each of these in turn. First, consider simple metallic elements, such as wires appropriately aligned. A wire fence for example can act as a strong scatterer. While the analysis is complicated, we can note that, one, if the fence wire is aligned exactly at right angles to the incoming radar beam, which we call zero incidence angle in this context, and the electric field is in the plane of the wire, then it will reflect the beam strongly exhibiting a high radar cross section. Secondly, if the fence wire is of finite length, then it will also reflect strongly at angles just off zero incidence. Thirdly, if the electric field is not exactly aligned with the wire, there can still be some sizable reflection, particularly if the wire is thick. Finally, if the length of the wire or another elongated metallic element is a multiple of half a wavelength, the element is then called resonant and will give an extremely high response if it is correctly aligned to the incoming beam. Let's look at facet reflection. The roof of a building or another planar structure facing the incoming radar beam is called a facet scatterer. Its radar cross section is given by the formula shown on the slide. We call this it's bistatic radar cross section because the scattering or reflection occurs at an angle two Theta away from the incoming beam. Most of the time in radar remote sensing, we are interested in the case where Theta equal zero so that the radar cross section becomes four Pi times the square of the area of the facet with the dimensions expressed as a fraction of wavelength. Now we come to a very interesting mechanism and one which occurs surprisingly frequently in practice in a number of different forms. It is a manifestation of dihedral corner reflector behavior. The figure on the left shows a dihedral corner reflector. If it is large compared with the wavelength, then it's radar cross section within three decibels over 30-60 degrees is given by the left-hand equation. Now look at the right-hand figure, which is meant to represent the side of the building. The wall of the building has a radar image in the ground plane but the length of that image will depend on the incidence angle as indicated in the figure. It's radar cross section is therefore different from that of a simple dihedral corner reflector because the horizontal surface is not fixed. If we have a standing structure such as a building, then the formula on the right-hand side of this slide provides the appropriate model to use in order to understand its appearance in radar imagery. We can use that same dihedral corner reflector approach as the model for a tree trunk in radar imaging. As shown on the left-hand side here, the trunk has an image in the ground plane. Theoretically, we could use the expression for the radar cross section of a cylinder to help develop a model for the tree trunk ground combination. However, there is a simpler approach. If the trunk radius Y is large compared with the wavelength,it is simpler mathematically to represent the cylinder as an effective flat plate, as indicated in the right-hand diagram, with the equivalent widths as shown. The radar cross-section of the trunk standing on the surface is then given by the equation on the right-hand side. We need to make two modifications to the last expression to allow it to be used in real situations. First, for a tree, the vertical and horizontal materials are not ideal reflectors but are real substances such as wood and soil. We can account for that by introducing the power reflection coefficients for the trunk, that's row subscript T squared and the ground row subscript J squared into the expression for the red at cross-section. Secondly, we have to account for the attenuation of inner canopy that envelops the trunk. We can do that by adding a two-way absorption term, giving as the final radar cross-section of the tree the last formula on the slide. We now have a complete model for the trunk in a thorough situation and can use it to model the backscatter response of a forest in radar imaging. The backscatter curves shown here were produced from the expression for a tree trunk acting as a corner reflector as a function of canopy loss and with the parameters indicated here on the slide. Note that this is the first time we have seen backscattered drop away at small incidence angles. That is because the reflection of the vertical structure in the ground plane shrinks away as the angle becomes smaller. The falling backscatter at the high angles is the result of canopy attenuation, as can be appreciated when looking at the values of the extinction coefficient. Because of the pathway involving the ground in trunk scattering, the radar cross-section is a fairly strong function of the ground dielectric constant, as seen in the formula we derived for the radar cross-section of a tree. It will change, therefore, with changes in the ground material. Particularly if the ground changes from dry soil to water, as when a forest might be flooded. Remember the dielectric constant at microwave frequencies of dry soil is around five or so, whereas for water it is about 81. Therefore, a forest with a water understory will appear considerably brighter than one with a dry soil surface. As shown here, it will be about five decibels brighter. We see an example in the next slide. This slide shows an image of a forest in Australia is across a river and which receives annual floods with snow melts further upstream. The bright response corresponds to flood water from the river under the forest as a result of the corner reflector effect. In this study, the change in backscatter from dry to flooded understory was found from the image to be about 6.8 dB compared with 5 dB, we might expect from the change in dielectric constant of the understory from dry sort of water. However, in this region, the dry understory is unlikely to be sufficiently smooth as to be modeled fully by surface reflection coefficient. Instead, the surface may be more Lambertian, meaning less forward scatter in the dry double bounce model, and thus a larger difference when flooding occurs. So in summarizing this lecture, note that strong scatterers include metallic structures appropriately aligned to the incident radar beam, along with facets and corner reflectors. Next, In many cases strong scatterers will dominate the response of a radar resolution cell. Next, size of houses, trees, ships at sea, and ocean oil rigs will all give strong corner reflector responses. Next, trace without a dominant trunk will not behave as strong scatterers. Next, tree canopy attenuation has to be taken into account when modeling forests with strong scatterers, but not when considering buildings, ships, and oil rigs. Finally, changes in the ground surface can be assessed from radar imagery, even with a canopy, because of the change in power reflection coefficient. The second question draws your attention to the equivalent of canopy attenuation. The third question asked you to pay attention to cylindrical structures like tree trunks. 

### Module 3 Lecture 18: The cardinal effect, Bragg scattering and scattering from the sea

Now look at some further unusual scattering behaviors, one of which is sea-surface scattering. This treatment will finalize our consideration of scattering by looking at some unusual mechanisms that occur because of the coherent nature of the incident radiation. We will look first at Bragg scattering, which occurs when there are periodic structures on the ground, and then we will look at sea surface scattering, which is in fact related to Bragg scattering. Just before that though, we want to look at another feature of the double bounce dihedral corner reflector model for buildings, particularly in urban and city regions where houses and buildings often occur in rows along streets laid out in a regular grid patterns. In one of the quiz questions in the last lecture, you were asked to consider what would happen to the response of a dihedral corner reflector if the incoming beam was not precisely aligned with the face of the reflector. This is generally not a problem for tree trunks as a result of their cylindrical nature, but for flat vertical structures, such as the sides of buildings, if the incoming beam is inclined as illustrated in the diagram, the backscatter drops very quickly. Thus, buildings will only stand out in an image if the direction of radar illumination is at right angles to the wall in the horizontal plane. This leads to what is called the cardinal effect, an example of which is seen in the next slide. The word cardinal here derives from the cardinal points on a compass, that is north, east, south, and west. Here we see a portion of a survey image acquired over Montreal, Canada in 1984 demonstrating the cardinal effect. The bright central portion of the image is where cross streets are aligned orthogonally to the incoming radar energy. Whereas the darker portions to the north of about the same urban density have street patterns that are not orthogonal to the radar beam. We now come to a non unusual scattering mechanism which had its origins in crystallography. The Australia father and son team of William and Lawrence Bragg received the Nobel Prize in 1915 for x-ray diffraction, which has the same basis as what we're now going to consider in radar scattering. The wavelengths that are used in radar remote sensing that is about 3-50 centimeters are not too different from some of the periodicities we see in the landscape, such as row crops. Consider the interaction of an incoming radar front with the surface shown in the diagram on this slide, which is periodic in the plane of irradiation. It could represent, for example, the cross-section of a newly plowed field. When this region is irradiated, there will be moderately strong reflections from regularly spaced parts of the surface as indicated. It is a set of those reflections which constitute the backscatter that occurs from the radar resolution cell or pixel. Note that some returns travel further than others, so when looking at reconstructing the composite pixel response, we have to convert those additional travel distances into phase angles. Thus, the pixel response is made up of a set of signals with different phase angles. Those phase differences will be related to the angle of incidence and the spatial wavelength of the surface periodicity. Sometimes the signals will add constructively, giving a brighter than average return, whereas at other times they will add destructively. We saw the same effect when we considered speckle but here the interference results from periodicities in the landscape rather than randomly distributed incremental scatters. Usually, when we considered the different mechanisms within a pixel which contribute to backscatter, we simply add the backscattered powers. With Bragg scattering, however, we add the electric fields, which then squares the backscatter power density, making the pixel much brighter than if the periodicity were not present. There is one important condition to all this, and that is that the row-like periodicity has to be aligned orthogonally to the incoming wave front, otherwise the phase reinforcement will not occur. When it does, the condition for constructive reinforcement is that the surface and electromagnetic wavelengths are related by the formula shown, which is called Bragg's Law, and the behavior is called Bragg resonance. Here we see an example of Bragg Resonance with circular pivotal irrigated agricultural fields in Libya. The strong returns are much likely associated with Bragg Resonance. The radar of illuminations from the bottom of the scene, so that plowed furrows running across the scene give the enhanced returns. We now turn to how scattering occurs from the ocean and work on Bragg scattering is important here. A flat sea will behave like a specular reflector and will appear dark in radar imagery. To receive measurable backscatter, the sea surface must be made rough by some physical mechanism. The principle means for surface roughening is the formation of waves. There are two broad types of wave, both excited by the action of the wind blowing across the surface, they are distinguished by the mechanism that tries to restore the water surface against the driving effect of the wind. Gravity waves depend on gravitation acting on the disturbed massive water to counteract the effect of the wind, their wavelengths tend to be long, typically in excess of a few centimeters. Capillary waves have wavelengths shorter than a few centimeters, and rely on surface tension to work against the disturbance caused by wind action. They appear to ride on the gravity waves. For both waves, amplitude and wavelength is a function of wind speed, fetch, that is the distance over which the wind is in contact with the surface of the water, and the duration of the wind event. The bottom left-hand diagram here shows the so-called energy spectrum of typical capillary waves. It peaks around the wave number, roughly aligned with C band radar. Importantly though, this diagram tells us that presence among the capillary waves are components, in a farrier analysis sense, that can match the wavelength of an incoming radar beam and thus cause Bragg resonance to occur. From the spectrum, we see there is more energy at smaller wave numbers, and from the expression for Bragg resonance, we see that for a given radar wavelength lambda, we can select a smaller wave number on the capillary wave spectrum if we make the incidence angle small. Thus, oceanographic radar imaging is usually best done with small incidence angles. These images, recorded by SeaSat and SIR-A over a region of the Californian coastline of Santa Barbara, illustrate the importance of incidence angle in sea surface scattering. The seawater detail is much better expressed in the 20-degree imagery than in the 40-degree imagery. Note however the terrain distortion, at 20 degrees and the consequent difficulty in assessing terrain detail. Note also the strong dihedral corner reflector responses from oil rigs in the channel, easily seen at 40 degrees but less so at 20 degrees. Finally, notice what happens in radar images of the ocean. If the capillary waves are damped, oil slicks attenuate, considerably, the amplitudes of the capillary waves, meaning that incident radar energy has nothing to couple into, making those regions on an image look dark as seen here. Note also the effect that ship wakes have capillary waves and thus on the radar response. Summarizing here; First, the dihedral corner reflector effect requires the incoming radar beam to be orthogonal to the reflecting elements. Secondly, the cardinal effect shows how the alignment of straight patterns affects strong reflections in urban zones. Next, Bragg scattering occurs when the earth's surface has regular periodic features, it is also a strong reflection mechanism. Next, sea scattering entails coupling of the incoming electromagnetic radiation in the radar beam with capillary waves on the ocean's surface, it is strongest at small incidence angles. Finally, oil slicks damp the capillary waves and thus considerably reduce radar backscatter from the sea surface. When looking at the second question here, keep in mind that radar reflections are separated spatially in an image if the returns for the radar happen at different times. 

### Module 3 Lecture 19: Geometric distortions in radar imagery

We now come to the last week of our course. In it, we will look at some final topics in radar, including some innovative applications and say something about how different remote sensing imaging data types can be used together. First though, we look at distortions in radar images. The first form of distortion is shadowing. While not exactly a distortion, it does affect interpretation. As with optical images, shadows are caused in radar when signal cannot be received from behind an object. Unlike optical image shadows, however, shadowing in radar imaging is absolute. For nadir viewing optical sensors, it is possible sometimes to detect measurable signals from shadow zones because of atmospheric scattering of incident radiation into the shadowed regions at fairly short optical wavelengths. Radar shadowing is likely to be most severe in the far range and for larger angles of incidence. Whereas it is often non existent for smaller incidence angles. That can be appreciated by looking at the diagram on this slide. The first real geometric distortion we encounter, is that to do with the changing range resolution across this width. It is called near range compressional distortion. Recall that the ground grains resolution is best at far range and poorest at near range. Thus, the near range pixels cover a larger region of the surface than the far range pixels, and yet both are made the same size in the display product as illustrated in the diagram here. As a result, near range features are compressed into smaller than realistic display pixels. We see the impact of that on the next two slides. Consider a region on the ground in which there is a square of grid like features, such as field boundaries. Within each of the square cells, there could be many pixels. Imagine also that there are some diagonal lines as shown. They could be roads connecting across field corners. The right hand figure shows how that region on the ground will appear in recorded and displayed radar imagery. Not only do the near range features appear compressed, but linear features at angles to the flight line appear curved. The combined effect is as if the image were rolled over on the near swath side. The image at the top of this slide shows the effect again, but with a real image recorded by an aircraft radar. The bottom image has been corrected. We now look at the distortion peculiar to radar imaging. It arises because objects are delineated in the range direction by differences in time delay. As a result of that, how would a tall tower appear in the range direction in a radar image? The radar echo from the top of the tower arrives back at the radar before that from the base because it travels a shorter two way path. That causes the tower to lie over towards the radar on the image. To emphasize that, we draw concentric circles from the radar. All points lying on one of those circles, will be at the same slant ranged from the radar, and we'll create echoes with the same time delay. By projecting the circle which just touches the top of the tower onto the ground, we see that the tower top and indeed the whole tower superimposed on ground features closer to the radar, than the base of the tower. This effect is referred to as layover. By comparison, in optical imagery, vertical objects appear to lie away from the imaging device, since they are superimposed on features further from the device than the base. We now look at relief displacement, which is similar in origin to layover. Instead of a tower though, consider a vertical feature with some horizontal dimension, such as the model mountain shown in the diagram. Using the same principle of concentric circles to project the vertical relief onto the horizontal ground plane, several effects are evident. The front slope is foreshortened, the back slope is lengthened. Together, these effects suggest that the top of the mountain is displaced towards the radar set. In ground range format, they give the effect that the mountain is lying over towards the radar. If you go back to the Santa Barbara image in the last lecture, that relief distortion is quite evident in the 20 degree Seasat image. If we knew the local height, then the degree of relief displacement can be calculated. In principle, therefore, the availability of a digital terrain map for the region should allow relief displacement distortion to be corrected. Not only is the relief displaced, but the brightness of an image is modulated by topography, particularly in mountainous regions. On front slopes, the local angle of incidence will be smaller than expected, and thus the slopes will appear brighter. On back slopes, the angle of incidence will be larger than expected, making them darker than would otherwise be the case. This slide shows an example of relief displacement in a mountainous region, again with 20 degrees Seasat radar imagery. Slopes facing the radar illumination direction appear bright, while those away from the illumination appear darker. There is rather severe terrain distortion evident in the small circle, most easily seen by comparing the top image with the optical image at the bottom. In summary, because the formation of a radar image depends on resolving time delays in the range direction, several unusual geometric effects happen, which are not apparent in optical imagery. First, the recorded image appears compressed at near range, as against far range for optical imagery. Secondly, tall structures lay over towards the radar. Thirdly, relief is displaced towards the radar. Remember also, the displayed brightness of an image is modified by relief, according to the scattering characteristics of a surface at different angles of incidence. Finally, if mountainous terrain where covered by volume scatterer, such as shrubland at C band, relief modulation of brightness would be less evident. The first three questions here should help consolidate your understanding of terrain distortions in radar imagery. 

### Module 3 Lecture 20: Geometric distortions in radar imagery, cont.

This lecture, we finalize our consideration of distortions that occur in radar image data. Let's start by looking at the material we have covered on geometric distortions in radar images Seetha. We can in fact make some general observations that our value in choosing look or incidence angles suited to particular purposes. First, for low-relief regions, larger look angles will emphasize topographic features through shadowing, making interpretation easier, for regions of high relief, larger look angles will minimize layover and release distortion, but will exaggerate shadowing. Relief distortion is worse for smaller look angles, low look angle missions such as Seasat designed, for oceanographic applications often produced distorted imagery of high relief terrain. Recall the slide of ocean features in lecture 18, which compared Seasat at 20 degrees with SIR-B at 40 degrees. Finally, from spacecraft altitude, reasonable swath widths are obtained with mid range look angles, that is about 35-50 degrees. For those angles, there is generally a little layover and little shadowing. Look angles in this range are good would for surface roughness interpretation. With this slide, we introduce a new right at in-between concept. Not unreasonably, we have been concentrating on imagery that is representative of the earth's surface. That means in the across swath direction, we have been concentrating on converting slant range resolution into ground range pixels. But that gives rise to strange features, such as near range compressional distortion, as we have seen. Instead of producing images on the ground plane, we could keep them in slant range format, as seen on this diagram. In such a view, image has range coordinates measured along the slant direction, rather than along the ground. A simple way to envisage the slant range product is to project it out to the side of the platform as shown. The advantage of slant range imagery is that it doesn't suffer the near range compressional distortion encountered when the ground range form is used. That can be appreciated by looking at the series of full concentric rings in the figure. The distance between pairs of which represent the slant range for solution of the system. The dotted curves illustrate that relief distortion occurs in both forms of imagery. Let's now think about how we might correct geometric errors in radar imagery. There are two broad approaches to geometric error correction, depending on the level of topographic relief in the region being imaged. First, for low-relief regions, in which shadowing and relief displacement distortions are not considered significant, near range compressional distortion can be removed readily because we can model its effect. There will be remaining areas in geometry caused by the same factors we saw with optical imagery, platform attitude and altitude variations in earth rotation and so on. They are corrected by the same techniques, and we will look at some aspects of that approach on the next slide. For regions of high relief, we have to account for severe errors like relief displacement. If we had available a digital terrain map of the region, it is possible to use it to model relief distortion since we know the mechanisms which give rise to it. An artificial radar image of the region can be created from the DTM, with shading added through the adoption of a surface scattering curve. The actual image is then registered to the artificial image, and remember the pixels in the actual image are placed in their distorted positions. The relief distortion effects are then removed from the real registered image by reversing the DTM distortions. The pixels will then had been placed on their correct DTM positions. When we looked at geometric distortion with optical images, we use control points and mapping polynomials. We can do the same with radar images. However, because of the presence of speckle, it is often difficult to locate naturally occurring control points to the required degree of accuracy. As a result, artificial control points are often created prior to recording the image data, by deploying devices that will give recognizable returns in the received imagery. The positions of those devices are usually accurately known, through GPS fixing, for example, so that image correction is assisted. Those artificial control points can be passive or active. We will now look at both. Although flat plates facing the incoming radar beam could be used to provide abroad facet-like reflection. They have to be aligned very accurately to ensure the maximum radar cross section is available. They use as control points, therefore is limited. Instead, trihedral corner reflectors are used. Having three faces, they have a large range of angles around boresight over which their radar cross section is within 3dB of maximum. In this slide, we see the radar cross sections. First, for a square trihedral reflector, which has a 23 degree beam angle, and secondly, a triangular trihedral reflector, which has a 40 degree beam angle. Sometimes corner reflectors are deployed in set patterns so that they are more easily recognizable in the recorded radar image. Knowing the radar cross section of the reflector, it can also be used for radiometric calibration of an image. Rather than relying on passive reflectors for control points and the calibration, active radar calibrated is can be used. They are simple transponders in that they have a receiver and a transmitter. The incident radar energy on reception, is amplified by a given amount and then re-transmitted. As a result, the return signal can be much larger than that from a passive corner reflector. One small problem with an active transponder, which in radar remote sensing is called an active radar calibrator or ARC, is that the inbuilt electronics will introduce a small time delay into the reflection. That means the calibrator will appear at a larger range in an image than it really is. That is handled by making up the time delay to a known amount, which thus gives a known range shift in the image, but which is easily corrected by subtraction. A variation on the ARC is to have different polarizations for the receiver and transmitter antennas, so that cross polarized imagery can be calibrated. They are then called PARCs or polarized active radar calibrators. Summarizing this lecture, we note first that shadowing in radar imagery is a problem in regions of high relief, the choice of incidence angle is significant in reducing shadowing, minimizing terrain distortion, and highlighting sea surface features. Mid-range look angles are good for land surface remote sensing. Small look angles are good for sea surface applications. Radar images can be presented in ground range or slant range format. Control Points for rectification of radar imagery can be synthesized using passive radar reflectors, such as trihedral corner reflectors. Finally, active radar calibrators can also be used for control points, and are also good for accurate radiometric calibration. That's the second question here, remind you about our treatment of the cardinal effect. 

### Module 3 Lecture 21: Radar interferometry

We now start a treatment of some innovative uses of radar imaging that are not available to us in the same form with optical imagery. But first, we summarize the benefits of radar imagery. As we have seen in this series of lectures on radar, remote sensing, imaging with radar brings a number of advantages, including that imaging can be carried out through Cloud cover, imaging can be carried out at any time of day or night, sea state features can be detected and mapped. Imaging is sensitive to soil moisture. For very dry surfaces and long wavelengths, subsurface imaging is possible. Floods can be detected beneath forest canopies. At long wavelengths, forests back scatter is dominated by trunks, allowing woody biomass to be assessed. At shorter wavelengths, green leaf biomass can be assessed. Finally, radar scattering mechanisms at different from but complimentary to those experienced with optical imagery, suggesting there are benefits in using the two imaging modalities together. Now, let's look at a very different style of operation. Because radar uses pure or coherent radiation for imaging, as against the incoherent sunlight used in optical imagery, we can interfere two radar signals intentionally, to achieve a remarkable new application called radar interferometry. It allows topographic mapping to be carried out and changes in topography with time to be detected. We have seen interference in radar before, as in the speckle created by the interfering backscattered electric field vectors from incremental scatterers within the pixel and of course, in inbred resonance. The relative phase angles of the fields cause the interference. Those phenomena though are natural scattering mechanisms that depend on phase difference. What we want to do now, is use the difference in phase between two radar signals deliberately. We will see that that will allow us to determine the height of a pixel, as well as its latitude and longitude, and how that height changes with time. The technique is commonly known as interferometric SAR or InSAR. Consider two radars operating side-by-side at the same altitude, irradiating a region on the ground at height, h above a datum. The horizontal distance between the radars, B is called the baseline. Because the two radars are at different slant distances to the target shown as R_1 and R_2, there will be a difference in their phases on reception back at the radars for a pulse that has transmitted from both radars simultaneously. That two-way phase difference can be shown to be given by the first expression on this slide. If we differentiate that phase difference formula with respect to the height of that region being irradiated, we get an expression for phase sensitivity, which is a function of the known system parameters. As an example, look at the interfering of two S_1 beams. Substituting the relevant numbers into the formula shows us that we get almost 10 degrees of phase difference between the beams for every one meter change in elevation. We can invert and integrate the last equation to get height as a function of the differential phase angle, in which alpha subscript IF is called the interferometric phase factor. The constant of integration can be found from the height and phase difference at a particular location if that is important. Thus, by finding the phase difference at each pixel position, we can determine the corresponding pixel height, allowing a topographic map to be determined. For completeness, we note that we can represent the two separate radar signals at position i j in terms of fields, as shown in the center of this slide. When they are received, the complex product is formed as shown. The angle at which is the interferometric phase difference. This type of signal is actually called the interferogram. There are, however, some practical difficulties with using phase. In a previous slide, we saw that the phase difference between the received signals is given by the formula shown in the first line of the slide. It shows that the phase difference varies because of a change in incidents angle. There are two mechanisms that affect incidence angle. The first is height variations, which is what we are interested in and the other is position across the swath, which occurs even for a flat earth, and effectively interferes with the height-dependent changes in incidence angle which we are trying to find. This is called the flat earth variation of phase difference. It needs to be removed from the recorded phase difference between the two radars, which is reasonably straightforward to do. The correction is sometimes called phase flattening. The second problem with phase, which needs to be compensated concerns its cyclic nature. That can be seen in the example on this slide, which illustrates how there is an ambiguity in interpreting the difference in phase between two sinusoids. Phase angle is modulo 2 pi, so even though the absolute phase might surpass 2 pi in practice, mathematically, it gets reset to the range between zero and two pi. Again, the effect can be compensated fairly easily before the image product that is the interferogram is produced. This process is called phase unwrapping. By phase flattening, and phase unwrapping as standard operations carried out when generating topographic information right to out imagery. Interestingly, if we have two radars irradiating the surface, there can be two styles of operation. The option exists for having both transmit and receive, or only one transmit and both receive. The latter style of operation, called the standard mode, is common when both radars are carried on the same platform. When two separate platforms that used or a single platform is used on successive orbits with a baseline between them that is called repaid pass interferometry, then the formal style of operation issues and it is called the ping pong mode. Having summarized the main spur which topographic detail can be mapped with radar interferometry, we present in this slide an example produced by the TanDEM-X topographic satellite mapping mission. This was acquired using two satellites operating in a standard interferometric mode, one transmitting and both receiving. This map of matter is a very good examples of the detail that can be obtained. Summarizing this lecture: Because SAR uses coherent radiation, which preserves the phase angle of the transmitted and received electric fields, it is possible to employ two radars side-by-side to detect terrain height. As a result, three-dimensional topographic maps can be created. Usually, for SAR, they are called interferograms. Phase variations associated with a change of incidence angle as though the earth was flat have to be removed when creating the interferogram. That is called phase flattening. The 2 pi ambiguity and phase must also be compensated for. That is called phase unwrapping. There are two styles of operation, the standard and the ping pong mode. The two radars can be on the same platform or onto platforms flying side-by-side. Alternatively, a single radar and platform can be used, but with images tagged on subsequent orbits, which are then interfered to form the interferogram that assumes that the landscape does not change during the repeat passes. These questions relate to three different aspects of SAR interferometry, all of which are important to the system designer, but probably less so to the use of the interferometric product, which would normally be calibrated in terms of height. 

### Module 3 Lecture 22: Radar interferometry for detecting change

Having looked at topographic mapping with SAR, we now explore issues for detecting topagraphic change. Suppose we now have two radars, one following the other in the same orbit or even as subsequent orbits of the same platform. And that they image the same region of terrain at times 1 and 2 as indicated in the figure. Imagine that between those two times, there has been a shift in topographic feature in the range direction as shown. That will show up as a shift in slant range as indicated. A two-way interferometric difference in phase will occur between the two acquisitions given by the formula shown here. It depends only on the change in slant range as a fraction of wavelength. For ERS-1 for which the wavelength is 5.6 centimeters, a change in phase shifter 180 degrees corresponds to a change in slant range of just 14 millimeters. At 23 degrees incidents, that corresponds to a lateral shift in the range direction of 5.5 millimeters. That is an amazing degree of sensitivity to change. Ideally, there should be no horizontal separation of the two radars in along track interferometry. If that is acheived, then from the work of our previous lecture, there can be no phase difference between the two returning signals resulting from unchanging terrain relief. In practice though, there will often be a horizontal baseline as well as a time separation which we call a temporal baseline. That means that the phase difference we're looking for associated with topographic variations between the two times can be obscured by the phase difference resulting from static topography. That latter has to be removed. Removal of the phase difference associated with static topography can be done if a digital terrain model is available for the region. That model can be used to synthesize pixel by pixel the topographic interferometric phase. That can then be subtracted from the total phase difference on reception, leaving only the phase difference resolving from topographic movement. An alternative technique is to have three radar acquisitions, two of which are used to create an interferogram from the static topography, that is a DEM. Again, that is used to remove the effect of topography in the phase difference of two other acquisitions. This latter technology goes by the name of digital interferometric SAR or DinSAR. This slide shows three common methods for achieving the temporal baseline needed for a long track interferometry. One involves two antennas on the same platform. Another entails using two passes of the same platform. And the third uses two platforms in the same orbit, one following the other's and so called TanDem operation. This is an early example of change detection using along track interferometry. It's based on two acquisitions of the ERS-1 satellite. Note the amazing sensitivity to the subsidence of Bologna. This is a second example in this case generated from the Japanese ALOS-2 satellites inbound SAR. It is the data of the 2019 California earthquake, again note the enormous sensitivity to small changes in the land form. And as a final example, we returned to the Mount Etna illustration. Now an acquisition by the TanDem-X satelite has been used with a topographic map produced previously with the Shuttle Radar Topography Mission, SRTM. To produce a map of lava flows from an 8th of April 2010 eruption of Mount Etna. The SRTM used interferometric SAR to produce global scale topagraphic maps. The SRTM DEM was acquired in February 2000 and the TanDem-X acquisition was in October 2010. The SRTM data was used to remove phase variations due to static topography. Allowing the phase changes associated with topographic change to be obtained from the TanDem-X mission. The left hand image here shows differential phase variations associated with topographic change. Then the largest phase variations are those associated with lava flow. On the right-hand image, the lava flow detected by a long track SAR interferometry has been overlaid on the topographic model generated by crosstrack SAR interferometry. This is a great example of why SAR imaging is such an important and useful technology. Nothing like this can be so easily achieved with optical imaging. Summarizing this lecture, we have first that topographic changes can be detected using a long track interferometry or ATI. ATI can be acheived by mounting two radars on a single platform such as an aircraft by using repeat passes of the same platform usually spacecraft or by flying platforms in TanDem. When using temporal phase difference, should it take topographic change the phase difference between acquisitions resulting from any horizontal baseline must be removed? ATI is very sensitive to small changes in topography with time. And finally, the website shown here gives some good recent examples of SAR interferometry. The last two questions here are two aspects of the same effect. 

### Module 3 Lecture 23: Some other considerations in radar remote sensing

Now, look at some other imaging modalities with radar imaging, expanding our understanding of the possibilities of this wonderful imaging technology. The form of radar imaging we have been looking at so far, is referred to as strip mode, because it produces a continuous strip of imagery, as with optical remote sensing platforms. Other image types are all sat possible, three of which are illustrated in these next three slides. Sometimes, the swath widths generated in strip mode are not wide enough for some applications. The ScanSAR mode, provides a wider swath, by breaking the imaging into cells in the across track and the along track directions. The antenna, used is steered electronically over the cells as shown in the diagram. Each cell, is treated as a mini swath, in terms of the signal processing carried out to form an image, composite swaths of several 100 kilometers, then become possible. The processing needed to produce a ScanSAR image, is more complex than with conventional SAR, because of the need to join the scanned cells, but that is manageable. If the antenna beam, does not look directly to broadside, but for example, looks forward, the system is said to have squint. That can happen if the platform yaws, in which case, it is unintentional or it can be the result of a planned maneuver. It leads to coupling of the range, and azimuth dimensions of the image and to geometric distortion. If the antenna is squinted forward and then steered backwards as it passes, and continues to irradiate a target, as seen in the left-hand diagram. Then, higher resolution of that target region is possible, at the expense of resolution over the rest of the domain, this is called spotlight imaging. The image of, Sydney, shown on the right hand of the slide, is a great example of spotlight imaging. In which exceptionally good spatial resolution is achieved. We will see that in the next slide, in which the region around the bridge is expanded. The image was recorded by TerraSAR-X and created by averaging three high resolution spotlight images. The images were recorded on descending parsers, and the look direction was from the East, which is the right, in the image. Here, we see the former image zoomed in to the vicinity of the Sydney Harbour Bridge. Although, there are three images of the bridge, and is that unusual, one of them is highly detailed. The form of construction is easily seen, highlighting the exceptionally good spatial resolution, achieved with the spotlight mode. Take note of the other comments on the slide, concerning the features of the bridge and the image. We now need to say something about the types of radar image products available. For those of you with an engineering, or physics background, this should be straightforward. Radar image providers, make data available in a range of formats. Some tailored to particular applications, apart from interferometric and the tomographic products we will see later. Most will be derivatives of two fundamental formats, single look complex format and scattering coefficient format. Remember, that radar antennas radiate and receive electric fields, even though we often discuss the overall radar operation in terms of power and power density. If the amplitude and phase angle of the received field is recorded, that is called a complex signal. It is also a single look, in the sense that multi-look processing, to reduce speckle is carried out after reception. Taking these two properties together, the data as recorded will be single look complex. Mathematically, after it has been processed to extract the ranging pulse data from the underlying carrier frequency, the received signal can be described by the equation shown. When providing the single look complex data to the user, the radar operator, converts the received field into the complex elements of the scattering matrix, shown here. In which the operating polarization, is indicated by subscripts, as we saw earlier. Most often, the user will acquire scattering coefficient imagery, from the operator. Single look complex data, is modified in two ways to produce that form. First, the field amplitude is squared to produce intensity, which is directly related to received power. Secondly, look averaging is performed to reduce speckle and to produce a resolution cell that is roughly square in shape. In the single look complex product, the resolution cells will often be highly rectangular in the anticipation of square cells resulting from look summing. Again, in a multi polarization radar, a matrix of scattering coefficients can be provided as shown. Here, we see a variant of the traditional remote sensing imaging radar. It is not strictly necessary to have the transmitter and receiver at the same location as we have done up to now. Instead, we could put them at different positions and on different platforms. Such an arrangement is called bistatic SAR, as against the monostatic SAR configuration that we have been using so far. Clearly, there must be some form of communication between the transmitter and receiver so that the ranging pulse delay from transmission to reception can be computed. But that is only a matter of a telecommunications link. The transmitter could be on a space platform and the receiver on an aircraft. Also, the system can use what we call transmitters of opportunity, such as the signals from navigation satellites, which also reflect from earth surface features. Interestingly, note that the sun is an energy source of opportunity, in optical remote sensing. Although the analysis is more complicated, shadowing effects are different for bistatic SAR, and Earth's surface features behave differently because just their backscatter is no longer important, scattering at angles different from incidence angle, then become important. We can generalize the bistatic concept, it is possible to envisage a system with many transmitters and receivers. Variously called multistatic or network radars, their contemplated use is more common in surveillance applications than remote sensing, but with the trend towards clusters of small satellites for remote sensing applications, simple multistatic constellations are readily envisaged. Note that each receiver in such a configuration receives scattered signals from the target as a result of every transmitter. We now come to the next generalization of the operation of imaging radar. Standard synthetic aperture radar generates images of the landscape in the two horizontal spatial dimensions with detail in elevation projected onto the two-dimensional plane. In that respect, it is similar to optical imaging. InSAR, using two radars deployed across end or along track, takes the next step and enables the mapping of topographic relief and changes in relief with time, but it does not permit discrimination of detail vertically, such as the internal structure of a forest. By appropriately utilizing several radars, usually with vertical separation, for example, as passes of the same platform on different orbits, it is possible to identify vertical structure with the technique known as SAR tomography, or sometimes TomoSAR. Tomography resolves vertical detail by employing a synthesized vertical aperture much as azimuthal detail is resolved using aperture synthesis in normal SAR. This slide shows a recent example of TomoSAR, which illustrates the vertical resolution of forest detail. Five radar acquisitions recorded by the German Aerospace F-SAR system on 10th of June 2014, were used to derive the tomographic information. The F-SAR system is fully polarmetric, but was operated at L band for this experiment. It can also operate at X, C, S, and P bands. The top left-hand picture shows a radar image. A white transect indicates where forest vertical detail will be mapped, the bottom left-hand picture is a radar map of the canopy top or ground as appropriate. The canopy top information along the transit is seen on the very bottom right-hand picture, that allows a comparison with the canopy vertical or tomographic detail shown in the top right-hand picture. We have now completed our coverage of radar remote sensing. By way of summary of this last lecture, note that widespread widths can be achieved using the ScanSAR mode. The spotlight mode allows higher than usual resolution to be obtained over selected target regions. Bistatic radars use two platforms for imaging. Multistatic radar involves several transmitting and receiving antennas, each receiver can detect signals that result from scattering from each transmitter. Vertical detail can be resolved within a volumetric or compound scatterer using tomographic SAR, and TomoSAR uses a baseline orthogonal to the direction of propagation. The second question here is particularly important, it involves a scattering situation often encountered in radar imaging when multiple bounces of the incident radiation can occur before it is returned to the receiver. 

### Module 3 Lecture 24: The course in review

We have come to the end of our three modules, in which we have looked at optical and radar modes of image acquisition in remote sensing, and their applications. In this final short lecture, we'll look at the course in overview and consider what might be possible if we use more than one imaging technology in combination. In summary, our course has covered: sources of radiation for remote sensing, the importance of the atmosphere, optical remote sensing imagery, which included means for correcting errors and means for analysis, machine learning methods, including feature selection, and sampling and accuracy assessment, and radar remote sensing imagery, including distortions, understanding radar scattering, bistatic and multistatic radars, and interferometry and tomography. Now, let's look at how we can use the different images types together. Bringing different datasets together to extract information unable to be derived from a single source on its own is called data fusion. In remote sensing, that term is often applied just to the registration step, which ensures that the various image types are co-registered, and then registered to a map. But in many fields, data fusion refers to the fusion of evidence so that joint decisions can be made. We now wish to explore that interpretation briefly in remote sensing. To see why that is important, consider what we could do with registered optical and radar imagery. One example might be to use machine learning methods to create a thematic map of land cover from optical imagery and then overlay that on a digital terrain model derived from radar interferometry so that topographic influence on land cover can then be assessed visually. Another example is to use thermal imagery to map temperature gradients in the water outflows from a power station while using optical data to map the surrounding land use. In many cases, such combinations of different data types can be done manually, but the more general case is interesting. For example, how can we devise machine learning techniques to combine the diagnostic capabilities of both radar and optical imagery to identify land covers or land use? In this slide, we look at some possibilities of what could be done using the two data types together to make joint inferences about cover types. On the left-hand side, we see the sorts of labels one might find with thematic mapping from optical data, along with those which might result from the analysis of radar imagery. On the right-hand side, we see what refinements of the label for a pixel might be possible if we use the two data sources in combination. For example, a pixel which exhibits the characteristics of vegetation in optical imagery, but behaves as a corner reflector in radar imagery, would most likely be a forested pixel. Similarly, a vegetated pixel exhibiting Bragg scattering in radar might indicate row cropping, and so on. How can we perform such combined inferences? One approach is to fuse evidence at the label level, much as humans would do. The method for analyzing each individual data type would be matched to that data. For example, a support vector classifier might be used to analyze a contributing hyperspectral image whereas a knowledge of the incidence angle and polarization dependencies of scattering types in radar might be used to analyze co-registered radar imagery. We could then use several different approaches to combine those recommendations, including sets of rules, such as the one illustrated here. Such rules arise from the knowledge of expert image interpreters. There are other approaches, including probably the use of convolutional neural networks. We still have to see concrete examples of how they can merge evidence from different data types to derive pixel labels. One prospect might be to operate CNN's [inaudible]. The first layer is to analyze the individual data types, and the later layer is to do the fusion. Time will tell. In summary, radar, optical, and thermal imagery, each provide quite different diagnostic tools for understanding ground covers, that is, thematic classes. Optical imagery is determined by vegetation pigmentation, cellular structure and moisture content, by soil mineralogy and moisture content, and biomaterials suspended in water and by the bottom material. Radar scattering response is determined by the geometry of the scattering elements on the earth's surface and by moisture content, via the other property of dielectric constant. Finally, one way of forming joint inferences from the results of radar and optical image analysis is to make logical decisions based on user knowledge of how ground covers behave in both imaging modalities. This question illustrates how expert systems, based on production rules, can be used to merge evidence from different data types. 

## Course Closing Comments

You have now completed all three modules of the course, and hopefully you have undertaken the quizzes and tests too. As a result, you should now be familiar with optical and radar remote sensing imaging technologies and have an appreciation of how they're used in practice. You should also have a good appreciation of the major methods for image analysis that are used in remote sensing, particularly for optical data. That includes both supervised and unsupervised approaches. If you are interested in pursuing radar remote sensing further, you will find that it has its own analytical methods as well. Methods that exploit the special nature of radar images and what they tell us about the landscape. You are now in the position to pursue a second more advanced course in the theory and technologies of remote sensing or a more in-depth treatment of machine learning, and artificial intelligence procedures if they are of interest to you. More particularly though, you can focus in detail on a particular application domain. Developing application specific knowledge in remote sensing by building on the treatment in this course, following, you will find some suggestions I've made for further reading. I wish you well in your endeavors, and trust you enjoy applying the tools and techniques of remote sensing in your own field. 

{% include links.md %}
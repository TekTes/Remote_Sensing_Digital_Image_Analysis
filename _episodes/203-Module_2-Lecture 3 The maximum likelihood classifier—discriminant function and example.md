---
title: Module 2 Lecture 3 - The maximum likelihood classifier—discriminant function and example
teaching: 
exercises: 
questions:

- "???"
objectives:
- "= = ="
keypoints:
- "- - -"
---
### Module 2 Lecture 3: The maximum likelihood classifier—discriminant function and example

In this lecture, we will develop some further aspects of the maximum likelihood classifier and provide an example. This is a slightly longer lecture because we have kept the material together in a related group of topics. In this slide, we take the decision rule we have been using up to date, and now substitute in that expression the formula for a multidimensional normal distribution. As we will see, some simplifications are possible. That mean, that we do not have to evaluate the normal distribution itself on each occasion when we wish to label a pixel. To make the results simpler, we take the natural logarithm of the product of the class conditional and probabilities. As is well known, the result is the sum of logarithms. Since the normal distribution contains an exponential term, taking the natural logarithm means the exponent is removed. As we will see in the last three steps on the slide. We have decided to name the logarithm of the product of probabilities as the discriminant function for reasons which will become clear shortly. When we examine the form of the discriminant function, we see that the first term contains no information that will contribute to discriminating among the classes. Therefore, we remove that term to give the slightly simpler version shown in the center of this slide. We now express the decision rule in terms of the discriminant functions. Which is a form used in remote sensing image analysis software. They are called discriminant functions, because they are used to discriminate among the classes. Now consider a simple example, we met this one briefly in the introductory material in module one. The image segment is a portion of a Landsat Multispectral Scanner image recorded in 1979. There are four obvious ground cover classes: vegetation, burned vegetation, which we'll label as fire burn, urban and water. The first task in a supervised classification is to select training data. That is, pixels whose ground cover labels we know. In practice, that may be an expensive and significant exercise, sometimes requiring site visits, the use of reference maps, photos and the log. But in this case we can see the groundcover types easily in the image, so that we can determine a set of training pixels by inspection. The numbers of training pixels per class are shown in the table. When the training data is presented to the classification software, the first output is a set of class signatures, that is the mean vectors and covariance matrices for the four classes, as shown in the table on this slide. As we noted in the first module, when trading the principal components transformation, the covariance matrices are symmetric. Once the classifier has been trained, it can be used to label all the pixels in the image as shown in the thematic map of this slide. Not only is a map reduced, but we also have a table of the numbers of pixels per class, which we can translate into areas shown here in hectares. Note that the training pixels represent only 7.5 percent of the scene. That is a benefit of supervised classification. By putting effort into labeling a small number of pixels ourselves through training, we gain the advantage of having the classifier label a much greater number for us. One of the benefits of classifiers, such as the maximum likelihood rule, is that the main factor elements represent the average spectral reflectance curves of the cover types as seen here. In those curves, we can identify many of the important properties of the scene, such as the loss of grain biomass through the fire and the fact that the urban zone is a mixture of bare surfaces and vegetation. An important consideration when using any supervised classification procedure is to know how many training pixels per class are required. For an N dimensional space, the covariance matrix has N times N plus 1 divided by 2 distinct elements. To avoid the matrix being singular, at least N times N plus 1, independent samples are needed. Fortunately, each N dimensional pixel vector contains N separate samples in each certain spectral measurements. Therefore, a minimum of N plus 1 independent training pixels is required. We usually try to obtain many more than that so that we can assume independence and get good reliable estimates, of the elements of the mean vector and covariance matrix. With the maximum likelihood classifier, experience has shown that we should look for a bare minimum of 10N training pixels per class, but desirably at least 100N training pixels per spectral class with more than that, if at all possible. For data with low dimensionality, such as multispectral images, the minimum of 10-100N pixels per class is usually easily achieved, but when we have to work with hyperspectral images, there can often be a problem with not obtaining enough independent training pixels. For example, if there are 220 wavebands, we should be looking for about 20,000 labeled pixels per class for good training. Clearly, that is often a difficult target to meet. If we wish to use a maximum likelihood classifier with hyperspectral data therefore, we have to use so-called feature reduction techniques to allow the data dimensionality beforehand, or else resort to other classification methods that don't require as many samples for reliable training. We will look at those procedures soon. This slide illustrates the point of not having enough training samples per class with increasing data dimensionality. The example is from an old paper, but nevertheless illustrates the problem quite well. It involves an exercise with 400 training pixels per class over five classes. As the number of bands or features increases, there is an improvement in classifier performance or generalization, but beyond about four features or dimensions, the performance drops because of the poor estimates of the elements of the covariance matrix. [inaudible] for five features we should be looking for at least 500 pixels per class if possible. This problem has been known in remote sensing for many years and goes under the name of the Hughes phenomenon. It is also an example of the curse of dimensionality, often referred to in the machine learning literature. When we commenced our discussion of classify methods, we sketched the positions of the pixels from a given class in the spectral domain. Effectively, what our classifiers do is place boundaries between the classes. Pixels from one class lie on one side of a boundary while pixels from another class lie on the other side. Intuitively, one would expect that simple geometric boundaries will be less successful in separating two classes of pixel, than boundaries of a higher geometric order. The way we find these boundaries for any classification algorithm is to find the locus of points in this spectral space for which the two relevant discriminant functions are equal. By doing so, this slide shows that the locus of points for the maximum likelihood classifier is quadratic. In other words, the boundaries between classes are high dimensional circles, parabolas, ellipsoids, and so on. If you go back to one of the slides in the last lecture, when we started our work on the maximum likelihood classifier, you will see those sorts of boundaries or decision surfaces in the diagram, showing three intersecting two-dimensional normal distributions. Here we illustrate the quadratic nature of the surface between two classes on the left and on the right the rather more complicated decision surfaces that can be obtained if you allow more than one spectral class per information class, which we will treat later. So in summary, we use the discriminant function to discriminate among classes. Supervised classification requires training using a small set of data, and the payoff is labeling of all the pixels in the scene. Because second order statistics are involved in the elements of the covariance matrix, it is often difficult to acquire enough training samples for the maximum likelihood classifier to get good estimates of the covariance. Some of these questions ask you to think about the shapes of the decision boundaries for the maximum likelihood rule under different condition 

> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
>  {: .solution}
 {: .challenge}

{% include links.md %}
---
title: Module 2 Lecture 7: The support vector machine—the classification step and overlapping data
teaching: 
exercises: 
questions:

- "???"
objectives:
- "= = ="
keypoints:
- "- - -"
---
### Module 2 Lecture 7: The support vector machine—the classification step and overlapping data

We now take the support vector classifier to the next stage of development by looking at the actual decision rule and how it can handle the practical case of overlapping classes. In this slide, we summarize how decisions about the class membership of an unknown pixel with measurement vector x is determined using the support vector machine. Although the decision rule shown in the center of the slide is general to all linear classifiers, in the case of the SVM, it is made simpler because the weight vector at the top, and that's the decision rule, is specified just in terms of the support vectors, that is, those that lie on the two marginal hyperplanes. Given that the Lagrange multiplier is Alpha_i and the support vectors had been found during training, the decision rule is simple and fast to apply. In this slide, we find the one remaining unknown, the offset W_N plus 1. It is most easily found by choosing a support vector from each class and substituting them into the respective marginal hyperplane equations. Given that we know the weight vector, we can find the value of W_N plus 1. To get a more reliable estimate, sets of support vector pairs can be chosen and the sets of values of W_N plus 1 sets generated can be averaged. We now have to address the next three steps. The problem with the SVM: assuming the classes are completely linearly separable, the fact that it is only a linear classifier, and the fact that it is a binary classifier. We now look at the first of those handling overlapping classes. For real data, it is highly unlikely that two classes will be linearly separated, as in our previous analysis. Instead, there's likely to be overlap as shown in the diagram here. To make the support vector machine able to handle such realistic situations, we need to introduce some slackness into the training process, by which we mean we cannot achieve the best separating hyperplane but probably one that is as best as we can do. The approach taken is to introduce a set of slack variables, Xi_i, one for each training pixel. Their role is to allow a softer decision to be taken considering the class membership of a pixel. Note the new version of the decision rule at the bottom of the slide. We choose values for the slack variables as shown on the slide. If there are zero, then the corresponding pixels are correctly located. If they are unity, then the pixel sits directly on the separating hyperplane. If they are greater than one, then the corresponding pixels are on the wrong side of the separating hyperplane. Otherwise, they lie on the correct sides of the separating hyperplane, but in the gap between that decision surface and the correct marginal hyperplane. Since the Xi_i are positive, the pixels located in the wrong class or in the gap between the marginal hyperplanes and the decision surface, their sum is a good measure of the total error incurred by poorly located pixels during training. What we want to do now is maximize the margin as before, but also minimize the error caused by poorly located pixels. We have a decision to make. Is it more important to maximize a margin or to minimize the class overlapping pixels, or should we seek a compromise? We can find a compromise by adding a proportion of the sum of the Xi_i to the norm of the weight vector as a new measure to be minimized, as seen in the equation at the bottom of this slide. However, that minimization has constraints on it. There are two constraints. Similar to the case of non-overlapping data, we seek to ensure that the argument y_i brackets W_t x_ i plus W_N plus 1, close brackets, minus 1 plus Xi_i remains positive. Remember previously this constraint did not contain the Xi_i. Secondly, we need to ensure that HXi_i remains positive as defined. Again, using the process of Lagrange multipliers, that leads to the Lagrangian at the bottom of the slide, which we now seek to minimize. As with the case for non-overlapping data, a numerical solution is used to find the two sets of Lagrange multipliers. However, beforehand, the user has to specify the value of the parameter C, usually called a regularization parameter. We will see how that is done when we come to the examples later. But once a value is given for C, the numerical solutions produce the required set of Alpha_i. The decision rule stays the same as before, but the Alpha_i will now not be optimal for a maximum margin, but will reflect the compromise between maximizing the margin and to minimizing the error due to overlapping training pixels. Effectively, what we are trying to do with the slack variable approach, is to recognize that there will be a decision surface which is the best choice in terms of minimizing classification error caused by overlapping pixels. Would some form of trial and error approach be acceptable as an answer to the last question? 

> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
> >    {: .solution}
> >    {: .challenge}

{% include links.md %}
---
title: Module 3 Lecture 3 - Feature reduction by transformation
teaching: 
exercises: 
questions:
- "???"
objectives:
- "= = ="
keypoints:
- "- - -"
---

### Module 3 Lecture 3: Feature reduction by transformation

In this lecture, we are going to look at what can be done when using data transformation as I feature reduction tool. This has been a common approach for many years, particularly using the principle components transform. Remember that the principal components transformation maps image data into a new, uncorrelated coordinate system or vector space, it produces a data representation in which the most variance is along the first axis. The next largest variance is along the second mutually orthogonal axis, and so on. The later principle components would be expected, in general, to show little variance. They could be assumed to contribute little value to separability and thus could be ignored, thereby reducing the essential dimensionality of the vector space, and leading to more effective classification. As noted earlier, this is a widely used approach. It requires a computation of the covariance matrix, but using the data set as a whole. So there is generally sufficient training data available to be able to estimate the global covariance matrics accurately. We will illustrate the approach, but working through an example. Consider the two class, two- dimensional example shown here. If you look at the distribution of the data by class, you can see even now that the two classes should be separable in an axis aligned at about 50 degrees to the original axis. In other words, the first principle component. Now, let's verify that. Using the standard formulas, the global mean vector and covariance matrix are as shown here. We then compute the eigenvalues of the covariance matrix and thus the corresponding eigenvectors. Note that the eigenvalues suggest that a large part of the data variance will be along the first principle component. From the eigenvectors, we get the principle components transformation matrix from which we generate the formula for the first principle component as a function of the original axis. That is shown plotted in the diagram along with the projections or components of the data onto that axis. As expected, the classes can't be separated in the first principle axis, meaning that the second axis can be ignored. Subsequent classification can therefore be based just on the first PC alone. Thus, in this simple example, the number of features has been reduced from two to one. There are some situations, however, where the principal components transformation will not work as a feature reduction tool, such as that illustrated here. Feature reduction using the principle components transform, only works if the classes are separable along a reduced set of principle axes, as in the example just considered. In the example here, we could not use principal components analysis for feature reduction because of the manner in which the classes are distributed. There is, however, a transformation that can be used in such a situation. It is called Canonical analysis. If we could find an axis, such as that shown in the bottom diagram then, again, we could represent the data with a single feature, rather than the two original bands. That axis is defined such that when the classes are projected onto it, they appear as far apart from each other as possible, while having the smallest spread. If, along that preferred axis, we define the '' among class variance"" and the average "within class variance" as shown in this slide. Then we want to find the axis rotation such that we maximize the ratio of those two variances. In order to find this expression and the actual transformation, we have to express the problem in matrix form, so that it applies to any number of features, we need also to express it in terms of the variances in the original features. In this slide, we define the key properties of importance when finding the so called Canonical axis. They include the global mean and the two variances expressed in matrix form as required. Now this looks a bit complicated, but it follows an approach almost identical to that of the principal components transformation. Let y equals day transpose x be the required transform that generates a new set of axis y. In which the classes have optimal separation. By the same procedure that was used for the principal components transformation, we can show that there within class and among class covariance matrices in the new y coordinate system can be expressed in terms of those in the original x coordinates as shown in the center of the slide. As with principle components, the row vectors of the transpose define the axis directions in y space. Let's small day transpose. Be one particular vector. Say that one that defines the first axis along which the classes will be optimally separated. Then the corresponding with in class and among class variances will be as shown on the bottom of the slide. We now set up the measure shown as equation A, which we have to maximize, that is, differentiate with respect to the axis direction D. We then solve the generalized eigenvalue equation shown subject to the normalizing constraint at the bottom. That solution generates the Canonical axis for us as we see in the analysis on the next slide. Here we see a two class data set which is not separable in the first principle component access. But will be soon to be separable in the first Canonical axis using the computations shown on this slide for the covariances. Substituting the covariants calculations into their characteristic equation at the top of the slide yields the eigenvalues of 2.5, four and zero. Having a zero eigenvalue tells us that there is only one Canonical axis in this example. Using the non zero eigenvalue in the characteristic equation at the top along with the normalizing condition leads us to the new axis shown in the equation at the bottom of the slide. That access is plotted on the original diagram in which it is saying that the two glasses are now separable using just one transformed feature. Feature reduction can be carried out by transforming the original spectral bands to a new coordinate system in which some transform bands can be discarded without affecting the separability of the classes. The principle components transform is the most popular method to use. Even though it is based on the whole data set, that is the global mean vector in global covariance matrix, provided the classes themselves are approximately distributed along the first few principal axis, it shows good results. If the classes do not distribute along the principal axis, then principle components analysis is not a good technique to use for feature reduction. It is better than to use a transformation associated with Canonical analysis. But principle components and Canonical analysis depend on covariance information. In the former, it is a global measure and thus easier to estimate accurately with limited training data. Whereas for Canonical analysis, class specific covariances are required and thus the method has limitations with high dimensional data such as hyperspectral imagery. These questions direct your attention to an alternative measure for computing class separability. 


> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
>    {: .solution}
 {: .challenge}

{% include links.md %}

---
title: Module 3 Lecture 4 - Separability measures
teaching: 
exercises: 
questions:
- "???"
objectives:
- "= = ="
keypoints:
- "- - -"
---

### Module 3 Lecture 4: Separability measures

This lecture traits the third approach to feature selection, in which we set up measures to assess the importance to a classification exercise of the original sets of bands. This is the more classical approach. It entails setting up measures that help us to evaluate the relative importance of each of the bands, allowing us to discard some when we want to undertake thematic mapping. Generally, these approaches depend upon a measure known as separability. It can be based on an assumption that the classes of interests, can be described by probability distributions, or it can avoid the use of distributions. Obviously, the first approach is restricted to image datasets with small numbers of bands. Because of the need to estimate the statistics of the probability distributions, such as the Gaussian model that is almost always use. It is the old problem of not having enough samples per class, in order to estimate the distribution parameters reliably, provided the number of bands does not exceed about 10, those methods should be suitable. The second set of methods was developed to help us and detect feature reduction with high dimensionality datasets, such as hyperspectral imagery. That are generally more complicated to develop but are more widely applicable. Measures of separability tell us how distinct or spectrally different two thematic classes are, remember, classes are defined by sets of features. Our objective here is to see whether we can use fewer features and yet still carry out an acceptable classification. We use separability, to tell us where the class is based on choice and subsets of features, are distinct enough to allow good classification results to be achieved. Generally, we adopt a process such as this. First, we start with the full feature subset and a separability metric, compute how distinct the classes are. Next, we then remove a feature or sometimes sets of features and say by how much separability has been reduced. If separability is not badly affected, we could live at that feature. Otherwise we keep it and test other features. We then benefit from reduced classification costs and avoidance of this phenomenon. But the search is an exhaustive one, requiring all features to be assessed for a full evaluation. Divergence is one of the earliest and simplest measures of separability. It is based on quantifying the overlap of two class distribution functions, as illustrated in the diagram on this slide. Suppose we have two classes, omega1 and omega2, that are separable in the two features, x_1 and x_2. Being separable means they can be easily mapped as different classes by a classification algorithm, leading to high classification accuracy. If we discard feature x_2 and return only x_1 as shown on the bottom of the slide, we see that the class is then overlap and thus cannot easily be separated with high-accuracy by our classifier using just that feature alone rather than the two original features. Divergence tells us the penalty we pay by way of loss of separation, if we reduce the number of features by discarding one or more. Without going into the derivation, the formula here shows the computation of diversions for a pair of classes, are shown to be modeled by Gaussian or multidimensional normal distributions. As noted on the previous slide, we use divergence to see how the similarity of classes is affected, if we remove bands. What we're looking for is whether the separability of the two distributions is made never simply worse by the removal of features. If it is, then those features are important when running a classification and should not be ignored. On the other hand, if removing a feature doesn't drop the separability by very much, then it could be removed when doing a classification without significantly affecting accuracy. Divergence has a number of important properties. First, it is always positive. Secondly, it is zero if the distributions are identical. Thirdly, it never decreases with the addition of features. That means it is never high, if features are removed. Another measure of separability, is the Jeffries-Matusita distance, which is again a measure of the average distance between two distributions, i and j, for normally distributed classes. It has the form shown, involving an exponential function of a distance measure called the Bhattacharyya distance. The exponential term will be seen soon to be very important. While JM distance bears some similarity to divergence, it's significant difference is the exponential term. To see the effect of that, we sketch on the next slide, how divergence and JM distance would vary as a function of the distance between two distributions measured by the differences in the main vectors. By looking at the formulas for divergence, and JM distance, the behaviors shown on the two diagrams here can be seen. Divergence increases without bound as the distance between the classes increases. Whereas the JM distance curve saturates. Which behavior is the more useful. If we think about how the probability of correct classification might change as the two classes move further apart, it is more likely they behave like the JM curve, as we will see on the next slide. That is because the overlap between distributions drops quickly at first and then approach zero. Once the distributions are well separated, and thus classification accuracy is close to 100 percent, further separation will have little effect on the classification result. Here we see the probability of correct classification compared with the behaviors of divergence and JM distance. This suggests that the JM curve is more realistic all because of the saturating behavior of the exponent in its formula. However, because JM distance requires a computation of matrix inverses of the sums of all pairs of classes, it is time consuming if it is to be used to check the average similarity of many spectral or information classes. By comparison, divergence is much quicker to compute because the inverse of each class covariance matrix is computed only once, even when checking the average similarity of a set of classes. A third separability measure was developed heuristically to take advantage of the inherent benefits of both divergence and JM distance called transformed diversions. It embeds the usual divergence expression into a saturating exponential function, so that its overall behavior emulates that of the probability of correct classification. It is also faster to compute than the JM distance. Transformed divergence is perhaps the most commonly used measure of separability in remote sensing when classes can be described by the normal probability distribution. As we noted earlier, that generally applies when the number of bands or features does not exceed about ten. One of the important uses of separability measures is to be able to assess beforehand whether the spectral classes are well enough separated to ensure a certain level of classification accuracy. While the actual performance of a classifier depends on many factors, including how well the analyst has defined the spectral classes, collected reference data and trained the algorithm. If the classes are not well separated then there is an upper limit on the classifier accuracy that can't be achieved. The formula shown on this slide provides an upper limit on the probability of correct classification in our two class case as a function of transformed divergence, and this is plotted on the next slide. Here we see the theoretical upper bound of classification accuracy versus transformed divergence, along with some empirical classification results computed from 2,790 trials. As seen, unless transformed divergence is close to its maximum, binary classification accuracy falls well short of the theoretical upper limit. We met clustering algorithms in the last module. They are the basis of unsupervised classification and form a component of a combined unsupervised or supervised classification methodology that we will visit shortly. One of the last stages in clustering is to evaluate the size and relative locations of the clusters produced. If the clusters are too close to each other in spectral space, they should be merged. Well, what does too close mean? If cluster mean and covariance data is available, we can make merging decisions based on a pre-specified transformed divergence. By establishing a desired accuracy level for the subsequent classification, from which the corresponding value of transformed divergence can be specified, cluster pairs with separabilities below that value should be merged. That completes our treatment of measures of separability based on distribution functions. In the next lecture, we will relax that requirement and consider separability measures that can be used when it is not possible to estimate second order parameters, such as covariances, that is usually the case with hyperspectral data. For any classifier, it is good to use as few of features as possible in order to constrain the classification time and thus costs. Feature reduction is important in maximum likelihood classification in order to avoid the Hughes phenomenon, in which not enough training samples are available to develop reliable estimates of the elements of the covariance matrix. Divergence, Jeffries-Matusita Distance and Transformed Divergence are three methods commonly used when classes can be modeled by probability distributions. JM distance performs well but it is time-consuming to evaluate, whereas divergence is faster to use but doesn't work well when classes are widely separated in spectral space. Transformed Divergence combines the benefits of both approaches. Separability measures allow us to place an upper bound on classification accuracy. Remember to consult the solutions if you are having problems with any of the quiz questions at the end of each lecture. 

> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
>    {: .solution}
 {: .challenge}

{% include links.md %}

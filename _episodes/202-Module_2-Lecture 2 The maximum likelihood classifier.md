title: Module 2 Lecture 2: The maximum likelihood classifier
teaching: 
exercises: 
questions:

- "???"
  objectives:
- "= = ="
  keypoints:
- "- - -"

### Module 2 Lecture 2: The maximum likelihood classifier

In this lecture, we're going to develop the maximum likelihood classifier. We will see that it has a couple of variance. At all tend to reduce to the same underlying principle. It is a value to recall that we modeled the data in a spectral space which has axes aligned with each of the spectral measurements of the remote sensing instrument. Here we use simple two-dimensional version which will be sufficient for later developments. But as a result of the mathematics we employ can be conceptually generalized to any number of dimensions. Importantly, we note that pixels of a given ground cover type tend to congregate together in regions of the spectral space. Although we admit the possibility of several spectral classes per information class, for simplicity in this development, we will, as we said earlier, assume that each information class can be represented by a single spectral class. Here we have shown a single spectral reflectance curve for each cover type such that there is a single point per class in the spectral space. In practice, of course, variations will exist as shown in the next slide. The natural variations in each particular cover type will exhibit as clusters of points in the spectral domain, as seen here. The locations of the classes are consistent with the spectral reflectance data and tend to group around appoint indicative of the main of the relevant spectral reflectance curve. The starting assumption for the maximum likelihood classifier is that the density of the cluster of the pixels of a given cover type in the spectral space can be described by a multidimensional normal distribution, sometimes called a Gaussian distribution. While this may seem a restrictive assumption, the normal distribution is convenient because it and its properties are well understood. And even if the actual density distribution is not normal, the normal model nevertheless works well. Also, later, we will represent a given class by a set of normal distributions, which helps in overcoming many concerns with using this model. What are the parameters of a normal distribution? They are the main position and standard deviation in one dimension. Generalizing to multiple dimensions, the parameters are the main vector and the covariance matrix. The mean vector describes the location of the density maximum or the center of the spectral class in the spectral space. And the covariance matrix describes the spread in pixel density about the main position. The covariance matrix is the multidimensional version of the variance of a one dimensional distribution. The formulas for the single and multidimensional distributions as seemed to have a similar structure as is to be expected. We now focus the formula for the multidimensional normal distribution on a particular class, class i. We indicate that by the subscript i to the main vector and covariance matrix, and further right at the probability itself as conditional on the fact that we are looking at class omega subscript i. We will adopt that nomenclature throughout this series of lectures. The class of interest will be indicated as omega subscript i. The probability of a pixel existing at position X in the multispectral space will be indicated as the conditional probability shown on the left-hand side of the equality here. And the class parameters, m subscript i and C subscript i sometimes collectively called the class signature will have subscripts indicating the particular class. Note that if we have training data available for that class, then we can estimate values for the components of the mean vector and the covariance matrix. We now assume a normal probability distribution for each class. Using training data, we estimate the means and covariances Of the classes. Once trained, we can then choose to allocate and unknown pixel with measurement vector X to the class of highest probability. Note that we are explicitly representing the probabilities as class conditional probabilities. And because we're choosing the class of the highest likelihood or probability, this approach is called maximum likelihood classification. In the diagram, note how the contours of equal probability segment the spectral space into class regions. There is however, a better, more general approach. He'll make an interesting distinction in conditional probabilities. At the top we show the class conditional probabilities. The set of conditionals tell us the probabilities of finding a pixel, a spectral location X. I with measurement vector X from each class. Consider now the second set of probabilities on this slide. That expression says that, given we are interested in pixels at position X in spectral space, what is the probability that the corresponding class is omega subscript I? This is a much better measure to use than the class conditional probability, since it focuses on the correct class for the pixel rather than the likelihood that there is a pixel from a given class at position X. That gives us another way of classifying unknown pixels. If we knew the full set of these new probabilities, then we could classify an unknown pixel with spectral measurement vector X by allocating it to the class of the largest of the set. We expressed that in the form of a decision rule as shown symbolically on the slide. The new probabilities are called posterior probabilities. The reason for which will become clear soon. The problem is, we don't now at the posterior probabilities, only the class conditional probabilities, which we have estimated from training unlabeled pixel data. The very famous Bayes' rule gives us a bridge between the two types of conditional probability. It is shown at the top of this slide. It introduces two you, probabilities P of ambiguous of i and P(X). The latter is just the probability of finding any pixel with measurement vector X. When we use Bayes' theorem in the decision rule of the previous slide, noting that the probability P(X) is common to both sides. It can be dropped out. That leaves the rule shown in the second equation, but what is the new probability P(m) because sub I. The probability P(m) equals sub i is the locker hood of filing a pixel from class ambiguous sub i anywhere in the image. That is, it is not X dependent. It is a property of the scene itself and is called the prior probability because it is the probability with which we can guess the class membership of an unarmed pixel without the benefit of the remote sensing measurements. For example, if we knew the rough proportions of the classes, we could use them to guess, supplies based on areas. In contrast, the P omega sub i given X are called posterior probabilities because they are the probabilities with which we assess the class membership of a pixel. After we have carried out our analysis using the information provided by the measurement vector X for the pixel. If we had no idea of the prize, we could assume that they were all equal. In which case the new form of the decision rule of the previous slide reverts to the one we started with, in which the decision is made using class conditional probabilities. That is, the distribution functions rather than the posterior probabilities. Again, this slide summarizes the important points that classes or pixel tend to cluster in multispectral space. We described as clusters by normal or Gaussian distributions. We estimate the mean vectors and covariance matrices of those distributions from available training data. We can make clusters. Decisions for pixels based upon a comparison of the probability distributions, or preferably the posterior probabilities, if we happen to know the prior probabilities. In the last question, you are asked essentially to draw Bayes' Theorem. It is based on the concept that the joint probability of two events happening simultaneously is not order dependent. 

> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
> >    {: .solution}
> >    {: .challenge}

{% include links.md %}
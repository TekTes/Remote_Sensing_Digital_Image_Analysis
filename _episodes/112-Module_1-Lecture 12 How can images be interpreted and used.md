---
title: Module 1 Lecture 12 
teaching: 
exercises: 
questions:
- "How can images be interpreted and used?"
objectives:
- "Understand the interpretation of images and how to use them"
keypoints:
- "Interpreting imagery visually - ie. by the technique called 'photointerpretation' - requires an image product to inspect. Since only three colour primaries can be used to form a display, the user has to select the best three bands for this purpose."

- "Almost-natural colour images can be produced  by using bands recorded  in the blue, green and red wavelength  ranges."

- "A common and very useful colour image is called colour infrared". It emulates the use of colour infrared film from the early days of remote sensing and photogrammetry.  A near infrared band is displayed as red, a red  recorded band is displayed as green and a green recorded  band is displayed as blue."

- "Although not treated here, most recorded images will be dominated by reddish hues. That can be appreciated  by looking at the spectral  reflectance cover types, in which both vegetated and bare/soil surfaces have high near infrared reflectances.  An image is usually contrast enhanced  beforehand  so that soils etc., take on a blue-green colour. See J.A,  Richards, Remote Sensing Digital Image Analysis, Springer, Berlin, 2013, p.83"
---


### Module 1 Lecture 12 How can images be interpreted and used?

We are now at a very important stage in the course. In the previous lectures, we have learned about how sensors are used to record images. We have learned about the nature of reflected sunlight and Earth thermal emission as sources of Earth data. We have noted how the atmosphere affects the transmission of radiation from the sun to the surface into the sensor, and how we can correct for image errors in brightness and geometry. Effectively, that means we're now at the stage of having acquired good quality data ready for analysis. The rest of our course depends upon having reached this stage satisfactorily. There are two very broad approaches that can be taken when endeavoring to interpret images recorded by a remote sensing platform. The first involves a person doing most of the work. A person skilled at interpreting an image visually is called a photo interpreter and the process overall is called photointerpretation. This is a long standing method for interpreting images that goes back to the days of a photo interpretation. The analyst builds up substantial knowledge of how to look at the various cues and clues in an image in order to extract meaningful information. The second part, which is much more recent, involves the use of sophisticated computer algorithms to help undertake an analysis. A humans still involved, but their skills are more in knowing how to get the best results out of the computer-based approach then being able to interpret the images themselves. When computers are involved in the interpretation process, the approach goes by a number of names including quantitative analysis, classification or machine learning. More recently, artificial intelligence methods have been contemplated for remote sensing image analysis. We will say something about that later in our course. We will now examine both photointerpretation and quantitative analysis in general terms, but the major emphasis of the remainder of this course is on the computer approach. Let's start with photointerpretation. A skilled analyst works for the color image product composed from the recorded image bands, or sometimes with the individual bands themselves displayed as black and white. Irrespective of whether black and white or color imagery is used, the analyst makes use of color or relative differences in brightness from band to band, and relationships between objects and pixels in order to carry out interpretation. Changes with time of features from one image to another in a co-registered set are also important clues for the photointerpreter. Because an analyst relies on understanding brightness and color, it is important that we know how color image products are formed if we want to appreciate the nature of the products with which a photo interpreter works. As you probably appreciate from your own knowledge of digital photography or color television, a color image is made up of three black and white images displayed on each of the three primary additive colors of blue, green and red. Keep in mind that an individual band of image data is always represented from black to white, that is lowest brightness to highest brightness. Even though it might represents same, the reflectance of the Earth's surface in the red wavelength range for example seen through a red filter. A challenge in remote sensing is to know which bands to display in which colors. We only have three display colors, but we could have 10s or 100s of bands of image data from which to choose We illustrate in this slide three different color images formed from different band combinations using data recorded by the high map hyperspectral sensor. Remember, you have to choose which bands to display in each of the red, green and blue color primaries. On the left hand column of images, we have chosen a recorded visible red band to display as red, a visible green band to display as green and a visible blue band to display as blue. The result is a natural color image where the vegetation appears green, and urban and other developed services take on a brownish or slightly bluish appearance. In the center column, we have created what is commonly called a color infrared image. It is based upon the choice of bands long used in color infrared aerial photography. A near infrared band is displayed as red, a visible red band is displayed as the green and a visible green band is displayed as blue. Although this sounds unusual, it gives a very rich color product in which healthy vegetation having a high near infrared response shows as red and bare regions show shades of blue and green, clear water generally displays as black. In the right hand column, we show a color image product made up from a set of infrared bands. As seen in the color arrays, there is a fairly good differentiation of different cover tops. Often an analyst will experiment with bands to display guided by their knowledge of the spectral reflectance characteristics of the Earth's surface covered tops of interest in a particular interpretation exercise. Are there any conventions observed when forming color image products? Well, the only real convention is that the bands are displayed in the same order of wavelength as the primary display colors. Therefore the recorded band with the longest wavelength is shown as red and therefore the shortest wavelength is shown as blue. Once we form the color image product, we have to work out how to interpret it. That requires reference back to the spectral reflectance characteristics of the different earth surface cover tops present in an image. On this slide we have shown where three bands green, red and near infrared are located on the reflectance spectra. When combined into the color infrared composite, vegetation appears as red, soil and urban regions appear as blue and green and water will appear as bluish black, depending on its depth and turbidity. There is however an intervening step, which you may have picked up by a careful examination of the spectral reflectance clues and the color infrared product. From the spectral curves alone, it looks like all cover tops should be predominantly reddish. Indeed, if the composite were formed from the raw data recorded by the platform, that will be the case. In practice however, we undertake what is called a contrast enhancement step in order to get the color product shown on the bottom right hand side of the slide. We will see how to do that in the next lecture. In the early days of remote sensing, with instruments only had three or four wave bands. Selection of which three to use when forming a color composite product was quite straightforward. With modern instrumentation however, the analyst is faced with a more complicated task to choose the bands to display. In some ways, this places more dependence on the analyst's knowledge of the spectral characteristics of the same properties of interest. These quiz questions are important in terms of drawing your attention to the limitations of a human analyst, limitations which we expect will not be a problem when you're using a machine to assist with the interpretation. 

> ## Quiz
>
> 1. Would a human photointerpreter generally examine an image at the individual pixel level? 
> 2. A 1000x1000 pixel image has 1 million pixels.  Even if the analyst chose to look at individual pixels why would that not be effective?
> 3. Discuss the limitations on human interpretation of an image that contains 100 bands, with each pixel having a radiometric resolution of 8 bits.
> 4. What sorts of features are best recognized by a human interpreter?
>
> > ## Solution
> >
> > 1. It could be done but, given the number of pixels in an image, it would be impracticable.
> > 2. Having to look at 1,000,000 pixels is not realistic. The analyst could not guarantee to use the same level of concentration on each pixel in such a large numbers.
> > 3. Again, it is impractical to think that an analyst could look at the properties of pixels over 100 different bands.  Further, a radiometric resolution of 8 bits means each pixel has 256 brightness values from black to white; a human can generally only discern about 15-20 different levels of grey.
> > 4. Spatially-defined features such as shapes, lines and edges.
> {: .solution}
{: .challenge}

{% include links.md %}
---
title: Module 2 Lecture 6 - The support vector machine—training
teaching: 
exercises: 
questions:

- "???"
objectives:
- "= = ="
keypoints:
- "- - -"
---
### Module 2 Lecture 6: The support vector machine—training

We're now going to build on the work we did with the simple linear classifier to develop the support vector machine. A technique that became very popular about 15 to 20 years ago and is still regarded by some as the benchmark classifier for hyperspectral imagery. For those who come from a remote sensing background rather than from the machine learning community, following the full development of the SVM can be challenging. For the same reason, we will not cover the full range of SVM theory in these lectures. But instead present just sufficient off the material so that the important points can be appreciated. For those interested in a full theoretical treatment, although from a generalized Machine Learning and not a remote sensing perspective. See the book by C.M Bishop Pattern Recognition and Machine Learning, Springer New York 2006. The support vector classifier has a similar objective to the minimum distance classifier, in that it is trying to find a hyperplane which optimally separates two classes of data. But it's approach is quite different. In the SVM, we look for the hyperplane which is midway between the two classes. We define it in terms of two marginal hyperplanes that just touch each of the classes as seen in this slide. There are several stages to the full development of the support vector classifier. It is helpful to review them at the outset so we know the direction we are taking in the overall development because the journey through the theory can be a bit tedious. If we're not sure what our objectives are, it is easy to lose our way. The first two steps shown here, one is to find the optimal decision surface on the assumption that the classes are linearly separable. The second is to accept the fact that most datasets will not be perfectly separable. That we will have to adjust our training process to accommodate the fact that there will be some class overlap as seen on the right-hand diagram. The third step in our development of the SVM will be to make an adjustment for data that is not linearly separable. We will then move on to the final stage, which is to accommodate multi-class as against binary dataset. The fundamental support vector machine algorithm is just boundary. Multi-class strategy is required if it is to work with remote-sensing problems that involve several data classes. Let's now look at the first step, finding the optimal position of the decision hyperplane. Again, it has the same mathematical form as a hyperplane for the minimum distance classifier. We now look at the marginal hyperplanes. It is helpful if we make their equations. Take the forms shown on the diagram in blue. We can do that because we can scale the parameters in the decision rule, which involves adjusting the offset. We now need an objective that will lead us to the best hyperplane. We adopted a goal of finding the hyperplane, which is midway between the two marginal hyperplanes, which are furthest apart. That is, they have the greatest margin between them. From vector algebra, we can show that the margin is given by the expression on this slide, where the norm of the vector W is the size of the bit vector. We use the Euclidean norm, which is the square root of the sum of the squares of the vector elements, as is well known. We do that analysis on the next slide. The margin is derived by taking the difference of the two perpendicular distances from the origin to the marginal hyperplanes as shown. Note that when we apply that formula to the marginal hyperplanes, the one on the right-hand side of the equal sign has to be taken into account as part of the offset or intersect as seen in the second, and third equations. Thus, in seeking to maximize the margin, we want to minimize the norm of the weight vector. However, unless we constrain that objective, we could make the margin as large as we like. But that will undoubtedly cause some of the pixel vectors to fall onto the wrong side of their respective marginal hyperplane. We have to introduce a constraint to make sure that does not happen. Such a constrained minimization or optimization problem can be carried out by the process called Lagrange multipliers, in which we set up our Lagrangian function, as in the next slide. The Lagrangian function that we wish to minimize is L equals 1.5 of the norm of the weight vector squared minus the sum over i of Alpha_i f_i. Where Alpha i all positive are a set of parameters called the Lagrange multipliers. The f_i are conditions, one for each training pixel that ensures that the pixels are on the correct side of their marginal hyperplane. We choose as the conditions that ensure all the pixels stay on the respective correct sides of the hyperplanes. That y_i brackets W_T X_I plus w_n+1 close brackets is equal to or greater than one. Or taking the one to the left-hand side, y_i brackets W_TX_i plus W_N plus one close brackets minus one is equal to or greater than naught. The binary variables y_i are indicators of the actual class for the -ith training pixel as shown in red in the middle of this slide. The y_i tech values only are plus one or minus one. Thus, the Lagrangian, to be minimized, is as shown at the top of this slide. We need now to find the weights in the hyperplane expression that minimizes this Lagrangian function. While we are trying to do that, the second term in the Lagrangian is trying to push up its value if pixels are on the wrong side of their respective hyperplane. The hyperplane needs to be placed such that that effectively doesn't happen. The mathematics now becomes a little tedious, but leads to some remarkable and important results. If you choose not to follow the detail, we will still summarize the important results at the end. To minimize the Lagrangian with respect to the weights, we have to differentiate it as seen here, making use of the fact that we can express the vector norm in the form of a dot product, that is w transpose w. The result shows us that we can find a set of weights provided we know the values of the non-negative Lagrange multipliers, Alpha subscript i. This tells us that the decision surface is found from the set of training pixels and their classes, as is usual in the training of any classifier. We also have to minimize the Lagrangian with respect to the offset W_n plus one, which we do on this slide. However, this gives us another interesting condition. Namely, that the sum over all the training pixels set Sigma_i of Alpha y_i is equal to 0. That also from our previous equation C, we get the helpful formula for the square of the weight vector norm as shown in the center of this slide. Our previous Lagrangian formula using the new equation D gives us the expression in equation E. After all this, we are now in the position to find the Alpha subscript i, the Lagrange multipliers. Remember, they are trying to make the Lagrangian large to keep the respective pixel vectors on the correct side so they're separating hyperplane. We now maximize E with respect to those Lagrange multipliers, which we do on the next slide. This maximization is complicated, so it is normally carried out numerically to generate the values of the Alpha_i. However, there is another constraint we can use to help us simplify the situation. It is one of what is called the Karush Kuhn Tucker or KKT conditions. It says that Alpha subscript i brackets y_i bracket W_T_X_i plus W_N plus one close bracket minus one close bracket is equal to naught. Which, when you think about it, is quite extraordinary, since it tells us that either Alpha subscript i equals naught or y subscript i, open brackets, W_T_X_i plus W_n plus one close brackets is equal to one. This last expression is true only for pixels lying exactly on one of the marginal hyperplanes, in which case, the corresponding Lagrange multipliers, Alpha_i, are nonzero. For training pixels away from the marginal hyperplanes, the expression inside the curly brackets is non-zero. The constraint can only be satisfied if the corresponding Lagrangian multipliers, Alpha subscript i are 0. Those pixels are therefore not important to the training process. It is only those lying on the marginal hyperplanes. We call those support pixel vectors since they are the only ones that support training. We now have values for the relevant Alpha subscript i. Thus, we now know all the variables that define the weight vector. Thus, once we have found W_n plus one, we can define the decision surface, the central separating hyperplane. Since the Alpha_i are 0 for all but the support vectors, we found that the weight vector expression is simplified to a sum over just the set of support vectors script S. Once we know the W_N plus one, the support vector classifier has been completely trained. We will see how to do that in the next lecture and look at the next steps needed to ensure that the support vector machine is a practical classification method. The process of minimizing the norm of the weight vector or maximizing the margin between the two classes, constrained by ensuring that the training pixels remain on the correct side of their respective marginal hyperplane, leads to the amazing but probably logical result that it is only those training pixel vectors that lie on the marginal hyperplanes that are important in defining the decision surface. They are the support vectors. In the last question, you will need to use a bit of imagination to come up with the distribution of training pixels. Set the perpendicular bisector of the line between the class mains does not always separate classes which are linearly inseparable. 

> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
>    {: .solution}
{: .challenge}

{% include links.md %}
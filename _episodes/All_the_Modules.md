title: Module 1 Lecture 11 
teaching: 
exercises: 
questions:

- "???"
keypoints:
- "???"


### Module 1 Lecture 4 How do we record images of the earth's surface?

In this lecture, we want to examine how images of the earth surface are recorded. 

In the last lecture, we looked at the types of platform that are used to carry remote sensing instruments. We now wish to look at the instrument technologies themselves, and those that are commonly employed. That is the focus of this lecture. 

Earth imaging could, of course, be carried out by mounting a simple digital camera on a platform. Today, that is often done with drones. 

Many of the earlier images of the Earth, taken by astronauts, are captured by film cameras. And in the first three of the Landsats satellites, a television camera was used. It was called a Return Beam Vidicon, and was one of two primary imaging instruments. 

Most remote sensing satellites today carry scanners of one form or another, to capture images of the Earth surface. They tend to make use of the forward motion of the platform along with some form of scanning, either across or along the path of the satellite, in order to build up an image of the landscape. 

In this lecture, we will look at the three most common types of image scanner. 

The line scanner. The earliest digital imager was a line scanner, a technology which is still in use today. It uses that mirror to scan across the path of the satellite, as shown in the slide, in order to focus strips of the Earth's surface onto a detector carried on the platform. 

By design, the rate at which the mirror scans and the forward velocity of the satellite, are arranged so that consecutive scan lines are adjacent to each other. And together form an image, as the satellite traces out its path. The mirror has to oscillate from side to side. 

In sensors like the Landsat multispectral scanner, upwelling radiation from the landscape is detected on the forward scan of the mirror. The mirror then resets to the opposite side of the satellite track, ready for the next scan. By contrast, the Landsat Thematic Mapper and its later variants, records data on both the forward and reverse scans. Sometimes, particularly in early aircraft sensors, a rotating mirror is used. This slide also introduces some important definitions. First, the width of the image recorded is called the swath. It is defined by the field of view, or FOV, of the sensor, which is the total angle over which it scans. The resolution of the sensor or scanner is called the instantaneous field of view, or IFOV. Again, this is defined by an angle which, when projected onto the surface, defines the pixel size in or the spatial resolution of, the recorded image. Most often the IFOV is the same in the along track and cross track directions. 

The mechanism described in the previous slide accounts for the scanning process. But most remote sensing imaging instruments are capable of recording in several wavebands simultaneously. With the line scanner, that is achieved by placing a dispersive element in the path of the radiation. That can be a diffraction grating or prism, the effect of which is to break up the incoming beam into individual beams, corresponding to the wavebands of interest. Those beams are then focused onto separate detectors, one for each waveband. 

The push broom scanner. Another common means for forming an image is to use a linear array of individual detectors, arranged across the direction of satellite motion. Usually, this is a CCD or charge-coupled device array. With one element for each of a series of longitudinal scan lines, that is, scan lines along the track of the satellite. Radiation falling on each detector is sampled in time, allowing pixels to be formed. The pixel size is defined by the detector's instantaneous field of view at the surface, and the rate at which the radiation is sampled compared with the forward velocity of the platform. No mechanical scanning is involved, apart from the forward motion of the satellite. The advantage of this technology is that more radiation can be gathered per effective pixel, allowing higher special resolutions to be achieved. Again, a dispersive device is incorporated to allow the simultaneous recording of several wavebands. There will be a separate detector CCD array for each waveband. 

Perhaps the most recent imaging technology, used on spacecraft and aircraft platforms, is similar to the push broom concept just discussed. But, instead of having a separate set of linear CCD arrays to record a discrete set of different wavebands, a two-dimensional CCD array is employed. The second dimension is used to replicate the effect of a very large number of separate linear CCD arrays. In this manner, the number of wavebands recorded simultaneously can be very large, typically up to 200 or so. Therefore, these sensors typically record simultaneously several hundred carriers of images of the landscape, ranging over the visible and infrared wavebands. As we will see later, that will allow a great deal of information to be obtained about the landscape being imaged. 

When the number of recorded wavebands is small, say 20 or lower, as for the two previous instruments, the devices are typically called **multispectral scanners**. When the number of wavebands can be in the order of 100 or so, as in this slide, the instruments are usually called **hyperspectral scanners**. 

Now that we have seen how images can be recorded, we can summarize their important properties when used for remote sensing purposes. Recall first the geometric properties of the scanner shown on the right-hand side of the slide. The IFOV of the scanner defines the spatial resolution or pixel size on the ground, whereas the FOV of the scanner defines its swath width. Also shown in this diagram are the typical units used to describe each of those properties in practice. On the left-hand side of the slide, we have summarized how an image, in remote sensing, is described. Overall, it is defined by the swath width in the cross track direction and the frame size in the along track direction. Most often, particularly for operational missions, the frame size is chosen to be the same as the swath width. The next important feature is the pixel size, which determines the level of detail about the Earth's surface being recorded. As indicated previously, we often refer to pixel size as the spatial resolution of the image. Clearly, we are also interested in the number of wavebands being recorded. As indicated here, the wavebands are all registered with each other geometrically. Finally, we need to know the number of brightness levels available for each pixel. For a black and white image, those levels will be graduations from black to white. In fact, any image recorded in a given waveband will effectively be a black and white image representing the levels of received radiation from zero to the maximum sensitivity of the detector. Later on, we will see how we display those recorded bands in color. The number of brightness values available per pixel is defined by binary arithmetic. For example, an 8-bit pixel is capable of representing 256 levels of gray, from 0 through to 255. Most current operational remote sensing images will have 8,10, or 12-bit pixels. The number of brightness values, often expressed as a number of bits, is called the dynamic range or radiometric resolution of the sensor. Although it is of interest to see how land scanners and hyperspectral instruments operate, the really important message from this lecture is the geometric characteristics of the sensor. That is, the instantaneous field of view, the field of view, and so on, and the properties of the image itself. These questions ask you to think a little more laterally about how images are recorded. In particular, the third question raises an important matter about the distortions in geometry that can occur for imaging systems with wide fields of view. 

### Module 1 Lecture 5 What are we trying to measure?



In this lecture, we want to look at the properties of the earth's surface that can be detected by remote sensing instrumentation. When we examine those properties, we will understand why certain weapons have been chosen in the design of sensors commonly used in civilian remote sensing missions. Once you have assimilated this material, you may wish to consult some of the many websites available, that include information on remote sensing programs and their sensors. As we develop further knowledge and light lectures, these websites will become valuable additional sources of information with which to illustrate the lecture presentations. The NASA website in particular is especially valuable. First, remember that most remote sensing missions record images of the earth's surface using reflected sunlight, just as you and I see the earth as a result of reflected sunlight. In remote sensing, though, wavelengths outside the range of human vision can also be used for imaging. But let's just concentrate on reflected sunlight. If the earth's surface was an ideal reflector, such as a sheet of white material, then all wavelengths would be reflected equally. Real earth surface materials, however, are not ideal reflectors. They selectively absorb sunlight at different wavelengths, giving the appearance of color in the visible range. For example, green vegetation appears to us as green, because in the visible range of wavelengths, there is a strong absorption of incident blue and red radiation from the sun, but less absorption of green sunlight. As a result, there is more green energy reflected from most plant matter. So that we see grass, trees, and other green vegetative matter as green, depending on their pigmentation, some plants are differently colored. For example, plants which appear red, strongly absorb incident blue and green radiation. But reflect a significant proportion of the incident red radiation from the sun. To develop a better idea of what satellite sensors detect, including in the ranges of wavelengths beyond human vision, we will now examine in a little detail the reflectance properties of earth surface materials as a function of wavelength. This slide shows how the three simple fundamental earth-covered types of vegetation, soil, and water reflect incident sunlight across just the optical range of wavelengths. Note that the vertical axis is calibrated in percentage terms and is expressed as reflectance. Note also that the range of visible wavelengths shown on the left hand end of the abscissa is quite small, compared with the full range of optical wavelengths. It is important to note that the amount of green energy reflected by green vegetation is really quite small. Nevertheless, it is larger than the blue and red reflections, meaning that we see most vegetation as green. Even that the energy levels are quite small. By contrast, in the near infrared range, vegetation reflects about 30% or more of the incident sunlight. If we were able to see in the near infrared region, vegetation would look much brighter than it does in the optical range. As we move to longer wavelengths still, we see that the vegetation reflectance curve drops away. It contains two large absorption dips resulting from the water content of the vegetative matter, depending upon the moisture content of the vegetation those dips can be very deep or quite shallow. If the vegetation is most distressed, the dips will be shallow to non existent. Most remote sensing instrumentation tends to avoid imaging in those regions. The water absorption dips also occur for soil, although, in this real example, the soil is quite dry, so that the dips are very shallow. Soil with a high moisture content will show dips comparable to those, in this case to the vegetation curve. The other interesting property about soil is that the reflectance curve rises steadily with increasing wavelength, in the visible and the infrared ranges. As a result, the visible color of soil is determined mostly by reflected green and red radiation, so that the color tends toward reddish yellow. That is particularly the case for clays and sand. Dark loamy soil will appear blackish, because there is high absorption at all wavelengths. The water reflection curve is dark everywhere, showing just small amounts of reflected energy in the blue, green range consistent without experience of watercolor. These three curves are typical of what we encounter in remote sensing, but they will vary considerably depending upon the moisture and mineral contents of soil, the moisture and pigmentation contents of vegetation, and the clarity and depth of water. If water contains a lot of sediment, for example, the water curve will be a combination of the reflectance of pure water, and that of the suspended soil matter. If the water is shallow, there can be reflectance from the bottom material. So when looking at cover types such as water, the remote sensing specialist needs to be aware that the reflectances observed can be mixtures of more than one pure material. In this slide we show how the wave bands of some common remote sensing satellite instruments are chosen specifically, so that they sample the most significant region, of the spectral reflectance curves of common earth surface covered hops. To consider a simple example, the Landsat ETM+ takes three measurements in the visible range, roughly corresponding to blue, green, and red. Another measurement just at the start of the near infrared range, and two broad measurements further out in the infrared, avoiding the water absorption dips. Sensors such as hyperion take a very large number of samples across the spectrum, with sufficiently fine spectral resolution that in principle, the full reflectance spectra could be reconstructed for each pixel. Remember, each pixel is sampled in all of the available light bands, so that the spectral information recorded by any of these sensors is done for every pixel in the image. Whenever you come across a sensor, have a look at the wave bands it operates with, and form an opinion as to its likely applications. If a sensor operates largely in the visible range, it is probably designed for surveillance, mapping, or urban studies. If it has wide bands in the near infrared, it is then good for mapping and monitoring cover types such as natural vegetation, crops, and forests. It operates further out in the infrared range, it is likely to be designed for geological studies as well. Since a number of important spectral features of minerals occur in the wavelengths up to about 2.5 micrometers. We are now in the position to introduce some further definitions. If an instrument records up to about ten wave bands, it is generally referred to as a multispectral sensor. If it records a very large number of bands say more than 100, it is often referred to as a hyperspectral sensor. At this stage, it is worthwhile re-examining other portions of the electromagnetic spectrum. And in particular, the region beyond the optical wavelengths. Remember though, we have to be mindful of the presence of atmospheric absorption, and can only reliably image over those wavelengths which correspond to atmospheric windows. As we noted in a previous lecture, there are good atmospheric windows in the vicinity of 10 micrometers. And in the very broad radio wave region of the spectrum. They are identified on the slide and we need to ask ourselves once again, whether those regions are useful for earth surface imaging. Let's look at the wavelengths in the vicinity of 10 micrometers first. There is a very famous law called Planck's radiation law, which describes how a body, a so-called black body radiates energy as a function of wavelength. The radiation emitted is also, importantly, a function of the temperature of the body. Planck's law is described by an equation, that allows us to plot radiation curves as a function of wavelength and temperature. In this slide, we show three examples of Planck's radiation law in practice. The first example is for the sun, whose surface temperature is taken to be 5,950K. The second is for the earth, which has a temperature of approximately 300K. The third is for an object with a temperature of 1,000K, which roughly corresponds to a burning fire. As seen, they have maximum radiation outputs at different wavelengths. It is important to note that the vertical scale is logarithmic, so that each editor spacing represents a power of ten or an order of magnitude. Also the sun curve has been scaled down to represent the reduction in radiation. Because of the distance it has traveled from the sun to the earth. The solar curve has radiation maximum at a wavelength of about one-half a micrometer. Whereas the earth's maximum emission is at a wavelength of about 10 micrometers. The two curves crossover a little below 5 micrometers, meaning that any observations of the earth's surface at wavelengths less than about four micrometers are dominated by reflected sunlight. Whereas emissions above about six micrometers are dominated by natural emissions from the earth itself. If we want to measure the natural energy emanating from the earth, which is a sensor with weapons in the range of 10 to 12 micrometers, near where the earth's radiation is at a maximum. That is the basis of so called thermal remote sensing, since we are effectively detecting variations in the earth's heat output. Although we have shown the curve as a simple continuous line, it does vary from place to place on the earth as a result of a property called emissivity. It is variations in emissivity that thermal remote sensing missions measure and map. If we extrapolated both the earth and solar curves of the previous slide out to the radio wavelengths, we would see that there is negligible emitted or scattered energy from the earth's surface at those very long wavelengths. In fact, there is some, and it can be detected with very low spatial resolution instruments. But in general, we assume that the earth is quiet at radio wavelengths. As a result, we can radiate the surface using an artificial energy source, such as a radio transmitter carried on an aircraft or spacecraft. A receiver on the platform detects scattered radiation, which is then used to create a radio or microwave map of the earth's surface. That is the basis of radar remote sensing. There are three important messages here. Number one, imaging can be done with reflected sunlight, thermal emissions from the earth itself, or via scattered radio waves or radar. Secondly, note the difference between hyperspectral and multispectral imaging. And thirdly, remember water absorption bands need to be avoided. When thinking about the first question, ask yourself how a burning fire front could be mapped against significant surface features such as roads, rivers, and forests. 

### Module 1 Lecture 6 Distortions in recorded images

Lecture 6, distortions in recorded images. Images are never perfect when recorded. It is the purpose of this lecture and those which follow to understand that the image is acquired by remote sensing missions contain errors compared with the region on the ground being imaged. We are familiar with image errors in our day to day life. When we take photographs with hand held cameras, we often encounter distortions because the subject or camera has moved, or because sunlight glints in from the side, or because the contrast and brightness of the image are poor compared with reality. Exactly the same is true with the images recorded by satellite sensors. Before we can make sensible use of those images, we need to understand the source of error, and distortion, and the means by which we can correct them to create an image which is as close to the actual landscape as possible. In this lecture, we will concentrate on the most important types of distortion that frequently occur in remote sensing imagery. Often they are corrected before the images are sold to the user. In your work, you may not be particularly aware of them. Nevertheless, it is important that we understand how distortions arise and the means by which they are corrected. In some cases, most notably when images are recorded using aircraft and drone platforms, you may have no alternative than to correct the errors yourself. An important reason for understanding the corrections of distortions in image geometry is that the techniques we employ for correction can also be used to register images to maps and images to other images. Finally, as with any computer processing in image analysis and remote sensing, if you understand the basis of the algorithms employed, you are in a much better position to understand their limitations and characteristics. Distortions can occur in the geometry of an image and in its brightness. We will look at each of those in turn, commencing with brightness distortions. Distortions in brightness are generally known as radiometric distortions. Radiometry, you will recall, describes the brightness levels in an image. As seen in this slide, we subdivide radiometric distortions into three subcategories: those caused by the persons of the instruments themselves, those caused by the shape of the solar radiation curve, that is the Planck radiation graph for solar radiation, and those caused by the effect of the atmosphere, and particularly how the effect of the atmosphere varies with wavelength, any radiometric characteristics that are wavelength dependent or cause relative brightness distortions among the bands. That is a critical concern because band to band relative brightness differences are what allow us to understand the spectral reflectance characteristics of Earth surface types. Recall the differences between the spectral reflectance curves for vegetation, soil, and water from the previous lecture. We don't want those relativities upset by radiometric distortions. Let's start by looking at the effect of instrumentation errors on brightness. We are particularly interested in the behavior of the radiation detectors that measure the upwelling energy in each band. As seen on the bottom left-hand side of this slide, the ideal radiation detector has a linear characteristic that indicates the magnitude of the electrical signal generated by a certain level of incident radiation. Ideally, the curve has to be linear so that variation of the output signal mirror those of the input signal. That is, of the input radiation level. There are two important detector properties described in this graph. The first is the slope, which is referred to as the gain of the detector. A higher gain detector will generate a larger electrical signal for a given input level of radiation. The second property is the offset, which indicates that the detector will generate a very small signal even if there is no radiation input. That is just a characteristic of electrical devices and is sometimes called dark current. It can be reduced by cooling the detector. If a particular imaging sensor consists of a number of detectors such as the ETM+ and the HRV family, it is important that their gains and offsets be matched as closely as possible. In practice, however, the situation is more like that shown on the bottom right-hand side of the slide. Although the drawing is exaggerated, the detectors will often have slight variations in their offsets, and slight differences in their gains. In the simple case of the Landsat Multispectral Scanner, there are six detectors per band. If there are noticeable differences in gains and offsets, it is possible to see its dropping in an image, which repeats every six lines. This slide gives an example of the land striping in the Landsat Multi Spectral Scanner image resulting from detector mismatches. You will see that the land striping is very noticeable in the water part of the image. How can we reduce the land striping distortion caused by a mismatch in sensor characteristics? A simple method is to compute the histograms of the signals recorded by each individual detector and then match the means and standard deviations of those histograms. We will not go into detail here, but it is a fairly simple process and is often employed in practice. When we get to image contrast modification in a later lecture, you will be able to see how this is done. When that procedure is applied, the corrected image on the bottom of the slide results. You will see that the striping has been reduced considerably. Next, we examine the variation with wavelength of the solar radiation curve. As seen here over the optical range of wavelengths, there can be a significant change in the level of radiation falling on the Earth's surface. That would suggest that the visible wavelengths would appear brighter than those in the infrared range just because of the level of sunlight and not because of the spectral reflectance variation of a given [inaudible]. Clearly, that solar variation needs to be corrected if meaningful spectral reflectance information is to be obtained in a recorded image. Correction can be carried out by using the ideal Planck curve for broadband sensors. For hyperspectral instruments, however, in which the spectral resolution is very high, the actual solar curve measured above the Earth's atmosphere needs to be used because the real solar spectrum contains fine spectral detail, notably Fraunhofer lines. Now let's look at the effect of the atmosphere on the brightness of the image. The atmosphere absorbs radiation on its path from the sun to the pixel and from a pixel to the sensor, as we saw in a previous lecture, but the atmosphere also scatters radiation directly into the sensor, as depicted in this slide. There is a third more subtle effect and that is scattering from the atmosphere onto the pixel via a diffuse and not a direct path, also shown here. There are other less obvious scattering pathways as well, all of which need to be considered when looking at the effect of the atmosphere on recorded image data. To make the situation simpler, consider just the two mechanisms shown here. We know that the direct path from the sun to the pixel and the pixel to the sensor suffers selective absorption. But what does the atmospheric scattering pathway do to the recorded signal? The answer to that question is quite complex, and depends on the size of the particles that make up the particular atmosphere through which imaging takes place. Here we show three different atmospheres. At the bottom, we have a clear atmosphere in which the only scattering elements are the molecules of the gases that make up the atmosphere itself. In the middle, the atmosphere also contains some smaller particulates such as mist. At the top, the atmosphere is foggy, containing large scattering particles of water. Scattering from a clear atmosphere is called Rayleigh scattering. It is strongly wavelength dependent, as illustrated in the graph, with blue light being scattered the most. The net effect is a blue coloring. That is why we see the sky as blue on a clear day. Scattering from a light haze is called Mie scattering. It is not as strongly dependent on wavelength as Rayleigh scattering, so that the effect is a bluish-white appearance. Scattering by large particulates is almost independent of wavelength, so that the atmosphere then appears almost white. That is why we see fog as white. These scattered signals superimpose on the direct path signal, from the previous slide. As you might expect, the short wavelengths are affected more than the long wavelengths. One simple means for compensating the wavelength dependent effect of atmospheric scattering, is to produce histograms of each of the recorded image bands. The effect of atmospheric scattering shows up in the histograms as offsets from zero, as illustrated in the histograms of this example. Recall that the shorter wavelengths are affected more. This is seen in the visible blue histogram in this example. As wavelength increases, the offset [inaudible] atmospheric scattering reduces as depicted. In order to correct for the effect of the atmosphere, the offsets are subtracted in each case, so that the image histograms are reset to start from zero. In fact, the offsets are subtracted from the brightness of every pixel in the image, which leads to the corrected histograms. Correcting for the effects of atmospheric absorption on the direct path signal usually requires a knowledge of the wavelength dependence of the various actual absorbers in the atmosphere. There are a number of quite comprehensive software packages that can be used for that task. This slide is an illustration of the effect of all atmospheric constituents on the spectrum of a vegetation pixel at the top. It also includes the distortion caused by the shape of the solid irradiation curve. The corrected version is at the bottom. Note that complete absorption due to water vapor cannot be compensated, but that is not a problem, because there is usually no significant information available in any case in those regions. As we noted at the start of this lecture, many of the distortions discussed will have been compensated in the image product repurchase, particularly from satellite missions. Nevertheless, as a remote sensing practitioner, it is essential that you have some background knowledge of what constitutes errors in radiometry so that you are mindful of what corrections have been carried out and what might be needed in the case of imagery you acquire, which has had no corrections applied. In the first of these questions, use your own experience, particularly if you have flown over regions of patchy fog and mist. The last question, in a sense asks you to build up what might actually be recorded by a sensor as against what we think might be recorded. 



### Module 1 Lecture 7 Geometric distortion in recorded images

We now come to the rather more important matter of geometric distortions in remote sensing imagery. This is important because images from aircraft and drones are often not corrected for these error. So you need to know how to do that yourself if geometric integrity is important. As we mentioned earlier, the techniques used for correcting errors in geometry are the same methods we use for making sure an image is registered to a non-map basis. There are many sources of geometric error, including the fact that the earth and platform are in motion during image acquisition. The fact that the platform can vary in altitude, attitude, and velocity, the fact that the earth is curved when seen from space, and the fact that the instruments themselves introduce geometric variations by virtue of their design. We make the assumption that all recorded bands are affected equally. Therefore, we apply our techniques to each band in an image separately, but only have to develop our correction methods on a single band. It helps for much of what is to follow. If we look carefully at how an image is built up from recorded pixels. The diagram in this slide shows a group of pixel or grid centers. The grid centers are spaced apart by the IFOV of the sensor and in this case, we have shown an array of L lines of M pixels each. What is the pixel? It is a sample of the earth surface equal in size to the IFOV of the sensor. A set of pixels is therefore just a set of those samples. The rate of sampling is arranged ideally so that when we form an image by laying down a record of pixels on the grid, the pixels join up with each other as depicted here. We need to keep this model in mind in everything we do now, with respect to image geometry. The first time this rectangular grid concept becomes useful is in understanding the error introduced into an image as a result of the rotation of the earth. The signal coming from a remote sensing platform is just a string of pixels for each recorded line of data. Knowing nothing better, we just lay them down on a regular image grid, as discussed in the previous slide. That gives the screen looking image on the left hand side of this slide. However, during satellite image acquisition, the earth is moving to the East, underneath the platform. In this case, image in the platform is moving down the slide. A part of the image that is recorded last was actually to the West when the recording of this image segments started, and has moved itself directly under the platform by the time the last line of pixels is recorded. Therefore, to make the image geometry reflect that of the actual portion of the earth surface being imaged, the second and last lines of pixels need to be shifted progressively to the West, as shown in the right-hand image. If we know the velocity of the spacecraft, the earth velocity of that location, and the size of the image, it is relatively easy to work out how much each line of pixels needs to be shifted by. We now consider a very unusual form of geometric distortion. It occurs when the scan of the sensor is very wide. In other words, it has a large FOV or field of view. The diagram on the left-hand side shows the relevant geometry. Notice that the area of earth image at the swayed extremity is much larger than that at nadir, that is directly under the sensor. Yet, even though it corresponds to a larger region on the ground, the actual pixel is displayed as the same size as the pixel at nadir, when used to construct an image. When those recorded pixels are placed on a regular display grid, the pixels towards the swath edges, even though displayed in the same size as those towards the center of the scan, cover a much larger ground region and thus appear compressed by comparison to those at the swath center. To illustrate what this does to an image, consider the ground scene on the right-hand side of the slide. Be careful here. The regular grid shown might represent a set of BigQuery size square fields, it is not the display grid. Image in the ratio of those fields consists of many pixels. The two broader diagonal lines might represent roads that cut across the fields at 45 degrees as shown. When the recorded pixels are laid down on the regular display grid, the effect is to compress the edges of the image as shown on the very right-hand side image. That is because the pixels at the edges represent a greater geographical region than those at the center, and yet are displayed as the same size. The visual effect is to give the impression that the image rolls off at the edges. The diagonal roads take on the S shape shown, so that sometimes this panoramic distortion is called S binned distortion. From satellite altitudes, sensors with a wide field of view are sensitive to earth curvature as seen in this slide. Without going into too much detail, the net effect on the image is to exaggerate the panoramic distortion of the previous slide. Now let's look at some of the errors caused by the instruments themselves. For an oscillating mirror scanner as seen here, the mirror has to slow down, stop, and reverse direction at the end of each scan. By definition, the scan has to be non-linear at the swath edges. To minimize the non-linearity, a scanning window is defined over which data is recorded and beyond which any other detected radiation is ignored. Clearly, the scanning window is chosen to exclude the extreme non-linearities at the scan edges. We now look at two other instrumentation problems. The first is related to the fact that some scanners take a finite time to cross the swath while a platform is moving forward during data acquisition. That means that the portion of the earth being scanned is further forward on the swath edge at the end of the scan. To correct for this effect, the display pixels had to be shifted progressively backwards towards the age of the image. The second error is called aspect ratio distortion. It is caused by the pixel sampling rate across the scan line not being synchronous with respect to the IFOV of the instrument. If the sampling rate is too fast, the pixels overlap. If it is too slow, the pixels have spaces between them. Again, when these pixels sequences are displayed on a regular grid, the effect is to expand or compress the image across the track. The last point in this summary demonstrates the expansion of the image across the trend caused by oversampling with respect to the IFOV of the instrument. This is a particular problem with the landsat multispectral scanner. When answering the questions in this quiz, you should keep in mind that the displayed version of the image almost mirrors what is happening on the ground in terms of geometric errors. For example, to correct for the effect of earth's rotation to Eastwards, pixels have to be shifted to the West. 

### Module 1 Lecture 8 Correcting geometric distortion

We now come to methods for correcting any errors in the geometry of a recorded image. This is a fairly large topic and it will occupy the next two lectures using two different approaches. Both methods for compensating or correcting geometric errors in remote sensing imagery are used in practice. The first involves setting up mathematical equations which describe the types of distortion and then using those equations to correct the distortions. That can be a complicated process if many different sources of distortion are of concern, but it is regularly used for systematic correction by satellite operators. It also depends on a good knowledge of the ephemeris, that is the velocity, position, and pointing with time of the platform. The second method accounts for all sources of distortion in a single operation. It avoids the need to model mathematically the various types of distortion, and instead, uses a mathematical relationship between where pixels appear in an image and where they should appear in a map. This is a very common approach and it has the added advantage that the map basis can be chosen according to the projection preferred by the user. We will illustrate the first approach by using the error introduced into an image by the rotation of the Earth during image acquisition. This slide shows the image lie down on a regular rectangular grid, which is known to be incorrect geometrically, and beside it, the version which has been compensated for this rotation. To start the process of correction, we need to define two coordinate systems: one for the image as recorded, and one for the corrected version. The coordinate systems have their origins at the top because the platform is assumed to be moving down the page as it acquires its image, this is equivalent to a satellite in a descending node. We use the coordinates u and v to describe the recorded image laid on the regular grid, and the coordinates x and y to describe the pixels in their correct positions with respect to the Earth's surface, i.e., with respect to the chosen map grid. Clearly, what we have to do now is form a mathematical relationship between the pairs of coordinates. We now set up two equations that describe the position of a pixel in the correct coordinate system as a function of the position of the pixel as recorded. Note that the line coordinate, v and y respectively, does not change, therefore, we get the second unchanged equation shown at the bottom of the slide. However, to compensate for Earth's rotation, the coordinate across a scan line has to be adjusted progressively to the West as we increment down the lines of the image. In other words, as well as being a function of u, x must also be a function of v, representing this shift. The coefficient Alpha describes the degree of correction to the West as the platform moves southwards. It is convenient to express the pair of equations in matrix form because that helps when we have to make several corrections. The value of Alpha depends on the platform orbital properties and the location at which the image was recorded. If we examine the geometric characteristics of other distortion types, it is possible to set up similar equations as shown on this slide. The next step would be to use these relationships to create a geometrically correct image, that requires inverting them to root u and v in terms of x and y. Because the technique we are going to treat in the next lecture is so flexible in terms of correcting any types of geometric distortion, we will not proceed any further with mathematical modeling. We have only reviewed the most significant sources of geometric distortion here, please consult some standard textbooks to see the very large range of mechanisms that can cause geometric errors in an image. The first question here has been treated in the lecture, the idea of the question is for you to derive the matrix form of the correction. The third question is particularly interesting. Later in this course, we're going to treat methods for attaching labels to pixels indicating what they represent in terms of ground cover types. This question asks you to consider whether the geometry of an image should be corrected before that labeling process is carried out, or whether the labeling should be done first and then the labeled pixels corrected for the geometric positions. 

### Module 1 Lecture 9 Correcting geometric distortion using mapping functions and control points

This lecture introduces a very powerful and common method for correcting errors in geometry. As noted earlier, the technique also allows us to register one image to another image, or an image to a map. In fact, by doing this registration, any errors are removed entirely. It is an alternative technique to the mathematical modeling approach of the previous lecture. And it is based upon two principal requirements. First, we assume we have available a map of the region, which is correct geometrically, and is in scale not too different from that of the image we wish to current. Secondly, we need to be able to identify points called control points, in both the map and the image, so that we can tie the two together mathematically. The real power of this technique lies in setting up mathematical equations that link points in the image, with corresponding points in the map. We will use the control points to set up those mathematical equations. In a sense, we are assuming that if we can relate the control points, from the image and the map mathematically. And that the control points are indicative of the map and image geometries, than other points in the map image will also be related by those same equations. This slide shows what we have in mind. The geometrically correct map is on the left hand side. And the image to be corrected is on the right hand side. Notice that these are in the opposite positions, from the treatment of the previous lecture. It is just more convenient in this analysis to have them in this new order. As before, we have chosen the coordinates x and y, to describe the position of a point, or pixel in the map. And the coordinates u and v to describe position in the image. In principle, it should be possible to set up equations that relate the coordinates of a point in the image, knowing its position in the map. This is shown here by the pair of functions, in which both of the image coordinates are shown as functions of both map coordinates. Those functions will contain unknown coefficients, which when given explicit values, will allow us to go between the map and the image. To find those explicit values, we need some points in common, that is simultaneous sets of x, y, and u, v, that we can substitute into the equations. Those points are the control points, or sometimes they're called Ground Control Points. Here, we show the relationship between points in the map and the corresponding points in the image. By drawing a grid or pixel centers in each case. What we are going to do is choose a grid position on the map, and by using the mapping functions, find the corresponding grid position in the image. What sorts of functions should we use for this? Well the simplest are polynomials. Here we show the mapping functions expressed as polynomials. In this case of order or degree two. In principle, any order could be used, expecting that higher orders will give more accurate results. However, higher order polynomials introduce their own problems, as we will see later. The unknown coefficients to be found using the control points are the a sub zero, b sub zero, etc. If we assume that we have used the control points, and identified the unknown coefficients. Then we use the two mapping polynomials, in the following manner. We've moved over the map grid, point by point, that is, grid intersection by grid intersection. And for each point, use the mapping polynomials to find the corresponding point in the image. We find the pixel value at that image point, and transfer it onto the map position we started with. That is done for all grid positions on the map. Thereby building up a geometrically correct image, essentially using the recorded image as a source of pixels. [BLANK AUDIO] This slide shows the process broken into the two individual steps. At the top, we find pixel locations in the image, which correspond to pixel positions on the map. On the bottom, we transfer the pixel from the image onto the map grid. This is just a small point, but it is worth noting, the pixels that are centered on an image grid. The map or record image are of such a size that adjacent pixels join up against each other. There are now two important matters to be considered, before the method is workable. The first is how to determine the unknown coefficients in the mapping polynomials. And I've said earlier that involves the use of control points. The second practical matter is that the mapping from a map grid position using the polynomials, may not project exactly onto a pixel position in the image grid, that will form the focus of our next lecture. Remember that the control points are pairs of coordinates, in the map and the image. Essentially, each control point is represented by the set of values of x, y, u and v. If we had, say 20 control points, we would have 20 sets of those four numbers. What we do is, substitute those knowing values into the mapping polynomials, and thus set up a set of simultaneous equations, which when sold, yield the required values of the unknown coefficients. Clearly, we need to have at least as many control points, as there are unknown coefficients, in one of the mapping polynomials. We can use the same set of control points to generate the unknown coefficients in the other mapping polynomial. We do not need the same percent. For a second order mapping polynomials, a minimum of six control points is required. Three are needed for a first order colinear mapping polynomial. And ten are required, if third order mapping polynomials are used. Usually we choose more control points than necessary and generate least squares estimates of the unknown coefficients. A particular advantage in choosing greater than the minimum number of control points is that, any specific control point which may not be well placed in either the map or the image, will not have an undue influence on the values of the coefficients being generated. Effectively, this technique depends upon having an accurate way of relating points in a map to the corresponding points in an image. We do this, by the process of choosing control points and establishing mapping polynomials. We will see an example in a later lecture. Pay particular attention to the second question. You might like to think about the similar problem of fitting a curve through a set of data points, using curves of increasing order. Will a straight line always do a better job, than a cubic polynomial or not? 

### Module 1 Lecture 10 Resampling

We now introduce a new term in this lecture. The process of finding an actual value for the pixel in the image to place onto the map grid using the process of mapping polynomials is called resampling. Remember, we determined the explicit mapping polynomials using the set of control points to find the unknown coefficients. We then used those polynomials to find points in the image which correspond to points in the map. If the points in the image indicated by the mapping polynomials fall exactly on a pixel grid center, then the pixel is simply relocated onto the corresponding or correct position in the map grid. However, rarely will that be the case. More often, the point in the image indicated by the mapping polynomials, will not fall exactly on a pixel center in the image, but somewhere between several pixel centers, as we see in the next slide. Here we share the mapping from a geometrically correct map to the image for the general case, where the projected image coordinate does not match a pixel grid center exactly. Instead, the projected point lies somewhere in a region bounded by four grid centers. What we have to do is to estimate what the brightness of the original scene would have been at that position. Effectively, that means carrying out an interpolation over the neighboring measured pixels. Note here we have indicated the pixels explicitly by dashed lines centered on each of the grid intersections. How do we go about finding a brightness for a pixel that is for the scene at the location indicated by the mapping polynomials? A simple approach, and one which was often used in practice, is to choose the nearest recorded pixel as seen here. This is called nearest neighbor resampling. For it to work effectively, the grid spacings in both the map and the image should be about the same. There are two other resampling techniques often used in practice. Both are based on interpolations of brightness over neighboring pixels. The first involves linear interpolations over the four surrounding pixels. It starts by computing the two interpolated brightness values along each scan line above and below the projected pixel point using the real pixel values either side. These are shown as the intermediate interpolants in the top right-hand's diagram. The next step is to do a vertical interpolation over those two intermediate values to get the final brightness value for the pixel. This process is called bilinear interpolation. The second, more sophisticated procedure is to interpolate over four sets of four pixel brightnesses, as illustrated on the bottom right-hand side of the slide, and then interpolate over the four vertical intermediate interpolants. Cubic polynomials are used to undertake the interpolation, so that the technique is generally called cubic convolution resampling. While a more complicated process, cubic convolution resampling, generally leads to a smoother looking image after geometric correction. By contrast, nearest neighbor resampling can produce a blocky looking product if the scales of the image and map are significantly different. Although we have developed the process of using mapping polynomials for registering an image to a map, and therefore effectively removing the major sources of geometric distortion, the same process can be used to co-register sets of images. This is often needed if the user has available for analysis, several images of the same geographical region, often taken by different platforms and sensor types. As noted here, we're not restricted in how many images we can co-register. It is just a matter of choosing one image as the master, taking the place of the map, and registering all other images to it. As seen in this summary, the process of registering an image to a map in order to remove geometric errors is often called geocoding or sometimes geo-referencing, because the pixels can now be represented by their map coordinates, latitudes and longitudes, rather than by the original row and column numbers. The questions here, ask you to think about some practical aspects of using ground control points and mapping polynomials for geometric correction. Please give them serious thought because that will feature strongly in the next lecture. 

### Module 1 Lecture 11 An image registration example

In this lecture, we will demonstrate the application of the procedure just developed by registering one image to another. The same procedure is used for registering an image to a map to remove geometric distortions. By using an image to image example though, it is easy to highlight some pitfalls that must be avoided. The example we have chosen involves portions of two Landsat multispectral scanner images of a region in the northern suburbs of the city of Sydney, Australia. One was acquired in 1979 and the other in 1980. We have only shown one of the near infrared bands here. Once those bands have been registered, all bands can be registered using the same control points and mapping polynomials to generate registered color products. Clearly there are major brightness differences between the two images. Since they have been recorded in the near infrared, both healthy green vegetation and urban regions, show as bright, water shows as black and the dark gray regions are bird vegetation resulting from fires. In the 1979 image. There is a major fast guy in the center by 1980, much of that has revegetated but two new fast cars are evident. One to the northwest on the boundary of the river and the other in the central southern part of the same. There are geometric differences between the images as well, even though they are less evident. It is by registering the knot in ATMs to the 1979 image that we will remove the geometric differences between them. Had the 1979 image spending map then that would amount to remove in geometric distortion from the 1980 image. Our first task is to choose a set of ground control points. On this slide, we have shown two different sets. Those on the top tier of images are well distributed over the scene. Whereas, those on the bottom images tend to be clumped with none towards the image edges. Notice that we have chosen some control points on the water, land boundaries. As much as possible, those sorts of control points should be on headlands and not river inlets. So the title variations are minimized. In general, we should choose as control points, features which do not change in position from image to image. Road intersections are good as our corners, our buildings, and fields. In this example, we are going to use third order mapping polynomials and cubic convolution resampling. This slide shows the results of registering the 1989 image, The Slave. To the 1979 image the master using the good set of control points. That is a set that was well distributed over the image. In order to demonstrate how well the registration has worked, we have displayed the master image as red and the slide image as green. The reason for doing that is that where the images are in alignment geometrically and have approximately the same brightness, the combination will appear yellow, which is the additive combination of red and green. Here we see that the registration is very good. Apart from the color differences due to the 1979 and 1980 fire scars, the combination is yellow with no obvious red green separation. The other red border is just part of the 1979 scene for which there is no not in AD coverage. The fire scars colors are interesting, the 1979 scar shows as green because there is very little red signal from the 1979 pixels. Whereas the 1980's scars show as red for the same reason. That is there is very little 1980 signal in those portions of the image. This slide shows the result of registration using the poorer set of control points. Where the control points were bunched, the registration is very good. But beyond the distribution of control points, the registration is particularly bad. Note especially the red and green offsets around the edges of the water. Note in the yellow color, we can see that the 1980 image has been stretched out of shape across the diagonal. We now wish to understand that behaviour. We can get a good idea of well behaved and poorly behaved registration by looking at fitting a curve through a set of points as shown in this slide. The first order equation matches the distribution of points moderately will and seems to extrapolate in a well behaved manner beyond the set of points. The second order curve gives a slightly better match to the points, but seems to diverge unnecessarily beyond the data points. Finally, the third automatic polynomial can be made to fit the data points more closely than the other two. But it behaves quite radically beyond the extent of the data. In other words, it does not extrapolate well. The general lesson we learned from this exercise in curve fitting is that low order polynomials may not fit a set of data points as well as higher order polynomials. But in general terms, they will extrapolate better. In contrast, higher order polynomials may match the available data better, but will extrapolate poorly. Returning to the previous slide, the third order mapping polynomial fits a region of the image in the cluster of control points very well. But beyond that region it extrapolates poorly. That gives us a particularly important guide in the choice of control points. As far as possible, the set of control points should be distributed uniformly across the same and enclose the region of the image for which accurate registration is important. If there's any data about the distribution of control points, a lower order mapping polynomial should be used. This example demonstrates the benefit of using lower order mapping polynomials when the distribution of control points, is not good, it uses a poor set of control points from the previous example. The result of that example, is shown on the left hand side of the slide. Remember, this was generated, using third, order mapping polynomials. Without changing anything else, the image on the right hand side was generated using first automatic polynomials. As observed, the extrapolation of the rich sit not in it image ingrained, is much better than when the third order polynomials we use. There will be greater registration error in the vicinity of the control points. But that is not noticeable visually in this simple case. Just by whatever Amanda every band of an image has to be registered or corrected in this manner. Provided the band to band registration from the image supplier is good, the same control points and mapping polynomials can be used. These questions relate to practical aspects of image registration, including geometric correction. And not so much to the theory of the process itself. They ask you to think about we might generate unintentional problems if you're not careful with your choice of technic scale, and methodology. 

### Module 1 Lecture 12 How can images be interpreted and used?

We are now at a very important stage in the course. In the previous lectures, we have learned about how sensors are used to record images. We have learned about the nature of reflected sunlight and Earth thermal emission as sources of Earth data. We have noted how the atmosphere affects the transmission of radiation from the sun to the surface into the sensor, and how we can correct for image errors in brightness and geometry. Effectively, that means we're now at the stage of having acquired good quality data ready for analysis. The rest of our course depends upon having reached this stage satisfactorily. There are two very broad approaches that can be taken when endeavoring to interpret images recorded by a remote sensing platform. The first involves a person doing most of the work. A person skilled at interpreting an image visually is called a photo interpreter and the process overall is called photointerpretation. This is a long standing method for interpreting images that goes back to the days of a photo interpretation. The analyst builds up substantial knowledge of how to look at the various cues and clues in an image in order to extract meaningful information. The second part, which is much more recent, involves the use of sophisticated computer algorithms to help undertake an analysis. A humans still involved, but their skills are more in knowing how to get the best results out of the computer-based approach then being able to interpret the images themselves. When computers are involved in the interpretation process, the approach goes by a number of names including quantitative analysis, classification or machine learning. More recently, artificial intelligence methods have been contemplated for remote sensing image analysis. We will say something about that later in our course. We will now examine both photointerpretation and quantitative analysis in general terms, but the major emphasis of the remainder of this course is on the computer approach. Let's start with photointerpretation. A skilled analyst works for the color image product composed from the recorded image bands, or sometimes with the individual bands themselves displayed as black and white. Irrespective of whether black and white or color imagery is used, the analyst makes use of color or relative differences in brightness from band to band, and relationships between objects and pixels in order to carry out interpretation. Changes with time of features from one image to another in a co-registered set are also important clues for the photointerpreter. Because an analyst relies on understanding brightness and color, it is important that we know how color image products are formed if we want to appreciate the nature of the products with which a photo interpreter works. As you probably appreciate from your own knowledge of digital photography or color television, a color image is made up of three black and white images displayed on each of the three primary additive colors of blue, green and red. Keep in mind that an individual band of image data is always represented from black to white, that is lowest brightness to highest brightness. Even though it might represents same, the reflectance of the Earth's surface in the red wavelength range for example seen through a red filter. A challenge in remote sensing is to know which bands to display in which colors. We only have three display colors, but we could have 10s or 100s of bands of image data from which to choose We illustrate in this slide three different color images formed from different band combinations using data recorded by the high map hyperspectral sensor. Remember, you have to choose which bands to display in each of the red, green and blue color primaries. On the left hand column of images, we have chosen a recorded visible red band to display as red, a visible green band to display as green and a visible blue band to display as blue. The result is a natural color image where the vegetation appears green, and urban and other developed services take on a brownish or slightly bluish appearance. In the center column, we have created what is commonly called a color infrared image. It is based upon the choice of bands long used in color infrared aerial photography. A near infrared band is displayed as red, a visible red band is displayed as the green and a visible green band is displayed as blue. Although this sounds unusual, it gives a very rich color product in which healthy vegetation having a high near infrared response shows as red and bare regions show shades of blue and green, clear water generally displays as black. In the right hand column, we show a color image product made up from a set of infrared bands. As seen in the color arrays, there is a fairly good differentiation of different cover tops. Often an analyst will experiment with bands to display guided by their knowledge of the spectral reflectance characteristics of the Earth's surface covered tops of interest in a particular interpretation exercise. Are there any conventions observed when forming color image products? Well, the only real convention is that the bands are displayed in the same order of wavelength as the primary display colors. Therefore the recorded band with the longest wavelength is shown as red and therefore the shortest wavelength is shown as blue. Once we form the color image product, we have to work out how to interpret it. That requires reference back to the spectral reflectance characteristics of the different earth surface cover tops present in an image. On this slide we have shown where three bands green, red and near infrared are located on the reflectance spectra. When combined into the color infrared composite, vegetation appears as red, soil and urban regions appear as blue and green and water will appear as bluish black, depending on its depth and turbidity. There is however an intervening step, which you may have picked up by a careful examination of the spectral reflectance clues and the color infrared product. From the spectral curves alone, it looks like all cover tops should be predominantly reddish. Indeed, if the composite were formed from the raw data recorded by the platform, that will be the case. In practice however, we undertake what is called a contrast enhancement step in order to get the color product shown on the bottom right hand side of the slide. We will see how to do that in the next lecture. In the early days of remote sensing, with instruments only had three or four wave bands. Selection of which three to use when forming a color composite product was quite straightforward. With modern instrumentation however, the analyst is faced with a more complicated task to choose the bands to display. In some ways, this places more dependence on the analyst's knowledge of the spectral characteristics of the same properties of interest. These quiz questions are important in terms of drawing your attention to the limitations of a human analyst, limitations which we expect will not be a problem when you're using a machine to assist with the interpretation. 

### Module 1 Lecture 13 Enhancing image contrast

This lecture is a little outside the mainstream of our treatment, but it is important because most of the images we deal with visually have had their contrasts enhanced in order to improve their visual quality and range of colors. Here we want to understand a little of how that is done. The detectors used to record each band of image data are calibrated so that they can respond to the darkest and brightest cover types anticipated. Typically, those cover types could be deep clear water or snow and cloud types. That means that most of the cover types that are imaged fall in a range of brightnesses well inside the range from total black to full white, as shown on this slide. For a typical recorded image, the appearance is therefore slightly dull and poor in contrast. It will be good if we could expand the range of recorded brightness so that the signal of interest covers the full range from black to white. What we need to do now is understand how that can be made to occur. In order to set up a process for changing the range of brightness values, we need first, some means for describing the distribution of brightness in an image or band. We choose for that, the image histogram, which is a bar graph of the number of pixels of a given brightness plotted as a function of that brightness value. Here we see an image as it has been recorded by an aircraft scanner. Note how poor it is in contrast, brightness, and color. Note particularly that it has a reddish appearance, as we mentioned in the last lecture. On the right-hand side, we have shown three histograms. One corresponding to each band and colored according to the color in which the band is displayed in the color composite image. Note that while each histogram commence as close to the lowest brightness value in each case, that is zero, the upper extremities of the histograms are well shorter the maximum available brightness of 255. Ideally, we would like the histograms to cover the full range of brightness. If we could stretch out the histogram as shown here, so that it covers the full brightness value range, we won't have an image with much better contrast, as we will now see. To do that, we introduce a graph called a brightness value mapping function. It shows us how to take the old brightness values in a histogram and change them to new values which cover the full brightness value range. Effectively, what it does is tell us how to relocate the bars in the histogram more favorably. We usually work out the nature of the brightness value mapping function by examining the recorded histogram and noting the desired new histogram distribution. Once we have that mapping function, we apply it to the image in the following manner. We start with the first pixel of the image, read its value onto the abscissa of the mapping function, and read off its new value from the ordinate. That new value is used in place of the old brightness value of the pixel. That is done pixel by pixel and line by line until all of the pixels in the image have had their brightness values modified according to the brightness value mapping function. The resulting image then maps a much better use of the available range of brightnesses. We have to do that for each band of data. The brightness value mapping function will be different in each case. Here we see the results. Commencing with the poorly contrasting original image, we construct the histogram for each band. Then expand those histograms via their own respective brightness value mapping functions. When applied to the pixels in each band, the color composite image, say generated, is a much nicer looking image product on the right-hand side. In that final product, we see a much better range of brightness and contrast, and certainly a much better use of color. You can see now why contrast enhancement is almost always used in remote sensing when visual interpretation is important. Because we are dealing with a discrete number of brightness levels when handling remote sensing imagery, the brightness value mapping functions of the previous slides are actually a set of points equal in number to the number of discrete brightness values. For 8-bit imagery, there will be 256 pairs of input old and output new brightness values. For 10-bit data, there will be 1024 pairs and so on. As a result, we represent the brightness value mapping function in a computer algorithm as a look-up table, as shown in the last point on this slide. Take particular note of the second question here. The brightness value mapping function does not always need to be linear, as discussed in this lecture. Instead, it can have a range of non-linear forms which can give added emphasis to particular ranges of brightness. 

### Module 1 Lecture 14 An introduction to classification (quantitative analysis)

We are now at the stage in our course where we can start seriously looking at how to interpret images recorded by remote sensing missions, particularly using computer-based algorithms. Ultimately, this will take us into a mathematical journey, sometimes quite complex, but we will ensure that if you do not have the required mathematical background, the accompanying description should be sufficient for you to understand how the techniques operate. We start though with our descriptive outline of the essential aspects of computer image analysis, which is often called classification or sometimes quantitative analysis in remote sensing. The first concept to keep in mind is that we almost always work at the level of the individual pixel. There are procedures that take a different approach, but the great many that we will encounter focus on the individual pixel because it is the smallest quantifiable element in an image. Remember, for each pixel, we have a set of recordings composed of the brightness values detected in each of the individual wavebands. That set of measurements is collected together and written in the form of a column surrounded by square brackets, which we call a column vector or a pixel vector. In classification, we use the properties of the pixel vector to help find the label for the pixel, a label that represents one of the classes of ground cover in which we might be interested. For example, if we wanted to use remote sensing to map the natural landscape, we might employ computer classification to analyze each pixel in the image and label them as grass, soil, water, forest, and so on. Once all pixels are labeled, we then have a map of cover types which we call a thematic map because it is a map of themes or classes. The labels are specified by the end user. What the image analyst has to do is come up with a computer based approach that allows those labels to be applied to the pixels through an analysis of the pixel vectors. In this slide, we developed several of the most important concepts in the image analysis. To do so, we have assumed for convenience that the sensor we are dealing with has only two wave bands, one in the visible red and one in the near infrared. Again, for convenience, we assume that the scene being imaged by the remote sensing instrument consists of just three natural cover types, vegetation, soil, and water. Invariably, the first step in any classification process is for the analyst to find sets of pixels in the recorded image for which the cover type is known. In the upper left-hand depiction of the scene, we have indicated by the shaded shapes that the analyst knows that the pixels in those regions are actually pixels of vegetation, soil, and water, as indicated. 

We now need to remember something from the previous lecture, and that is the shapes of the spectral reflectance curves for the three cover types of interest. They are shown on the bottom left-hand side of the slide, and in each case, several examples of reflectance curves are given indicating that in practice there is natural variability among pixels, even of the same color type. If our sensor takes measurements in the red and the infrared, that amounts to sampling the cover type reflectance curves at those wavelengths as seen in the figure. For the vegetation pixels, the red samples will have low brightness, whereas the infrared samples will have high brightness values. By contrast, the samples for soil would be moderately bright in both bands and those for water will be moderately dark in both bands. We now introduce one of the most important concepts in classification, the representation of pixels in a geometric space to by the sets of spectral samples. Such a space is shown on the right-hand side of the slide, in which individual pixels are plotted according to their sets of brightness values. This is just a Cartesian coordinate system in which the axes are the pixel brightness values. If we now return to the sets of pixels for which we know the correct labels, we can plot them in our coordinate system as shown on the diagram. Because pixels from the same cover type will have similar spectral reflectance curves. When they are plotted in our coordinate system, they will tend to group or cluster as indicated. Effectively, those pixels for which we know the labels to find regions in the coordinate system as their own. There is a region in which we expect to find water pixels, a region in which we expect to find pixels of vegetation, and a region in which we expect to fine pixels of soil. In other words, the pixels for which we know the labels effectively segment the coordinate system into a set of regions corresponding to the classes of interest. We could make those regions explicit by drawing lines or boundaries that separate the classes as indicated. We have now introduced some terminology. Besides pixel vector, we use the term class to describe the particular cover type on the ground and we call the coordinate system a spectral space. The process of using pixels, for which the class labels are known to find regions in the spectral space corresponding to each class is called training. That is the first step in classification. That is to use known data to find corresponding regions in this spectral space. How does the analyst know the labels for some pixels? That can sometimes be a long and expensive task depending on the complexity of the exercise. Often, field visits are required, so that the class labels for groups of image pixels can be found. Sometimes, particularly in simpler classification exercises, human image interpretation or even the analysis of accompanying photos can be used to establish the training pixels. Finally, this example has been based on a two-band sensor, which has led to a two-dimensional spectral space. In practice, of course, the dimensionality can be much larger. A sensor with 10 wavebands will generate a 10-dimensional spectral space. While we can easily envisage a two and even three-dimensional space, we are not equipped mentally with the capacity to go beyond that, but that is not a problem. While we will develop many of our techniques on two and three-dimensional examples, the corresponding models and mathematics are easily extensible to any dimension, so all we have to do is develop our ideas in say, two dimensions, and understand that mathematically, any number of dimensions or wavebands can be handled. We can now use the results of the training stage of the previous slide to help us add labels to all of the other pixels in an image, that is, those we do not currently have labels for. This slide shows the process. We take an unknown pixel and plot it in the spectral space according to its brightness values. Because using training pixels, we previously had segmented the spectral space into regions representing each of the cover types, we can now add a label to the new pixel by seeing where it falls in the space. In this particular case, we have labeled a vegetation pixel. As we noted, the step in the previous slide was called training. This second step is called classification, labeling or sometimes generalization. In summary, classification or quantitative analysis is a two-stage process. The first is understanding the structure of the spectral space by using sets of training pixels. The second step is to use the results of the training to enable us to add labels to pixels we do not yet know about. Looking at the diagrams in this lecture, we could create a simple classifier by setting up the equations which describe the lines which separate the different segments in spectral space. Indeed, a very simple and early classifier did just that by making the lines midway between the mean positions of two groups of pixels. See the quiz question following. We're going to look at much more complex classifiers in this course because of their improved performance. The questions in this quiz focus your attention on the structure of the spectral space. Even though most of the time, the dimensionality is too high for us to envisage, it is still nevertheless important to understand as much as possible the structure of the data domain with which we are dealing. 

### Module 1 Lecture 15 Classification: some more detail

In this lecture, we will consolidate some of the concepts from the last and compare the attributes of a human interpreter undertaking image analysis compared with analysis being carried out by a classification algorithm. Remember, we are dealing with a two-stage process for machine labeling, training and classification. In the first, a human interpreter is involved in identifying training data as well as selecting the particular classifier to use, as we will see later. Let's now look at what a human can do in image interpretation that a machine currently finds difficult and vice versa. In this slide, we compare the performance of a human interpreter to that of a computer-based algorithm from a number of important perspectives. First, when an image can consist of thousands to millions of pixels, it is impractical for a human to try to work in general at the scale of an individual pixel. By comparison, because of the speed of a computer working at the pixel level is practical and normal. Because we estimate area in an image by counting pixels, if a human interpreter cannot easily work at the pixel level then the ability to estimate areas is also limited. Again, by contrast, area estimation by computer is very easy. We have already discussed this next one to some extent. But when a sensor has more than three bands, it is difficult for a human interpreter to maintain attention over all bands when carrying out an analysis. A machine, by comparison, can handle any dimensionality provided we setup the mathematics suitably which we are about to start in this lecture. Human beings are able to process only about 16 different levels of brightness between black and white. A computer can handle any number of brightness levels when looking at the properties of a given pixel right right to the set of values defined by the radiometric resolution of the sensor. Recall, an eight-bit sensor can represent 256 levels of brightness per band. An area which is easy for human beings to work with, but let's say if a computer algorithm is the determination of shape, proximity, and general spatial analysis. Humans are easily able to discern crop fields, roadways, lakes, and other shapes, and their locations and juxtapositions, whereas quite complex software algorithms are required for shape and spatial analysis by machine. A conclusion that can be drawn from this analysis is that the most successful image interpretation exercises in remote sensing are those in which the skills of the analysts are used to best effect with the powerful properties of computer algorithms. As inferred in the last slide, most image interpretation exercises in remote sensing would be based on machine classification, now often called machine learning, but will nevertheless involve the input of a skilled human analysts. We'll always keep that in mind, even though this course will be heavily focused on machine methods for image interpretation. The idea is to use human knowledge optimally to get the best results out of the machine classification algorithms in such a way that applying human knowledge on a small part of the data yields an understanding of the whole image. In the labeling stage, usually that's a very good investment in time and money. This slide represents the classification process in four blocks. The first is the potentially expensive one, in which the analyst has to find some labeled training data to feed into the classification algorithm. As noted here, in a real exercise, this can be a time-consuming and expensive step because it often involves field visits. The second step is to use the labeled training data to try in the particular classifier algorithm that the analyst has chosen to use. Software is always available for that task. The third step is the important one and where we reap the benefits of classification. After having put on lot of effort into training a classifier, we now set it to work labeling the whole image. The final step is to examine the thematic map produced by the classifier to see where the error is made in a classification, and there always will be some are within the bounds acceptable to the application at hand. Are we happy that certain crops are labeled with 95 percent accuracy, or do we want to do better, for example? In practice, we often find that we have to iterate or redo parts of our classifications to get the levels of accuracy required. Incidentally, how can we assess the accuracy of the thematic map? To do so requires further labeled samples, just like the samples we use for training. Indeed, during the acquisition of training pixels, the analyst will at the same time set aside some labeled pixels as testing data to be used when looking at how well the classifier has performed. We will consider that in some detail later in the course. Here we have shown the two steps of training and labeling in a very simple classification exercise. The image is a segment of a four-band Landsat multispectral scanner image with four predominant classes; vegetation, burned vegetation, urban, and water. Here it is easy to select training data from an inspection of the color composite image product as shown in the middle image. The training pixels of water are shown in purple, those for vegetation are shown as green, those for burned vegetation are in red, and those for the urban regions in deep blue. Those training pixels are fed to the classifier to produce the thematic map and table of areas on the right-hand side. Much of our attention in image understanding will now focus on machine methods for analysis, based on training as just illustrated, or in some cases using algorithms that discover the structure of the spectral space in which the regions are labeled afterwards. That is, rather than during a training step. We will look at a number of techniques. Some are simple and easy to use, but other more complex algorithms often give better results. As seen in the illustrations in this slide, straight lines might be simple separating boundaries, but higher curves can often separate more interwoven datasets. In general, the latter are more difficult to implement than just placing straight lines between the classes. Up to now, we have talked about lines, planes, and surfaces, which are concepts we normally associate with two and three-dimensional drawings. Once the dimensionality exceeds three, we use terms like hyperplanes and hypersurfaces. To finish off this lecture, we want to revisit the spectral space itself, since it is fundamental to the development of many machine learning methods. In a later lecture, we will also see how to transform the space into other, sometimes more effective representations of our recorded remote-sensing image data. Recall that the spectral space is just a Cartesian coordinate system in which pixels plot according to their brightnesses in each band or axis. At this point, it is of value to remind you that the next lecture, we'll commence a detailed analysis which involves properties of vectors and matrices. If you do not have that background and wish to follow the development, please consult some standard introductory treatments of that material. You can also consult appendix C of J.A. Richards Remote Sensing Digital Image Analysis, fifth edition, Springer Berlin, 2013, which develops the material in a form matched to how we will use it in remote sensing. Each pixel point is represented by a column vector, which is given a single symbol in lowercase bold font. It will have as many entries as there are bands. For an N-band sensor, the column vector will be N-dimensional. By representing the vector measurements in this form, we can use the whole field of vector and matrix algebra in developing our Machine Learning Tools. This summary simply reminds us of two aspects of the lecture, the human machine comparison and the importance of the spectral domain. The first question in this quiz asks you to think about the size of the spectral space. What does that also tell you about its density for a typical image that might consist of, say, a million pixels? 

### Module 1 Lecture 16 Correlation and covariance

We now start our mathematical journey by looking at some properties of groups of pixels in the spectral domain. As noted in the last lecture, we now assume you have a background in vectors and matrices. If not, we will summarize the key points geometrically and descriptively, so that the key results should be clear. We are going to look at three important spectral space concepts in this lecture; the mean vector, the covariance matrix, and the concept of correlation. Because we'd like to talk about where pixels are located in the spectral domain, it is useful to have measures of where they are most likely to be found and how they are spread about that location. We start with the concept of the mean vector, which is just the average or mean value of a group of pixel vectors, as illustrated on this slide. We denote the mean vector by the symbol m. It is calculated by taking the mathematical average of the k pixel vectors. Another term for average is expectation. Note we have introduced the expectation operator, which is just the calculation of the average as shown. Here we show a very simple calculation of the mean of two pixel vectors. Note that the calculation is performed separately for each element of the pixel vector. Coming back to the concept of expectation to position m is where we most expect to find the pixel from that group. The mean vector tells us the average position of the group of pixels in the spectral domain. We would now like to describe how they spread about that mean position. That is a concept we're familiar with from statistics, where we use the concept of variance or standard deviation for that purpose. Because we are dealing with data of a dimensionality, the simple concept of variance does not apply, but we can develop a multidimensional equivalent. This is called the covariance matrix, and is again defined in terms of an expectation using the formula shown on the slide. If you look carefully, you can see that it is very similar in structure to the formula for variance in a one-dimensional case. Let's explore this a bit further. First though, note that the denominator term in the average calculation is k minus 1, and not just k. That gives a better estimate. First, we subtract the mean vector from each pixel vector as shown in the center of the slide, just like we take the difference from the mean in a one-dimensional variance calculation. We then need to square the mean difference, which is the role of the right-hand term, but it is turned into a row vector by the transpose operation shown. In the next slide, we will see what that does to the product. We then take the average or expected value of all the mean differences squared. The end result is a new construction called a matrix, represented by the uppercase letter C. We have added the subscript x to the symbol for the covariance matrix, since it relates to calculations where the pixels are described by x. Later on, we will encounter other coordinate systems, and will therefore use other subscripts. This slide shows the result of multiplying a column vector by a row vector, where the row vector appears on the right-hand side of the column vector. This gives a square array of numbers called a matrix, in which the elements are the result of the rules for multiplying vectors. The dimensions of the matrix are equal to the number of bands in the image data. A different result will be obtained if the order of the vectors is changed. We will see an example of that later in the course. To see how all of this works, we will now consider a simple example. Here we have a two-dimensional data set at points that are distributed around the space in a squashed circle arrangement. They will be the pixel brightness values of an image with just six pixels. A set of hand calculations are shown, including the mean vector, the steps to calculate the covariance matrix, and the covariance matrix, which results. Note that the covariance matrix has zeros in the upper right and lower left positions, and only has non-zero values down the diagonal entries of the matrix. Those diagonal values are respectively the individual variances of the data points in the horizontal abscissa and vertical ordinate directions. We have just talked about the diagonal of a matrix. By definition, that is the set of entries that run from the upper left-hand side to the lower right-hand side of a matrix. This particular covariance matrix is called diagonal because it has zero entries everywhere, except down the diagonal. This slide shows that the diagonal elements of the covariance matrix and the correlation matrix to follow relate to individual bands. Whereas the off-diagonal elements describe the relationship between one band and another. Because of the zeros in this case, there is no relationship between the bands. Now consider another two-dimensional data set. Here the data points seem to be spread in an elliptical pattern at an angle to the axis. The hand calculations for this data set show that the covariance matrix is non-diagonal. That is, it has non-zero entries everywhere. At this stage, it is important to define the correlation matrix, whose elements are computed from those of the covariance matrix as shown. They are the equivalent covariance matrix elements divided by the square root of the product of the two covariance elements on the diagonal, one on the same row and one on the same column as the entry under consideration. Let's see what the correlation matrices looked like for our two data sets. Here are the two data sets compared, including their covariance and correlation matrices. Note that the correlation matrices always have one for all of their diagonal entries. You can see why that is the case from the definition of the previous slide. It tells us that the pixel brightness values in a given band are fully correlated with themselves. Secondly, for the left-hand data set, there are zeros in the off-diagonal entries, implying that there is no correlation between the corresponding two bands of data. What does that mean? Put simply, it means that knowing the brightness of a data point or pixel in one band, we cannot, with any degree of certainty, predict what its brightness is likely to be in the other band. If we examine the right-hand data set though, the off-diagonal terms of its correlation matrix are non-zero and imply there is about 76 percent correlation between the bands. In other words, if it is brought in one band, it is, with about 76 percent certainty, likely to be brought in the other band. That is because the way the data is scattered largely about a line at an angle to the data axis. As reminder, for the first data set, the points do not distribute other than as parallel to the axis. There is no correlation between the bands, and one cannot say that the pixel will be brought in one band if it is brought in the other. This is an essential but simple dot point summary of the important elements from this lecture. Note the last particularly, because it is essential to what we are going to do next. Note that the last question relates the dimensionality of the two matrices and the mean vector to the number of bands recorded by a particular sensor. 

### Module 1 Lecture 17 The principal components transform

We now come to a particularly important concept that you will encounter time and time again in remote sensing image processing. We will see it has many uses, from classification and the general representation of images on a display system through to highlighting changes that have occurred with time. It is called the principal components transformation, PCT, or principal components analysis, PCA. All remote sensing image processing software packages will include a module for computing this transform. Depending on your background, you may find some of the mathematics here a bit complicated. However, once we have been through all of that, we will summarize the essential steps towards computing the transform. They are straightforward and easy to apply in practice. When we apply the principal components transform, we generate a new set of bands with which to describe the image. The pixel brightness values in these new bands turn out to be weighted linear combinations of the original pixel brightness values. There are a number of image transformations of this nature that will be encountered in image processing, including the so-called Fourier transform, the wavelet transform, and band arithmetic. We will only treat the principal components transform in these lectures and make some reference to band arithmetic, in which the elements of pixel vectors from two different images are added, subtracted, or divided. To start us thinking about principal components, consider again the two data sets from the previous lecture. One has no correlation between its bands or axes, the other has high correlation. Note the last comment on the slide. Our correlation matrices will always be diagonal matrices of the same dimensionality as the number of bands recorded by the sensor, with unity for each diagonal element. Although not important here, that type of matrix is called an identity matrix. We commence our development of the principal components transform by asking whether we can take a correlated data set and somehow transform it into a new set of coordinates in which the data exhibits no correlation. Consider data that is scattered in the form of a shaded ellipse shown in this slide. This is a bit the correlated set from our two examples. In the original coordinates, that is, as recorded by the sensor, the data is highly correlated. Bright pixels in one band tend to be bright in the other, and so on. But if we rotated our coordinates anti or counterclockwise, we'd see that there is a rotation angle, shown here as the y coordinates, that corresponds to a coordinate system in which the pixels in the data set are uncorrelated. In the language of vector and matrix analysis, the new coordinates can be generated from the old by the equation shown on the slide. The values of the g, i, j, etc., define the actual extent of the rotation. What we have to do is find those values so that the correct degree of rotation is given. We can write the matrix equation in the symbolic shorthand notation shown on the slide. Whereas vectors are written as bold lowercase, matrices are written and spelled uppercase. If you don't know how to multiply vectors and matrices, please consult some standard treatments or be prepared to follow the remainder of this lecture as best you can, even though it is addressed to the specific case of rotating axes. Here is the really important point, we define the new axes as those in which the data shows no correlation. Or in other words, in which the new covariance and correlation matrices are diagonal, that is, they have zeros everywhere except down the diagonals. Since we are interested in the covariance matrix, let's examine how it appears in the new rotated y coordinate system. This slide shows the standard definition for the covariance matrix that we have met in the last lecture, but we have added the y subscripts to remind us we are dealing in that coordinate space. The main vector in the y coordinates is easily related to its value in the x coordinates via the transformation matrix G, as shown. We can substitute that into the formula for the covariance matrix to get the very important expression in blue on the bottom right-hand side of the slide. To get to that point, we had to use some properties of matrices and vectors. If you do not have that background, just accept the last expression, noting that the transpose of a matrix flips it about its diagonal. The equation we last derived expresses the covariance matrix in the new y coordinate system in terms of the covariance matrix in the original x coordinate system. The two are linked by the matrix G and its transpose. Remember, when we started this analysis, we had the objective that the data would be uncorrelated in the y coordinates. Or in other words, the covariance matrix in the y coordinates would be diagonal. That constraint allows us to recognize this equation as the so-called diagonal form of the original covariance matrix. That tells us immediately what the elements of the matrix G are and also gives us the diagonal entries of the white space covariance matrix, remembering that the off diagonal entries are all zero. Each matrix has a set of properties call its eigenvalues and eigenvectors. We will see how to get them in the next lecture. Usually there are as many of each as the dimensionality of the matrix. The eigenvalues are just scalar quantities, which were called lambda in the slide, that can be rank ordered from largest to smallest, as seen at the bottom of the slide. They are the diagonal entries of C subscript y, rank ordered from largest to smallest. The eigenvectors are a corresponding set of vectors. The matrix G transpose consists of the set of eigenvectors so that G itself is a transpose matrix of eigenvectors. It turns out that the eigenvalues of the covariance matrix in the original set of bands are the variances of the brightnesses in the new axes. Remember, these axes are the brightness values of the image pixels in the new set of bands. Those new bands are called the principal components of the original image. We generate the actual principal component pixel brightness values using the elements of the eigenvectors of the original covariance matrix. In the next two lectures, we will demonstrate how all of that is done using first a model example and then a real set of images. Those examples will also answer the question as to why it is good to have low correlations among the bands of data. This summary just reminds us that the steps in principal components analysis are simple, provided we have the image processing software available. These questions require an understanding of how to multiply vectors and matrices. They provide useful guidance for material to come. 

### Module 1 Lecture 18 The principal components transform: worked example

In this lecture, we are going to do a set of hand calculations of a principal components transform. You will not have to do this in practice. The purpose of the exercise is to demonstrate the importance of the eigenvalues and eigenvectors of the covariance matrix, and how we should interpret their specific value. We reiterate the important aspects of the principal components transformation. The equation at the top shows how we generate the principal components, that is, the new bands y, from the original set of bands x, that relies on the elements of the transformation matrix G. We get the elements of G by computing the eigenvectors of the original covariance matrix C_x. Remember, G is the transpose of the matrix of column eigenvectors. The covariance matrix of the transformed data, that is, in the y coordinate space, is the diagonal matrix of the eigenvalues of C_x. Remember there are four steps in obtaining the principal components as we saw in the previous lecture. We will compute each of these steps in our example. Once we have finished the example, you should be quite familiar with how the principal components of an image come about. We will do our calculations using the highly correlated dataset from the previous lectures. Here we see the data distribution again. It's covariance matrix and the equation we need to solve to find its eigenvalues and eigenvectors. That equation is called the characteristic equation, which is in the form of a determinant. Lambda is an unknown, which will be given in the solution to the equation. It has several values which will be the eigenvalues we are looking for. As we noted in the last lecture, the identity matrix is just a diagonal matrix with ones in the diagonal positions. When multiplied by the scalar Lambda, it gives a matrix of zeros except with Lambda in each diagonal position. To solve the characteristic equation, we substitute for the covariance matrix as shown here. When matrices are added or subtracted, that is done element by element, also as seen here. For this simple two-dimensional case, the determinant is evaluated using the expression in gray on the right-hand side of the slide. When we use that formula, the quadratic equation in the unknown Lambda results as shown. For greater than two-dimensions, the evaluation of the determinant is less straightforward, but software procedures are available for that purpose. The characteristic equation has the two solution shown. What do they represent? The values of Lambda are the eigenvalues of the covariance matrix of the data, that is, the pixels in the original coordinate system, that is, in the original set of bands. We can therefore write down immediately the covariance matrix in the new principal components coordinates, which, remember, is the diagonal matrix of eigenvalues. This tells us that the variance of the data in the first of the new principal coordinate x is 2.67, and that in the second principal component is 0.33. Thus, we expect to see the data scattered predominantly along the first principal component as expected from the diagram we used initially to start our thinking about the principal components transformation. It is common to express the amount of the variance accounted for by each of the principal components. Here we see that the first component accounts for 89 percent of the scatter of the data. Having found the eigenvalues, we can now proceed to find the corresponding eigenvectors. When we have done so, we will then be able to form the transformation matrix G. There are as many eigenvectors as there are eigenvalues; one corresponding to each. Let's start with the largest eigenvalue, which we now called Lambda_1. The corresponding eigenvector is a solution to the vector equation shown on the slide. When we substitute in the values we know, we get a pair of simultaneous equations in the unknown components of the eigenvector. Both the equations are in fact the same and yield the single relationship between the two eigenvector components shown. We now introduce a new bit of information, that is, that the eigenvectors have unit magnitude as seen in the middle of the slide. The magnitude of a vector is the square root of the sum of the squares of the elements. Since that sum has to be unity, we don't need to worry about the square root operation. It is sufficient to say that the sum of the vector elements squared has to be unity. When we substitute into that, the solution to the eigenvector equation above, we now have unique values for the two components of the eigenvector, allowing the eigenvector to be written as shown in the second last equation on the slide. If we go through the same process using the second eigenvalue, we get the last equation on the slide. We now write the two eigenvectors side-by-side in matrix form and then transpose that matrix to give the required matrix, which transforms the original dataset into the set of principle components. The second half of the slide applies to transformation matrix to the dataset we are working with, yielding the new [inaudible] values for the pixels in the new y coordinates. Here we see the data plotted in the new principal components coordinates. Even visually, we can assess that the data shows no obvious correlation. It's maximum spread variance is in the first principal direction, and the second largest spread is along the second principle axis. Note that the scales of the abscissa and ordinate are different here, which tends visually to mask the fact that the variance horizontally is much greater than the variance vertically. If we were dealing with data of high dimensionality, that trend of decreasing spread would continue with each subsequent component having progressively less variance. We now need to see what all this means in the context of a real image example, which is the subject of the next lecture. Here, we reiterate the essential steps in producing a principal components transformation. The first two questions here are just to test your understanding of the meaning of the eigenvalues in principal components analysis. The last question is particularly important and will arise time and again whenever you use principal components analysis. It is a common question in any situation where we seek to discard low variance components of a transform dataset, not just with PCAs, but with other transforms as well, that compress data variance into a small number of components. 

### Module 1 Lecture 19 The principal components transform: a real example

We now wish to consolidate the lessons from the last couple of lectures by looking at a real example. We will use the two datasets in this real example, both drawn from the same sensor. The first at the top is a four-band set where the bands are spaced over the spectral range. The first two are in the yellow part of the spectrum, the third is in the near-infrared, and the fourth is in the middle infrared. This dataset has a low degree of correlation. In the second dataset at the bottom, we have intentionally chosen four wavebands close together in the near infrared range. This dataset has a high degree of correlation. You can tell that by inspection because the bands all look the same. In each case, we also show a color composite image product created using the band- color combinations shown under the color product. The color composite for the correlated dataset is not very colorful, whereas that for the low correlation dataset is colorful. The color very much helps us pick out details among the cover types as we have come to expect from color image products. Now let's examine the principle components for the first of those datasets. That is the one which shows low correlation. This slide shows the computed eigenvalues of the covariance matrix and the eigenvectors here displayed by row so that we can see which eigenvector corresponds to which eigenvalue. For all of these examples, the images have been contrast enhanced separately, otherwise the lighter principal components might all appear black. Note that even though the eigenvalues show rapidly decreasing variances, the components PC2, PC3, and PC4, still show a lot of detail, such that when the second and third principal components are used with the first to create a color product, a fairly colorful result occurs. Compared with the original color composite of the same, there is a little more detail evident in the PC color composite than that formed from the original bands. Now look at the principle components of the second highly-correlated dataset. Here we see a much more rapid drop-off in the eigenvalues, telling us that most variances in the first principal component, which is what we see when looking at the PC images. Again, the second, third, and fourth components would be to dataview if they were not individually contrast enhanced. However, don't overlook the fact that while their variances might be almost negligible, there can be significant detail in later PC images. Look at the role of buildings in PC4, for example. In this case, the color composite formed from the first three principle components shows significant detail not evident in the color composite formed from the original bands. Not especially the alarm green areas in the upper right-hand side of the image and several of the buildings down the left-hand side. 

Here we compare the two principal components sets on one slide, that especially the different drop-off in the eigenvalues, the more apparent detail in the PCs in the top case, meaning the variance for that image is more shared among the PCs compared with the bottom in which the variance is highly compressed towards the earlier principle components. Also how PC2 and PC4 in the second case, seemed to highlight particular features. This is the same set of PC images as on the previous slide. By looking at the color composite comparison, we note that there will generally be a more wholly colored product from the PCs if the original dataset has low correlation among the bands. Another thing to note, in both cases, is that colors appear in the PC color composites that we almost never see in the color composites full from the original data. Lime green and bright purple are examples. That is a direct result of removing the correlations, and we will explore that more later in our course. This slide puts it all together so you can see the comparison between the two examples directly. The general conclusion we should draw is that if the bands of an image dataset are not strongly correlated to start with, then computing the principal components images will not achieve a great deal. There will always be some visual effect unless there is absolutely no correlation in the original data. In general, the more correlation there is among the original bands, the greater will be the effect of principal components analysis. This can definitely be seen in the second example. Computing PCs gives a much better color result, for example, than with the original data. This slide allows you to see the original data and the principle components for both examples. Note that you can get an idea of the correlation among the bands just by looking at them. In the first example, they looked different from each other and thus exhibit low correlation. In the second example in the third row, they're almost all of the same. That is why the color composite looks almost monotone. It is in cases like this that principal components has the greatest effect. Two important points emerge in the summary. First, even though later principal components may contain very little variance, they may nevertheless contains some interesting detail, detail that is quite different from the rest of the image. Secondly, we get much better use of color with principal components symmetry than with the original data. These questions stress the importance of the actual values of the eigenvalues and eigenvectors of the covariance matrix 

### Module 1 Lecture 20 Applications of the principal components transform

We now want to examine the number of applications of principal components analysis to see how it is used in practice. There are essentially four areas in remote sensing image processing where the principal components transform finds application. The first is to produce color imagery that is easier to interpret manually, particularly when special features are important. The second is to compress imagery in terms of data volume to benefit efficient transmission and storage. The third is as an aid in classification, in which we try to reduce the number of features that define the data vectors that have to be labeled. The fourth is in detecting changes between two images of the same region taken at different times. We will now look at each of those in turn. Often in manual image interpretation, that is, photointerpretation, we use the displayed colors and our knowledge of spectral reflectance curves to identify various regions on the Earth's surface, giving them labels corresponding to the cover types of interests. However, in some applications, the colors themselves are not so important as such, but rather they can be an aid to seeing special structure in an image. That is particularly the case in exploration geology, where the delineation of surficial lithological units is important as is the identification of linear features, that is, lineaments. The vivid colors produced in principle components imagery, especially for highly correlated images, often allow structure to be seen that is otherwise very difficult to discern. We will now show an example from geology. As is often the case, geologically, significant features occur in barren regions for which the bands recorded by remote sensing instrumentation tend to be fairly similar and thus highly correlated, especially in the visible and near infrared regions of the spectrum. It is exactly this type of image that benefits from an application of principal components analysis. The images down the left-hand side of this slide are the four Landsat multi-spectral scanner bands of a region called Andamooka near Central Australia. The standard color composite formed by displaying the second near infrared band as red, the red band as green, and the green band as blue is shown on the upper right-hand side of the slide. The second column of images on the left are the four principal components generated from the original bands. The color composite formed from the principal components shown on the bottom right-hand side of the slide allows features and regions to be seen that can't be discerned easily on the original color composite and in sites of particular value in region delineation in geology. The next slide shows an example with a similar intent, although this time in a region in Australia's northern territory. The top row of images are the six reflected bands of a Landsat thematic mapper image segment recorded in the north of Australia. The thermal band has been left out. Shown also is the color composite formed from the standard mapping of near infrared to red, red to green, and green to blue. In the middle strip are the six principle components of the TM image segment. Two different PC color renditions are shown by choosing different sets of PCs with which to create the color products. Look for features that are easier to see in the PC color composites than in the standard color composite formed from the original bands. For example, note the good differentiation of water features in the second principal component example. Note also the discretization noise in the later principal components. That occurs because of simple variance, that is, the actual variance of the real information left over in the principal component. That variance is so small that the actual radiometric resolution of the sensor is showing up as the limiting factor. Now consider the use of principal components with data compression. Because the lower principle components of an image often contain little variance, and by implication, little information, they can often be discarded without sacrificing much useful information on the average. Each PC that is discarded saves that much data volume for transmission or storage. If the original image is to be reconstructed from the remaining PCs, the inverse of the transformation matrix is used with the discarded PCs replaced by matrices of zeros. When we come to look at methodologies for use in classification algorithms to produce thematic maps, we will often try to limit the number of bands or features used in the process. There are, as we will see, a number of good reasons for doing that, including the speed of the process. The original number of bands as recorded, defines the features to be used in classification. In our classification methodologies, we will look to reducing that number. There are actually many ways of reducing the number of features in a classification exercise. The use of PCs is one of the simplest, which we will now consider in outline. Remember, there are as many PCs as there are original bands. If we transform the data, we can remove the least important PCs and thus have reduced data dimensionality to handle. In other words, the original feature vectors, which have as many elements as there are bands recorded, are replaced by vectors of PC brightness values, but with only as many elements as the number of PCs retained. In some cases, that can be a very large reduction in the dimensionality of the feature vectors, giving a major improvement in the efficiency of the classification process. We will demonstrate this technique later in the course. We now come to a very interesting application of principle of components which had its origins in a 1979 conference paper and has been rediscovered many times since. It relates to the use of principal components to detect features which have changed from image to image in a multi-temporal data-set. We will develop this application of principle components by considering a model flooding example. Following which we will look at a real case involving bush fires. Both examples involve two images of the same region of the air which means the images have to be registered beforehand. We will be processing a before and an after image together, and thus we'll be dealing with twice the number of bands as with a single image. While that will happen in our life, a real example, we will work our analysis through here by just looking at a near-infrared band from before the event of interest and the corresponding bands from the image taken after the event of interest. As will become clear, it is important if the technique is to work well, that the number of pixels in the image which change as a result of the event of interest, not make up more than about 10-20 percent of the same. All other pixels remain the same from image to image. Apart from natural variations and possible seasonal differences which give rise to changes in the level of solar irradiation. This two-dimensional multi-temporal spectral space allows us to see what happens with a dynamic event such as a flood. We have plotted the near infrared brightness value of the image pixels in a space with the IR response in the first type plotted horizontally, and that for the second they've plotted vertically. Pixels which do not change from image to image except for natural variations scatter about a diagonal in the spectral domain in a roughly ellipsoidal fashion. The angle which sets scatter marks to the axis will be determined by the relative solar irradiation levels. If there are no seasonal differences, it will be close to 45 degrees in this two-dimensional space. Any pixels that were originally vegetation or soil but have become flooded in the second day will appear as a group in the regional space defined by high IR response in type one, and low IR response in type two. We now envisage where the axis of the multi-day principle components transform will appear in the space. Recall that the first PC will be in the direction of maximum data variance, and the second will be at right right that is orthogonal to it, and in the direction of the second most data variance and so on in general. Also, the PCs are essentially just a rotation of the original axis in the anti or counterclockwise direction. These axes are shown in red dashed form in this slide overlaid on the original data-set. In the second PC, we see that the flooded pixels are readily separated from those which don't change substantially from day-to-day. This is emphasized on the next slide shown separately for convenience. We now see why it is important that the change pixels not occupy too much of the scene. We want the first axis to be set by the pixels that are essentially constant. Here we see, explicitly, that in the second principal component, the static cover types occupy a different range of values of PC2 than the range occupied by those cover types which change between images. Thus, they are easily separated and will appear with very different brightness values when displayed. Here the photo pixels would appear to be dark by comparison to those which are relatively constant between those. Note that negative PC brightness values can be generated. That is not a problem, theoretically. But for display, all brightness values need to be positive. That can be handled by adding a constant to all the second PC values to make them all positive, but that does not change the results. Having established the basis of the technique that the second and like the PCs of a multitemporal dataset will show up isolated regions of change, we can now look at the real bush fire example. This employs the same pair of images that we used in the image-to-image registration example of Lecture 11. What is appealing about this case is that there are fire events in both directions. The fire scar from 1979 is regenerating in the 1980 image, while two new fire scars appeared in 1980 that were not there in 1979. As for the third example, this slide shows where the 2PC axis would appear in the multi-diadspectral domain. Again, highlighting that the major changes show up in a second PC, at least in this two-dimensional illustration. This slide reminds us that the exercise involves land set MSS data, for which there are four bands in each date. We are dealing with an eight-dimensional multitemporal image, and from which we compute eight principal components. When we look at the results in detail, we will say that the first PC looks like a weighted sum of all the eight original bands and is thus often referred to, not quite correctly, as a total brightness image. Here are the eigenvalues and eigenvectors, by row, of the eight-dimensional multi-temporal image. Note the rapid drop off in the sizes of the eigenvalues, and note that all the elements of the first eigenvector are positive. That's why the first PC is a weighted sum of the original set of bands. All other eigenvectors have some negative elements, which is why they are good at highlighting changes that have occurred between the dates. Here we see the first four principal components from the 1979-1980 image pair. We cannot see the change events in the first PC, but they are very obvious in PC2, PC3, and PC4. We will see those changes more vividly if we form a color image from those three PCs as in the next slide. Look carefully at the first principal component. It seems to have suppressed the fire events and looks like a very good image with clear topography. Here we share the color composite PC image created with the mapping Schalin, that is PC2 to red, PC3 to green, and PC4 to Blue. Note that the 1979 fire scar, which is re-vegetating in 1980, shows up as a red to magenta color. The two new fire scars show as lime green, and two stream beds on the southeast part of the image shows deep blue which in this case represents complete re-vegetation from the 1979 fire. This example has served to highlight the usefulness of the principal components transformation for highlighting differences from image to image. You should be able to find many other examples in the literature. Examine the first PCs in the examples of this lecture and see how as total brightness images, the holler topography, whereas topographic detail is missing in many of the PC images. The questions in this quiz asked you to think about computing the covariance matrix of a subset of an image. That allows much greater flexibility in using PCs, especially in applications like change detection, and in some cases, for feature reduction prior to classification. 

##  Module 2

### Welcome to Module 2

We now come to module 2, which is key to understanding how to interpret images in remote sensing. In this module, we treat image analysis in depth. It is the most mathematical of all three modules. If your background is not mathematical, please don't be discouraged from working through the material. We step through it carefully and provide surrounding commentary so that, even if the mathematics challenges you, you should still understand what we're doing and appreciate the outcome. The module builds on the material of module 1. In particular, it starts with the assumption that images have been recorded of regions of the earth's surface that are of interest to us, and that any errors in those images have been corrected. It is based on the model of the spectral space. That is the fundamental descriptor of the image data that is used throughout all remote sensing image analysis. At the end of this module, you should understand first, the steps of training and classification, supervised classification, the concept of the thematic map, and then the maximum likelihood classifier, the minimum distance classifier, the support vector machine, and the neural network. We will then spend time on the convolutional neural network and deep learning, which is the most recent technique. We then finalize the module by talking about clustering with examples from unsupervised classification. 

### Module 2 Lecture 1: Fundamentals of image analysis and machine learning

In the first module, we looked at how remote sensing imagery is acquired. We also looked at how the imagery is affected by atmospheric conditions and various geometric distortions. We are now at the stage where we can concentrate on how to analyze the image data in order to extract information of importance to operational remote sensing. This module focuses on the range of machine learning methods that are used commonly for image interpretation in remote sensing. We will start with some simple historical algorithms and move on to procedures that have gained popularity in the last two decades. The material we are going to develop in this module requires an understanding of basic statistics, calculus, and vector and matrix analysis. If you do not have that background, the summaries should provide an overview. Also, we will step through the analysis slowly and in detail so that you should at least pick up the essence of what is being developed. When we finish this module, you should be in the position to appreciate how a range of popular machine learning methods can be applied in remote sensing. It is important to understand that from an operational point of view, you will not have to engage yourself in the range of mathematics we are to encounter when applying the various algorithms in practice. Instead, commonly available software packages used for remote sensing purposes will contain the algorithms in the form that the user can employ easily. By way of summary, recall that what we are doing is taking the multispectral, or hyperspectral measurements recorded by satellite or aircraft instrument and via an appropriate machine learning technique, reducing a map of labels for each of the image pixels. Those labels represent classes of groundcover. In other words, we're doing a mapping and mapping from recorded data to a thematic map of labels. There are several different classification scenarios that we will come across, all are shown here. In the first, we analyze pixels individually based on their spectral measurements and produce labels for them. Those labels should represent the groundcovers in which we are interested. Sometimes these are called point classifiers because they focus on individual points of pixels. In the second, while we're still interested in labeling individual pixels, some algorithms allow us to do so by taking into account the possible labels on the surrounding pixels. That is called context classification, or more properly, spatial context classification. It is based on the idea that neighboring pixels are likely to be of the same groundcover type, that is the same class. In the third case, we show the need to identify pixels using a range of sensor and data types, here, optical, radar and thermal. Algorithms for this so-called multi-source problem are difficult to come by. Although some authors ignore the intrinsic differences in the data types and simply concatenate them into a single vector to which they then apply classification techniques. Finally, particularly with high spatial resolution imagery, we might be interested in identifying objects. Those objects might be buildings in urban landscapes or aircraft and other vehicles in surveillance applications. Most of our work in this module will focus on the first scenario, but we will later on engage with the concept of spatial contexts. Remember from our earlier work, classification or machine learning in remote sensing consists of a number of stages. The first is training. Here, sets of known pixels are used to train the classification algorithms that we're going to use. The second stage goes by a number of names. Most commonly, it is referred to as the classification step in remote sensing. In the machine learning community, it is called generalization. It can also be called labeling or allocation. The third stage is a central one. When developing a classification, we need to know how well the trained classifier works on real unseen data. It is generally called the testing phase and involves sets of labeled pixels that the analyst has put aside in order to check the performance of the classifier. We'll have more to say about this later, especially in Module 3, where we will treat the need to test the accuracy of a thematic map in some detail. We are now going to embark upon the development of four common classifiers. The first is the maximum likelihood classifier, which was the main algorithm used in remote sensing for many decades. It is still highly usable, particularly when the number of wavebands is small, but it does have limitations in respect of hyperspectral data, and these methods are employed to reduce data dimensionality beforehand. We will look at some of those methods in Module 3. The second method we will consider is the minimum distance classifier. Again, a long-standing technique which is often useful in its own right, but more importantly, it provides a good basis upon which to develop the next two methods. The third procedure we will analyze is the support vector machine. It is a popular classification method for use with hyperspectral data. Finally, we will look at the neural network. While that predates the support vector classifier, it has a later version called the convolutional neural network that has gained particular popularity in the last five to 10 years. It often goes by the title of deep learning, which we will explain during the treatment. It is important here that we differentiate between supervised and unsupervised learning. Supervised classification is that in which labeled training data is used to establish values for the unknowns in the classification algorithm. When training data is not available, unsupervised techniques can be used to discover the class structure of an image dataset. We will look at unsupervised methods in the last week of this module. They are based on the data analysis process called clustering. Before we commence developing the classifiers, it is important to distinguish between what we, as users, think of as classes in the landscape and what machine learning algorithms see as classes in the data. The two may not be the same, although we do hope there is a close mapping between them. For example, we would expect that pixels recorded by an instrument will tend to group in regions in spectral space if they belong to areas on the ground that exhibit the same spectral response. In remote sensing, such spectral groupings are called spectral classes, or sometimes data classes, since they are the natural groups or classes in the data. The set of classes in which the user is interested are called information classes. They have names like natural vegetation, water, forest, crops, and so on. One would hope that they would align one for one with the spectral classes, but that is often not the case. Part of the skill of the analyst is to discover whether such a mapping exists or whether several spectral classes may be needed to represent properly each information class. As a simple example, a single crop class may have several constituent spectral classes, because the crop will exhibit slightly different spectral responses depending on the soil types on which it is sown, the availability of groundwater, and whether any particular portion of a crop is in shadow. We will have more to say about the relationship between spectral and information classes when we come to Module 3 of this course. But unless we say otherwise, we will assume in this module that our information and spectral classes are the same. Here we've summarized the important points raised in this introductory lecture. We talked about the different types of classification, the steps involved in classification, the difference between supervised and unsupervised classification, and the difference between spectral and information classes. Use these quiz questions to rehearse what you understand classification will do for you. Also, pay particular attention to the last question because it reveals some interesting properties of the data space or spectral space used in remote sensing. 

### Module 2 Lecture 2: The maximum likelihood classifier

In this lecture, we're going to develop the maximum likelihood classifier. We will see that it has a couple of variance. At all tend to reduce to the same underlying principle. It is a value to recall that we modeled the data in a spectral space which has axes aligned with each of the spectral measurements of the remote sensing instrument. Here we use simple two-dimensional version which will be sufficient for later developments. But as a result of the mathematics we employ can be conceptually generalized to any number of dimensions. Importantly, we note that pixels of a given ground cover type tend to congregate together in regions of the spectral space. Although we admit the possibility of several spectral classes per information class, for simplicity in this development, we will, as we said earlier, assume that each information class can be represented by a single spectral class. Here we have shown a single spectral reflectance curve for each cover type such that there is a single point per class in the spectral space. In practice, of course, variations will exist as shown in the next slide. The natural variations in each particular cover type will exhibit as clusters of points in the spectral domain, as seen here. The locations of the classes are consistent with the spectral reflectance data and tend to group around appoint indicative of the main of the relevant spectral reflectance curve. The starting assumption for the maximum likelihood classifier is that the density of the cluster of the pixels of a given cover type in the spectral space can be described by a multidimensional normal distribution, sometimes called a Gaussian distribution. While this may seem a restrictive assumption, the normal distribution is convenient because it and its properties are well understood. And even if the actual density distribution is not normal, the normal model nevertheless works well. Also, later, we will represent a given class by a set of normal distributions, which helps in overcoming many concerns with using this model. What are the parameters of a normal distribution? They are the main position and standard deviation in one dimension. Generalizing to multiple dimensions, the parameters are the main vector and the covariance matrix. The mean vector describes the location of the density maximum or the center of the spectral class in the spectral space. And the covariance matrix describes the spread in pixel density about the main position. The covariance matrix is the multidimensional version of the variance of a one dimensional distribution. The formulas for the single and multidimensional distributions as seemed to have a similar structure as is to be expected. We now focus the formula for the multidimensional normal distribution on a particular class, class i. We indicate that by the subscript i to the main vector and covariance matrix, and further right at the probability itself as conditional on the fact that we are looking at class omega subscript i. We will adopt that nomenclature throughout this series of lectures. The class of interest will be indicated as omega subscript i. The probability of a pixel existing at position X in the multispectral space will be indicated as the conditional probability shown on the left-hand side of the equality here. And the class parameters, m subscript i and C subscript i sometimes collectively called the class signature will have subscripts indicating the particular class. Note that if we have training data available for that class, then we can estimate values for the components of the mean vector and the covariance matrix. We now assume a normal probability distribution for each class. Using training data, we estimate the means and covariances Of the classes. Once trained, we can then choose to allocate and unknown pixel with measurement vector X to the class of highest probability. Note that we are explicitly representing the probabilities as class conditional probabilities. And because we're choosing the class of the highest likelihood or probability, this approach is called maximum likelihood classification. In the diagram, note how the contours of equal probability segment the spectral space into class regions. There is however, a better, more general approach. He'll make an interesting distinction in conditional probabilities. At the top we show the class conditional probabilities. The set of conditionals tell us the probabilities of finding a pixel, a spectral location X. I with measurement vector X from each class. Consider now the second set of probabilities on this slide. That expression says that, given we are interested in pixels at position X in spectral space, what is the probability that the corresponding class is omega subscript I? This is a much better measure to use than the class conditional probability, since it focuses on the correct class for the pixel rather than the likelihood that there is a pixel from a given class at position X. That gives us another way of classifying unknown pixels. If we knew the full set of these new probabilities, then we could classify an unknown pixel with spectral measurement vector X by allocating it to the class of the largest of the set. We expressed that in the form of a decision rule as shown symbolically on the slide. The new probabilities are called posterior probabilities. The reason for which will become clear soon. The problem is, we don't now at the posterior probabilities, only the class conditional probabilities, which we have estimated from training unlabeled pixel data. The very famous Bayes' rule gives us a bridge between the two types of conditional probability. It is shown at the top of this slide. It introduces two you, probabilities P of ambiguous of i and P(X). The latter is just the probability of finding any pixel with measurement vector X. When we use Bayes' theorem in the decision rule of the previous slide, noting that the probability P(X) is common to both sides. It can be dropped out. That leaves the rule shown in the second equation, but what is the new probability P(m) because sub I. The probability P(m) equals sub i is the locker hood of filing a pixel from class ambiguous sub i anywhere in the image. That is, it is not X dependent. It is a property of the scene itself and is called the prior probability because it is the probability with which we can guess the class membership of an unarmed pixel without the benefit of the remote sensing measurements. For example, if we knew the rough proportions of the classes, we could use them to guess, supplies based on areas. In contrast, the P omega sub i given X are called posterior probabilities because they are the probabilities with which we assess the class membership of a pixel. After we have carried out our analysis using the information provided by the measurement vector X for the pixel. If we had no idea of the prize, we could assume that they were all equal. In which case the new form of the decision rule of the previous slide reverts to the one we started with, in which the decision is made using class conditional probabilities. That is, the distribution functions rather than the posterior probabilities. Again, this slide summarizes the important points that classes or pixel tend to cluster in multispectral space. We described as clusters by normal or Gaussian distributions. We estimate the mean vectors and covariance matrices of those distributions from available training data. We can make clusters. Decisions for pixels based upon a comparison of the probability distributions, or preferably the posterior probabilities, if we happen to know the prior probabilities. In the last question, you are asked essentially to draw Bayes' Theorem. It is based on the concept that the joint probability of two events happening simultaneously is not order dependent. 

### Module 2 Lecture 3: The maximum likelihood classifier—discriminant function and example

In this lecture, we will develop some further aspects of the maximum likelihood classifier and provide an example. This is a slightly longer lecture because we have kept the material together in a related group of topics. In this slide, we take the decision rule we have been using up to date, and now substitute in that expression the formula for a multidimensional normal distribution. As we will see, some simplifications are possible. That mean, that we do not have to evaluate the normal distribution itself on each occasion when we wish to label a pixel. To make the results simpler, we take the natural logarithm of the product of the class conditional and probabilities. As is well known, the result is the sum of logarithms. Since the normal distribution contains an exponential term, taking the natural logarithm means the exponent is removed. As we will see in the last three steps on the slide. We have decided to name the logarithm of the product of probabilities as the discriminant function for reasons which will become clear shortly. When we examine the form of the discriminant function, we see that the first term contains no information that will contribute to discriminating among the classes. Therefore, we remove that term to give the slightly simpler version shown in the center of this slide. We now express the decision rule in terms of the discriminant functions. Which is a form used in remote sensing image analysis software. They are called discriminant functions, because they are used to discriminate among the classes. Now consider a simple example, we met this one briefly in the introductory material in module one. The image segment is a portion of a Landsat Multispectral Scanner image recorded in 1979. There are four obvious ground cover classes: vegetation, burned vegetation, which we'll label as fire burn, urban and water. The first task in a supervised classification is to select training data. That is, pixels whose ground cover labels we know. In practice, that may be an expensive and significant exercise, sometimes requiring site visits, the use of reference maps, photos and the log. But in this case we can see the groundcover types easily in the image, so that we can determine a set of training pixels by inspection. The numbers of training pixels per class are shown in the table. When the training data is presented to the classification software, the first output is a set of class signatures, that is the mean vectors and covariance matrices for the four classes, as shown in the table on this slide. As we noted in the first module, when trading the principal components transformation, the covariance matrices are symmetric. Once the classifier has been trained, it can be used to label all the pixels in the image as shown in the thematic map of this slide. Not only is a map reduced, but we also have a table of the numbers of pixels per class, which we can translate into areas shown here in hectares. Note that the training pixels represent only 7.5 percent of the scene. That is a benefit of supervised classification. By putting effort into labeling a small number of pixels ourselves through training, we gain the advantage of having the classifier label a much greater number for us. One of the benefits of classifiers, such as the maximum likelihood rule, is that the main factor elements represent the average spectral reflectance curves of the cover types as seen here. In those curves, we can identify many of the important properties of the scene, such as the loss of grain biomass through the fire and the fact that the urban zone is a mixture of bare surfaces and vegetation. An important consideration when using any supervised classification procedure is to know how many training pixels per class are required. For an N dimensional space, the covariance matrix has N times N plus 1 divided by 2 distinct elements. To avoid the matrix being singular, at least N times N plus 1, independent samples are needed. Fortunately, each N dimensional pixel vector contains N separate samples in each certain spectral measurements. Therefore, a minimum of N plus 1 independent training pixels is required. We usually try to obtain many more than that so that we can assume independence and get good reliable estimates, of the elements of the mean vector and covariance matrix. With the maximum likelihood classifier, experience has shown that we should look for a bare minimum of 10N training pixels per class, but desirably at least 100N training pixels per spectral class with more than that, if at all possible. For data with low dimensionality, such as multispectral images, the minimum of 10-100N pixels per class is usually easily achieved, but when we have to work with hyperspectral images, there can often be a problem with not obtaining enough independent training pixels. For example, if there are 220 wavebands, we should be looking for about 20,000 labeled pixels per class for good training. Clearly, that is often a difficult target to meet. If we wish to use a maximum likelihood classifier with hyperspectral data therefore, we have to use so-called feature reduction techniques to allow the data dimensionality beforehand, or else resort to other classification methods that don't require as many samples for reliable training. We will look at those procedures soon. This slide illustrates the point of not having enough training samples per class with increasing data dimensionality. The example is from an old paper, but nevertheless illustrates the problem quite well. It involves an exercise with 400 training pixels per class over five classes. As the number of bands or features increases, there is an improvement in classifier performance or generalization, but beyond about four features or dimensions, the performance drops because of the poor estimates of the elements of the covariance matrix. [inaudible] for five features we should be looking for at least 500 pixels per class if possible. This problem has been known in remote sensing for many years and goes under the name of the Hughes phenomenon. It is also an example of the curse of dimensionality, often referred to in the machine learning literature. When we commenced our discussion of classify methods, we sketched the positions of the pixels from a given class in the spectral domain. Effectively, what our classifiers do is place boundaries between the classes. Pixels from one class lie on one side of a boundary while pixels from another class lie on the other side. Intuitively, one would expect that simple geometric boundaries will be less successful in separating two classes of pixel, than boundaries of a higher geometric order. The way we find these boundaries for any classification algorithm is to find the locus of points in this spectral space for which the two relevant discriminant functions are equal. By doing so, this slide shows that the locus of points for the maximum likelihood classifier is quadratic. In other words, the boundaries between classes are high dimensional circles, parabolas, ellipsoids, and so on. If you go back to one of the slides in the last lecture, when we started our work on the maximum likelihood classifier, you will see those sorts of boundaries or decision surfaces in the diagram, showing three intersecting two-dimensional normal distributions. Here we illustrate the quadratic nature of the surface between two classes on the left and on the right the rather more complicated decision surfaces that can be obtained if you allow more than one spectral class per information class, which we will treat later. So in summary, we use the discriminant function to discriminate among classes. Supervised classification requires training using a small set of data, and the payoff is labeling of all the pixels in the scene. Because second order statistics are involved in the elements of the covariance matrix, it is often difficult to acquire enough training samples for the maximum likelihood classifier to get good estimates of the covariance. Some of these questions ask you to think about the shapes of the decision boundaries for the maximum likelihood rule under different condition 

### Module 2 Lecture 4: The minimum distance classifier, background material

We now commence a journey towards the development of more complex classifiers. To do so, we're going to look at another very simple algorithm that underpins our further development. This is called the minimum distance classifier. It is even simpler than the maximum likelihood rule. Consider two classes of data which are linearly separable. That is, they can be separated by a linear surface or straight line in two dimensions. If we knew the equation of that line, we could determine the class membership for an unknown pixel by saying on which side of the line its spectral measurements lie. How can we express that mathematically? The equation of a straight line is pretty simple in two dimensions as shown here. It is helpful though to write it in the generalized form shown, since that allows it to be taken to any number of dimensions as seen on the bottom of the slide. Incidentally, in more than two dimensions, we refer to the linear surface as a hyperplane. Here we write the equation in vector form, which is compact and allows manipulation by the rules of vector algebra when needed. Note that we can use either the transpose expression or that using dot products, both are equivalent versions of the scalar product. When we use the equation of the hyperplane in classifier theory, we often refer to the vector of coefficients Omega_i as a weight vector. Usually Omega_n plus 1 is not included in the weight vector and instead sometimes called the offset or bias. Having expressed the hyperplane in vector form, we now have an elegant expression for the decision rule to apply in the case of a linear classifier. That's the rule shown in the box in the middle of the slide. The rule evaluates the polynomial for a given value of the measurement vector. If it is positive, then the corresponding pixel lies to the left of the hyperplane and thus is labeled is coming from class 1. If it is negative, then the pixel is from class 2. This decision rule will feature often in our later work and will be the basis of further developments. How do we find the hyperplane that requires finding values for the weights and offset? As with all supervised classification methods that entails using sets of training pixels, we will take that further in the next lecture. In summary, a simple classifier can be found by putting a linear surface or hyper plane between the two classes of pixels. The equation of the hyperplane expressed in vector analysis is simple. The unknowns in that equation are the weights , which we find by training onsets of labeled pixels from each class. These questions simply ask you to verify some of the mathematics in this lecture. 

### Module 2 Lecture 5: Training a linear classifier

In this lecture we are going to look at Training Methods for Linear Classifiers, and in particular the Minimum Distance Method. We will then use that material to help us understand and develop training methods for more complex classification techniques. Remember, we are assuming at this stage that the two classes of pixel that we're dealing with are linearly separable. That is, a straight line can be placed between them. Many datasets, however, are not linearly separable. We will meet some later, but recall for the moment that the maximum likelihood classifier is able to separate datasets with at least quadratic hypersurfaces. There are many acceptable linear decision surfaces that can be placed between two linearly separable classes, as illustrated in this slide. One of the earliest methods for training which goes back to the 1960s involves choosing an arbitrary linear surface. That choice will almost certainly not be in their opposition, in that it will not have the classes on the right sides of the hyperplane, but then our repeated reference to each training pixel intern. The hyperplane is gradually iterated into an acceptable position. The book by Nilsson referenced here shows that method in detail. Know the restriction however, we are only dealing with two data classes. Will have to embed this method into some form of multi-class process later on. A better approach might be to choose as a separating hyperplane that which is the perpendicular bisector of the line which joins the means of the classes, as shown here. We can find that line as the locus of the points that are equidistant from the two class means. Note that we use them in nomenclature, d bracket x comma, m subscript to i close brackets to represent the distance vector between two points. If the distances are equal, then so will be there squares, saving the need to compute the expensive square root operation in software. So we are quite the squares of the two distances from their position x to the class means. Leading to the equation of the linear surface at the bottom of the slide. Not that it has the same structural form as the equation of a straight line that we are familiar with in two dimensions. That also that we have had to use two vector identities in coming to this result. Although we have computed the equation of the decision surface in the minimum distance rule, we actually don't compute the hyperplane explicitly. Instead, to label an unknown pixel, we just compare the distance squared to the class means and allocate the pixel to the class of the closest mean. That suggests that we can actually account for as many classes as we need to. In this slide we have shown three classes and given a general decision rule for the minimum distance classifier. So in summary, will the minimum distance classifier, one training data, is used to estimate the class means. Fewer pixels are needed compared with the maximum likelihood classifier, since now caviar its matrix estimation is required. Two, unknown pixels are allocated to or labeled as the class of the closest mean. Three, it is a multi-class technique. And four, it is fast in both training and classification. We are now at the stage where we can look in summary at the two classifiers we have treated so far and see the steps the user follows in practice. Although in our lectures, we have looked at the mathematics of the techniques, you do not need to know that material in practice. Although it does help you understand how the algorithms work and their comparative limitations. Note that three of the steps are common to both approaches, as highlighted in blue and will also be common to any of the classifiers we look at in this course. They are training, thematic mapping and accuracy assessment. The approaches differ only in how the classifiers are trained and how they are applied to unsent pixel data. If class conditional probabilities are used with the maximum likelihood method, there is an assumption that prior probabilities are available. Again, the software looks after that step, provided the user can assign values to the price. Note particularly the last step really in a real classification task, will the accuracy on all classes be acceptable the first time around. Instead, the analysts may have to consider whether some classes have been missed, leading to their pixels being miss allocated to another class. Or to some classes are too broad, spectrally and should perhaps be subdivided into constituent subclasses or spectral classes. Again, we will have more to say about this when we look at classification methodologies in module 3. We are going to meet a number, but not all classifiers that are used regularly in remote sensing. Some are quite complex and require a lot of user effort to make them work effectively. But they can give exceptionally good results. When selecting a method, though it is important not to go overboard in always choosing the newest and potentially the most complex one. Often the simple algorithms will work just as effectively in many real world applications. Remember what we said at the start of this course? Well, we are necessarily spending a lot of time developing classifier algorithms our course objectives are remote sensing and its applications. So we need to keep that in mind when evaluating classifier methods. Ultimately, we will have to embed them into a methodology which we will do in module 3. The first two questions here just test your knowledge of vector algebra while the last two, set you up for what is to come. 

### Module 2 Lecture 6: The support vector machine—training

We're now going to build on the work we did with the simple linear classifier to develop the support vector machine. A technique that became very popular about 15 to 20 years ago and is still regarded by some as the benchmark classifier for hyperspectral imagery. For those who come from a remote sensing background rather than from the machine learning community, following the full development of the SVM can be challenging. For the same reason, we will not cover the full range of SVM theory in these lectures. But instead present just sufficient off the material so that the important points can be appreciated. For those interested in a full theoretical treatment, although from a generalized Machine Learning and not a remote sensing perspective. See the book by C.M Bishop Pattern Recognition and Machine Learning, Springer New York 2006. The support vector classifier has a similar objective to the minimum distance classifier, in that it is trying to find a hyperplane which optimally separates two classes of data. But it's approach is quite different. In the SVM, we look for the hyperplane which is midway between the two classes. We define it in terms of two marginal hyperplanes that just touch each of the classes as seen in this slide. There are several stages to the full development of the support vector classifier. It is helpful to review them at the outset so we know the direction we are taking in the overall development because the journey through the theory can be a bit tedious. If we're not sure what our objectives are, it is easy to lose our way. The first two steps shown here, one is to find the optimal decision surface on the assumption that the classes are linearly separable. The second is to accept the fact that most datasets will not be perfectly separable. That we will have to adjust our training process to accommodate the fact that there will be some class overlap as seen on the right-hand diagram. The third step in our development of the SVM will be to make an adjustment for data that is not linearly separable. We will then move on to the final stage, which is to accommodate multi-class as against binary dataset. The fundamental support vector machine algorithm is just boundary. Multi-class strategy is required if it is to work with remote-sensing problems that involve several data classes. Let's now look at the first step, finding the optimal position of the decision hyperplane. Again, it has the same mathematical form as a hyperplane for the minimum distance classifier. We now look at the marginal hyperplanes. It is helpful if we make their equations. Take the forms shown on the diagram in blue. We can do that because we can scale the parameters in the decision rule, which involves adjusting the offset. We now need an objective that will lead us to the best hyperplane. We adopted a goal of finding the hyperplane, which is midway between the two marginal hyperplanes, which are furthest apart. That is, they have the greatest margin between them. From vector algebra, we can show that the margin is given by the expression on this slide, where the norm of the vector W is the size of the bit vector. We use the Euclidean norm, which is the square root of the sum of the squares of the vector elements, as is well known. We do that analysis on the next slide. The margin is derived by taking the difference of the two perpendicular distances from the origin to the marginal hyperplanes as shown. Note that when we apply that formula to the marginal hyperplanes, the one on the right-hand side of the equal sign has to be taken into account as part of the offset or intersect as seen in the second, and third equations. Thus, in seeking to maximize the margin, we want to minimize the norm of the weight vector. However, unless we constrain that objective, we could make the margin as large as we like. But that will undoubtedly cause some of the pixel vectors to fall onto the wrong side of their respective marginal hyperplane. We have to introduce a constraint to make sure that does not happen. Such a constrained minimization or optimization problem can be carried out by the process called Lagrange multipliers, in which we set up our Lagrangian function, as in the next slide. The Lagrangian function that we wish to minimize is L equals 1.5 of the norm of the weight vector squared minus the sum over i of Alpha_i f_i. Where Alpha i all positive are a set of parameters called the Lagrange multipliers. The f_i are conditions, one for each training pixel that ensures that the pixels are on the correct side of their marginal hyperplane. We choose as the conditions that ensure all the pixels stay on the respective correct sides of the hyperplanes. That y_i brackets W_T X_I plus w_n+1 close brackets is equal to or greater than one. Or taking the one to the left-hand side, y_i brackets W_TX_i plus W_N plus one close brackets minus one is equal to or greater than naught. The binary variables y_i are indicators of the actual class for the -ith training pixel as shown in red in the middle of this slide. The y_i tech values only are plus one or minus one. Thus, the Lagrangian, to be minimized, is as shown at the top of this slide. We need now to find the weights in the hyperplane expression that minimizes this Lagrangian function. While we are trying to do that, the second term in the Lagrangian is trying to push up its value if pixels are on the wrong side of their respective hyperplane. The hyperplane needs to be placed such that that effectively doesn't happen. The mathematics now becomes a little tedious, but leads to some remarkable and important results. If you choose not to follow the detail, we will still summarize the important results at the end. To minimize the Lagrangian with respect to the weights, we have to differentiate it as seen here, making use of the fact that we can express the vector norm in the form of a dot product, that is w transpose w. The result shows us that we can find a set of weights provided we know the values of the non-negative Lagrange multipliers, Alpha subscript i. This tells us that the decision surface is found from the set of training pixels and their classes, as is usual in the training of any classifier. We also have to minimize the Lagrangian with respect to the offset W_n plus one, which we do on this slide. However, this gives us another interesting condition. Namely, that the sum over all the training pixels set Sigma_i of Alpha y_i is equal to 0. That also from our previous equation C, we get the helpful formula for the square of the weight vector norm as shown in the center of this slide. Our previous Lagrangian formula using the new equation D gives us the expression in equation E. After all this, we are now in the position to find the Alpha subscript i, the Lagrange multipliers. Remember, they are trying to make the Lagrangian large to keep the respective pixel vectors on the correct side so they're separating hyperplane. We now maximize E with respect to those Lagrange multipliers, which we do on the next slide. This maximization is complicated, so it is normally carried out numerically to generate the values of the Alpha_i. However, there is another constraint we can use to help us simplify the situation. It is one of what is called the Karush Kuhn Tucker or KKT conditions. It says that Alpha subscript i brackets y_i bracket W_T_X_i plus W_N plus one close bracket minus one close bracket is equal to naught. Which, when you think about it, is quite extraordinary, since it tells us that either Alpha subscript i equals naught or y subscript i, open brackets, W_T_X_i plus W_n plus one close brackets is equal to one. This last expression is true only for pixels lying exactly on one of the marginal hyperplanes, in which case, the corresponding Lagrange multipliers, Alpha_i, are nonzero. For training pixels away from the marginal hyperplanes, the expression inside the curly brackets is non-zero. The constraint can only be satisfied if the corresponding Lagrangian multipliers, Alpha subscript i are 0. Those pixels are therefore not important to the training process. It is only those lying on the marginal hyperplanes. We call those support pixel vectors since they are the only ones that support training. We now have values for the relevant Alpha subscript i. Thus, we now know all the variables that define the weight vector. Thus, once we have found W_n plus one, we can define the decision surface, the central separating hyperplane. Since the Alpha_i are 0 for all but the support vectors, we found that the weight vector expression is simplified to a sum over just the set of support vectors script S. Once we know the W_N plus one, the support vector classifier has been completely trained. We will see how to do that in the next lecture and look at the next steps needed to ensure that the support vector machine is a practical classification method. The process of minimizing the norm of the weight vector or maximizing the margin between the two classes, constrained by ensuring that the training pixels remain on the correct side of their respective marginal hyperplane, leads to the amazing but probably logical result that it is only those training pixel vectors that lie on the marginal hyperplanes that are important in defining the decision surface. They are the support vectors. In the last question, you will need to use a bit of imagination to come up with the distribution of training pixels. Set the perpendicular bisector of the line between the class mains does not always separate classes which are linearly inseparable. 

### Module 2 Lecture 7: The support vector machine—the classification step and overlapping data

We now take the support vector classifier to the next stage of development by looking at the actual decision rule and how it can handle the practical case of overlapping classes. In this slide, we summarize how decisions about the class membership of an unknown pixel with measurement vector x is determined using the support vector machine. Although the decision rule shown in the center of the slide is general to all linear classifiers, in the case of the SVM, it is made simpler because the weight vector at the top, and that's the decision rule, is specified just in terms of the support vectors, that is, those that lie on the two marginal hyperplanes. Given that the Lagrange multiplier is Alpha_i and the support vectors had been found during training, the decision rule is simple and fast to apply. In this slide, we find the one remaining unknown, the offset W_N plus 1. It is most easily found by choosing a support vector from each class and substituting them into the respective marginal hyperplane equations. Given that we know the weight vector, we can find the value of W_N plus 1. To get a more reliable estimate, sets of support vector pairs can be chosen and the sets of values of W_N plus 1 sets generated can be averaged. We now have to address the next three steps. The problem with the SVM: assuming the classes are completely linearly separable, the fact that it is only a linear classifier, and the fact that it is a binary classifier. We now look at the first of those handling overlapping classes. For real data, it is highly unlikely that two classes will be linearly separated, as in our previous analysis. Instead, there's likely to be overlap as shown in the diagram here. To make the support vector machine able to handle such realistic situations, we need to introduce some slackness into the training process, by which we mean we cannot achieve the best separating hyperplane but probably one that is as best as we can do. The approach taken is to introduce a set of slack variables, Xi_i, one for each training pixel. Their role is to allow a softer decision to be taken considering the class membership of a pixel. Note the new version of the decision rule at the bottom of the slide. We choose values for the slack variables as shown on the slide. If there are zero, then the corresponding pixels are correctly located. If they are unity, then the pixel sits directly on the separating hyperplane. If they are greater than one, then the corresponding pixels are on the wrong side of the separating hyperplane. Otherwise, they lie on the correct sides of the separating hyperplane, but in the gap between that decision surface and the correct marginal hyperplane. Since the Xi_i are positive, the pixels located in the wrong class or in the gap between the marginal hyperplanes and the decision surface, their sum is a good measure of the total error incurred by poorly located pixels during training. What we want to do now is maximize the margin as before, but also minimize the error caused by poorly located pixels. We have a decision to make. Is it more important to maximize a margin or to minimize the class overlapping pixels, or should we seek a compromise? We can find a compromise by adding a proportion of the sum of the Xi_i to the norm of the weight vector as a new measure to be minimized, as seen in the equation at the bottom of this slide. However, that minimization has constraints on it. There are two constraints. Similar to the case of non-overlapping data, we seek to ensure that the argument y_i brackets W_t x_ i plus W_N plus 1, close brackets, minus 1 plus Xi_i remains positive. Remember previously this constraint did not contain the Xi_i. Secondly, we need to ensure that HXi_i remains positive as defined. Again, using the process of Lagrange multipliers, that leads to the Lagrangian at the bottom of the slide, which we now seek to minimize. As with the case for non-overlapping data, a numerical solution is used to find the two sets of Lagrange multipliers. However, beforehand, the user has to specify the value of the parameter C, usually called a regularization parameter. We will see how that is done when we come to the examples later. But once a value is given for C, the numerical solutions produce the required set of Alpha_i. The decision rule stays the same as before, but the Alpha_i will now not be optimal for a maximum margin, but will reflect the compromise between maximizing the margin and to minimizing the error due to overlapping training pixels. Effectively, what we are trying to do with the slack variable approach, is to recognize that there will be a decision surface which is the best choice in terms of minimizing classification error caused by overlapping pixels. Would some form of trial and error approach be acceptable as an answer to the last question? 

### Module 2 Lecture 8: The support vector machine—non-linear data

In this lecture, we look at the third stage of the development of the support vector machine. We introduced a transform that in principle changes a dataset that is not linearly separable into one that is. We will see surprisingly that we don't actually have to know the transform itself. Let's examine critically the form of the decision rule. That particularly that the pixel vector doesn't actually appear on its own, but rather is always in combination with the support vectors in the form of x_i transpose x. Suppose we now apply an unspecified transformation to the data space, call it phi brackets x. Thus a product of the support vector and the unknown pixel vector will in the transform space, become phi of x_i transpose times phi of x, which we defined as k x_i of x. We call that transform product a kernel, and represent it by that symbol k. Now insert that kernel function into the decision rule. All that we have really done here is transform the original pixel space. But we can interpret the resulting expression as one which says that we need to know the kernel, that we don't actually need to know the transform phi of x that led to it. The real question then becomes, what functions can be used as kernels? Because they represent a scalar product, they have to be decomposable into that form. Some common kernels that satisfy that condition are shown on the next slide. The most common kernels encountered in remote sensing applications are the last two shown here. Although the polynomial could also be used. Note that they have parameters for which we need to choose values. That is often done by running a series of trials to find which value works best, as we will see later. To see how kernels work, we will look at a very simple example. We will use the first example from the previous slide, a quadratic kernel with a two-dimensional data space. First, we demonstrate that the chosen kernel can be decomposed into a scalar product, which we did by expanding it and then rewriting it as shown on the bottom half of this slide. Having decomposed it, we can now see what the underlying transformation function is. Remember, we don't need to know this in practice. We are just looking at it here in the context of this simple example to see how kernels work. The transformation projects the data into a three-dimensional space of the squares and cross products of the original data axis, at least for this example. We now apply the transformation to the dataset below on the left. The two classes of data lie on the other side of the quadrant of a circle and clearly are not linearly separable. After transformation, that data is linearly separable into two-dimensional space. Even though a third dimension has been created in the transform of the previous slide, it is not needed. This slide summarizes the essential points about the transform and the kernel. Again, these questions simply reinforce the important properties of the kernel. 

### Module 2 Lecture 9: The support vector machine—multiple classes and the classification step

Now come to the fourth step in the support vector machine. That is how to turn a binary classifier into a multi-class process. The classical way of turning a binary classifier into a multi-class machine is to embed it in a decision tree. The simplest of which is that shown in this slide. Here, each class is separated from the remainder sequentially. It is immediately obvious though, that a problem with this approach is that the training sets used to effect each binary decision are very unbalanced, particularly near the top of the tree. Furthermore, there's probably an optimal order in which to do the class separations but we don't know that beforehand. A preferable approach is to use the one against all strategy in which a set of binary classifiers are trained in parallel. Each classifier separates one class from the rest. So there are as many classifiers as there are classes. Having been trained then when used to classify unknown pixels, the decision rule applied at the output of the tree selects the most favored label based on the SVM decision rule. Another approach is the one against one strategy. The topology is the same as in the one against all approach of the previous slide but here each classifier is trained to separate just two of the set of classes, all class pairs are implemented so that m into m minus 1 divided by 2 separate classifiers are needed. We get that number by looking at the number of times two classes can be selected from the available m. The calculation is m factorial divided by m minus 2 factorial. Note that each class will appear m minus 1 times among the set of classifiers. An unknown pixel is placed into the class which has the greatest number of recommendations in favor of it among the m times m minus 1 over 2 decisions. Because of the large number of individual classifiers involved, training can be quite time consuming. We have now covered the four stages in the development of the support vector classifier, as shown in this slide. We now wish to see how the machine can be used in practice, which we do by way of example, in the next lecture. There are a number of decisions that must be taken by the user before applying the support vector machine to real remote sensing classification problems. Effectively, we have to choose some multi-class strategy to be used, and the kernel. We then have to implement some process to find the best value for the kernel parameter and for the regularization parameter C. In the next slide, we put these steps into the overall classification methodology we outlined in the case that the maximum likelihood at minimum distance classifiers. In this slide, we examine the support vector machine from an operational strategic perspective. Recall, we did that in lecture seven for the maximum likelihood and minimum distance classifiers. The table here is identical to that earlier one but has some steps particularized to the support vector machine. Again, the three steps which are common to all classifier methods are highlighted in blue. Please note that software is freely available for implementing a support vector machine. Some of the most common are shown here. This slide simply summarizes the four important steps in the SVM. These quiz questions focus you on the practical aspects of using a support vector machine. 

### Module 2 Lecture 10: The support vector machine—an example

In this lecture, we present two examples of the application of the support vector classifier in remote sensing. We start with a simple example using a segment of QuickBird imagery recorded in 2002. The image segment consists of 500 by 600 pixels with four bands of data as shown in this slide. Several cover types are evident in the image. Two of the information classes each consist of two spectral classes. The support vector machine was trained on the identified spectral classes. As required, the authors chose a set of training pixels, in this case, as fields of pixels. And a separate set of pixels, again, as fields to be used for testing the accuracy of the result of the classification. Here we see the numbers of training and testing pixels for each spectral class used by the authors. The next step is to design the classifier, including the choices of kernel, and multiclass strategy, and the values of the kernel, and regularization parameters. The one against one multiclass strategy was used, which required 36 different binary SVM's. A grid search procedure is commonly used to find optimal values for the kernel and regularization parameters. In this example, a slightly simpler approach was employed, which involved two linear searches as described in the slide. The same parameter values were used in all 36 classifiers. This slide shows the results as a thematic map, and as a table of class by class accuracies. Overall, the average accuracy was 76.9%. Although the water class was handled perfectly, the performance on the rock and bare soil classes was quite poor, and would not be acceptable if those classes were of interest in practice. If we compare the thematic map to the image, we can see immediately the classification errors in the rock and bare soil classes. Some of the rock pixels have been labeled as tree. Whereas many of the bare soil class pixels have been labeled as asphalt. This is a very crude accuracy assessment. We will be much more precise when talking about classifier errors, when we come to Module 3. We now look at the results of a classification of hyperspectral imagery. A problem for which the support vector machine is said to be more suitable than most other classifiers is taken from a 2004 paper and involves a 220 band dataset called Indian Pines, recorded by the AVIRIS Sensor. This slide shows the image itself and a ground truth map. That is a map of pixels which have been labeled by site visits, use of photo interpretation, and other sources of reference data. Only 9 of the 16 classes were used in this exercise. The one against all multiclass strategy was used, as was the radial basis function kernel. By running a number of trial classifications, optimal values of the regularization, and group parameters were determined, the number of training and testing pixels are also shown in this slide. Although we don't have a thematic map in this case, we do have a table of accuracies, showing remarkably good results on all classes. That the interesting sensitivity analysis, which, at least for this example, indicates that the performance is not strongly dependent on getting the values of the kernel and regularization parameters exactly right. Here we summarize some of the practical aspects of using the support vector machine. Here we ask you to look at the support vector machine compared with other classification methods. 

### Module 2 Lecture 11: The neural network as a classifier

We now come to the 4th of the classification techniques, we're going to consider in this course. We will develop it in two stages. In its original form, and its return in a more powerful inflexible form, over the past five to 10 years. The neural network, sometimes called the artificial neural network. Was popular in remote sensing in the 1990s. But be cause it was complex to design, and required significant computational power. It was overtaken by other techniques, such as a support vector machine. However, with some simple modifications that lead to improvements in performance and training. It is gained popularity again, over the past decade. Now called convolutional neural networks. These later variants also go under the name of deep learning. All that that description could just as easily have applied to the neural network in its original form. As with the support vector machine, we start by a return to the simple linear classifier. Again, we will do our development with a simple 2 dimensional, 2 class situation. But it will generalize to any number of classes, and dimensions. And again, we use our standard decision rule for linear decisions. Which will now represent diagrammatically, in the form of what is called a threshold logic unit or TLU. In this diagram, the elements up to the thresholding block, create the linear function used in the decision rule. The thresholding operation, then chicks with the value of the function is positive or negative. And thus whether the Pixel Victor is in class 1 or class 2, as required by the algebraic expression of the decision rule. Sometimes we represent the overall operation, or the single block called the tail u. As noted on the previous slide, this was one of the building blocks used in early machine learning theory. That led to a classification machine called the perceptron. It is also where we start the development of the neural network. A breakthrough came when the hard limiting operation in the TLU, also replaced by softer function. And in particular, one that could be differentiated. As we will see, that allows a training procedure to be derived. Which is not otherwise possible. We call the Soft limiting operation an activation function. Typical examples, include the inverse exponential operation, or sigmoid. And the hyperbolic tangent as shown in this slide. As seeing the behavior still represents what we want, in terms of specifying the class to which a pixel belongs. Because it implements our decision rule, but without a hard limit. The old TLU but with a soft limiting operation, is now called a processing element or PE. In the nomeclature of neural networks, we also replace the offset W subscript N plus 1, by the symbol theta. But in the bottom right hand drawing, that we will write the output of the processing element as g equals a function of z. Where the function f, is the chosen form of the activation function. And z is the linear function in our normal decision rule. The classical neural network, and that which was applied wildly in remote sensing. Is called the multilayer perceptron, or MLP. And is composed of layers of processing elements, which are fully connected with each other as seen in this slide. The blocks in the first layer, are not actually preciously elements. They just distribute the input pixel vector elements, to each of the processing elements in the next layer. The outputs of those processing elements, then form the inputs to another layer of PEs. And so on, for as many layers as chosen by the user. The outputs from the last layer of pairs, determine the class of the pixel vector fed into the first layer. Now, the user can choose how that is done. Options are, that each class could be represented by a single output. Or the set of outputs could represent, for example, a binary code that specifies the class. Note the nomeclature used with the layers of a neural network. In particular, the first layer which does any real analysis, is the first hidden layer. Note also the litter designations we apply to the layers. And while we have shown only one, there can be many hidden layers. Each being fed from the outputs of the previous layer. As the number of hidden layers increases, training the neural network becomes increasingly time consuming. In many remote sensing exercises of several decades ago, only one hidden layer was used. And found sufficient to handle many situations. But training still took along time, as we will see later. Having chosen autopology, we now need to work out how to find its unknown parameters. That is, the weights W, and the offsets theta for each processing element. As with all supervised classifiers, that is done through training, unlabeled reference data. But to make that possible, we need to understand the network equations. So the training process can be derived. That is now our immediate objective. The equations describing each processing element, of those we noted earlier that is G. Is a function of WTX plus theta. And remember, WTX plus theta is the linear function. However, we need a naming convention. To keep track of where the inputs come from, and where the outputs go. We do that by adding subscripts to the white, and offsets as shown here. And nomeclature is simplified in that is layer specific, but not PE specific. We could add a third subscript to indicate each actual PE in a layer. But that turns out to be unnecessary. We can derive the relevant network equations for training, without going to that added complexity. To start developing their training procedure, we need a measure of what we're trying to achieve. Clearly, if the network is functioning well as a classifier. We want the output to be accurate, when the network is presented with the previously unseen pixel vector. We check the networks performance by setting up an error measure. And measure that looks at how closely the actual output matches what we expect. When a training pixel is fed into the network. We choose for that measure, the squared difference error measure. Shown in the center of the slide. Remember, the set of actual outputs of the network, energy subscript k. This measure, tells us how well those actual outputs match the desired or target outputs. t subscript k for a given training pixel vectors. In the next lecture, we will use that expression to help set up the necessary equations. With which we can train the neural network. Clearly, our objective is to find the unknowns, weights and offsets. That minimize the error measure. In other words, that might be the actual class labels match as nearly as possible. The correct or target labels. In this part of the course, we are examining the use of the popular multilayer perceptrons. As a classifier in remote sensing. It consists of layers of processing elements, where each PE contains a differentiable activation function. We are now at the stage where we can set up the network equations. The second question here is important. Because it starts to develop a feel for the complexity of training a neural network. 

### Module 2 Lecture 12: Training the neural network

We now look at how we can train a neural network. While this analysis is important in its own right, it turns out that the same process can be used when we come to look at the more modern convolutional neural networks later. We now wish to devise a training pressures for the neural network by seeking to minimize the error function we set up in the last lecture. We do that by making small adjustments to the set of weights such that those adjustments lead to a reduction in the error. The approach commonly taken is to modify a weight by subtracting a small amount of the first derivative of the error from its initial value as shown in this slide. The idea is that by doing so, we will move down the error curve as indicated. This is called the gradient descent approach. There are other adjustment procedures too, but the simple gradient descent method is good for illustration. The amount of adjustment is controlled by the parameter ETA, which is called the learning right. A two larger value of ETA leads to greater adjustments, but may lead to instability. That is, oscillations between both sides of the error curve in the illustration. On the other hand, a two smaller value of ETA means training time might be lengthened. The adjustment here is shown for the weights that the j and k layers. To workout a value for the adjustment to the weight, we need to perform the differentiation shown here. That is done by the chain rule as seen in the center of the slide. We now have to get values for each of the derivatives in that chain rule expression. Here we show each of those three derivatives and the final result when they are combined in the chain rule. Choosing b equals one in the activation function, we end up with the correction increment shown on the bottom of the slide. Let's call that equation A for lighter convenience. We now move to the front of the network and look to find the correction increments for the weights which think the i and j layers. We use the same gradient descent procedure as before to do that. But here we have a small problem. Since E is not directly a function of g subscript j, we cannot compute the derivative, dE, dgj simply. Instead, we need to use another chain rule expression as shown on the bottom of the slide. To get a value for dE, dzk in this expression, we again use the chain rule which would be equals one. We have an expression for dE, dzk in terms of the actual and target outputs. That leads to the expression for the correction increment for the i to j weights as soon. Which is not only a function of the actual target outputs, that is also dependent on the k to j linear weights. From the previous step, we know those said that we now have a suitable and usable expression for delta wji correction increments. With the two sets of analyses, we can now formulate a training algorithm for the neural network. We can simplify our two previous equations if we define some new variables delta k and delta j, which allow the correction increments for the two sets of linkages to be written simply as shown at the bottom of the slide. Wall out equations are specifically focused on adjustments to the weights, the thresholds delta j and delta k in the network equations can be evaluated using the same expressions just by making the corresponding inputs unity during training. We now formulate the training strategy. The chosen network is initiated with an arbitrary set of weights that allows outputs, although in error, to be generated by the presentation of training pixel vectors at the input layer. For each training pixel, the network output is computed from the set of network equations and initially, of course, that output will be an error. Correction to the weights is in performed using the equations of the previous slide. The value of delta k is computed first since it depends on the network outputs g subscript k compared with the target outputs t sub k. Then the result can be propagated back through the network, layer by layer, if there is more than one hidden layer using the other equations on the previous slide to generate corrections to the network weights. Specifically delta wkj, which is equal to ETA delta kgj can then be found. Following which, we get the delta j and then the delta wji. When all the weights have been adjusted, the output of the network is computed again using those new weights. Hopefully, the gk will now be closer to the target values tk. New values for the delta k will then be generated and the process of weight adjustment is repeated. This process is iterated as often as needed to reduce the difference between the actual and target outputs, tk minus gk to zero, or to a value acceptably close to zero. If it is zero, then delta k will be zero meaning that no further adjustments to the weights will occur with further iteration. The network is in fully trained. In the terminology of neural networks, an iteration is also called an epoch. Because training involves working back from the outputs at each epoch or iteration, the training process is referred to as back propagation. The interesting thing about training the multilayer perceptron is that when a training pixel is presented at the input, the calculations are propagated forward through the network to generate the output. That output is checked against the correct class for that training pixel, and if found to be an error, the equations were derived in this lecture are used to propagate backwards through the network, the adjustments to those weights. The first question here draws your attention to the possibility of local minima in the error curve. 

### Module 2 Lecture 13: Neural network examples

We will now look at some examples to illustrate the training and performance of the neural network. When considering the application of the neural network, there are a number of decisions that need to be taken beforehand about the network topologies. These include how many hidden layers to use, how many nodes should be used in each layer, how to use the output layer to represent the thematic classes, and what value to assign to the learning parameter. Generally, the first layer will have as many nodes as there are elements through the pixel vector. Often there will be as many output layer nodes as there are classes unless some form of c [inaudible] to reduce their number. Generally, the number of nodes in the hidden layer should not be less than the number of output layer nodes. Note that the multi-layer perceptron has all the connections in place that we described earlier, that is the output of a processing element or a node in any layer, is connected to every node in the next layer; that is called fully-connected. Later, in the context of the convolutional neural network, we will not use all of those connections. By way of illustrations, we start with a very simple example involving two classes in a two-dimensional vector space. Note that the classes are not linearly separable, as seen in the figure. We have chosen a network with two nodes in the hidden layer and two in the output layer. The network equations are shown explicitly on the right hand network diagram. Note also that we have chosen a zero threshold theta for the hidden layer processing elements, and we have also chosen b equals one in the activation function and eta equals two as the learning perimeter. The network was initialized with a set of weights shown in the first row of this table. As seen, the error before iteration was 0.461. The network was then trained for 250 iterations at which the error had been reduced to 0.005. At the same time, the whites can be seen to be converging to fixed, or final values. We stop training at 250 iterations and use the parameter values at that point. On the right hand axis, we have plotted the arguments of the two hidden layer PEs before the application of the activation functions. Effectively, when equated to zero, they've implemented linear separating surfaces. The activation function then places a response for a given pixel on either side of those surfaces. Each surface therefore segments the data space into two regions. In the output lab PE, [inaudible] responses segment the full space into the two class regions shown. Effectively, it implements a logical OR operation that is shown mathematically in the bottom table, which shows explicitly how the output layer functions for each of the four possibilities of patterns being placed on either side of the first two surfaces. Having trained the network, we need to see how successful it is in separating sets of pixel vectors that it has not previously seen. In the table here, there are eight new pixels. They can also be seen in the vector space. Patterns A to D are in class one, while patterns E to H are in class two as is evident on the diagram. The table shows the intervening calculations and the final classification by the network for each pixel. All testing pixels have been successfully labeled. We now come to a real remote sensing example taken from a 1995 paper that is listed on this slide. The data-set consisted of the six non-thermal bands of a 900 by 900 pixel segment of the thematic mapper scene recorded over Tucson, Arizona on 1 April 1987. There are 12 classes evident in the same. They were chosen by the authors. The band for the infrared image shown here does not make those classes easily seen, but the grid structure of Tucson streets is evident. In keeping with mastery [inaudible] exercises of time involving neural networks, the authors chose a network was just one hidden layer. Since it was six bands, the input layer consisted of six nodes or processing elements. Those nodes also scaled the data to the range between zero and one. Since there were 12 classes, the output layer was chosen to have 12 nodes with each representing a single class. The scale of the outputs was chosen such that during training, an output of 0.9 on a node indicated a target class while a value of 0.1 means that the class does not respond to the training pixel being presented. The hidden layer was chosen to have 18 nodes. Since the authors decided to compare the neural network results against those obtained with a maximum likelihood classifier, the choice of the hidden layer nodes was based on having the same number of parameters to determine as for the maximum likelihood rule. This slide shows the information classes and the numbers of training and testing pixels used by the authors. Although the network was allowed to run for 50,000 iterations or epochs, the error has stabilized after about 15,000 iterations. Note that more than 96 percent of the training pixels are properly handled once the network has reached that number of iterations. It is because so many iterations are needed to train a neural network in practice, that training time can be so excessive. The network performance using unseen testing pixels was a very good 93.4 percent accuracy. If training was stopped after 10,000 iterations, the network was still capable of achieving 92 percent accuracy. If stopped at 20,0000 iterations, that improved marginally to 93 percent. A maximum likelihood classifier was also run on the same data set, although there is no indication as to whether it was optimized for the choice of sets of spectral classes to represent the specified information classes, which we will do in other examples in module 3. Nevertheless, the maximum likelihood classifier achieved 89.5 percent accuracy on the testing data, but it was 10 times faster to train. This slide shows the thematic map produced by the neural network on the right-hand side, along with the key to the colors. The authors included two variations to the standard neural network training process to improve the learning rate. The first wants to add a momentum term to the gradient descent rule used to adjust the weights. On the top of this slide, we summarize the standard gradient descent adjustment. On the bottom in green, an additional term is added. It is chosen as a proportion of the previous weight adjustment, which forces a modification to follow the pattern of the previous iteration. Another perimeter is introduced in this process, Alpha, which controls the degree of momentum used. The second modification was to adjust the learning and momentum rate adaptively in order to improve convergence. That was done every fourth iteration according to the rule shown on the top of the slide. Note that the convergence and the ultimate result of neural network training can be affected by the initial choice of words and that the initial set cannot all be the same. Otherwise, of course, the network will not train. More details on this example will be found in the paper. However, this original neural network approach is now rarely used. We introduced it here as preparation for the more recent development of the convolutional neural network, which we commence in the next lecture. When we come to the convolutional neural network, we will often talk about deep-learning. Simply put, network depth is described by the number of hidden layers. A deeper network has more. The idea is that when there are more hidden layers, the network should be more powerful. The network is in more difficult or time consuming to train because of the vastly larger number of unknowns that had to be found. When we come to the convolutional neural network, we will find that increased network depth is possible because we don't use all the connections between the nodes. By reducing the number of connections substantially, we can have more layers and still train the network. As a final comment on the operation of the layers in the neural network, this slide gives a different perspective on how the simple network of the first simpler example operates. Earlier, we regarded the hidden layer processing elements as implementing two decisions, with the third layer acting on those decisions as a logical O function. We could also view the hidden layer operation as in this slide if we examine the data as it appears at the output of the first layer processing elements. Now, represented by the variables J1 and J2, the data has been transformed into a linearly separable sit, which the output layer now handles. Again, this is a simple summary of what we've learned so far about neural networks. The second and third questions here will become important when we look at the convolutional neural network in the next series of lectures. What properties does it have to have in order that the back-propagation training algorithm can be made to work. 

### Module 2 Lecture 14: Deep learning and the convolutional neural network, part 1

Now start a series of four lectures on the transition of the neural network that we met in the past few lectures into the convolutional neural network that has become a cornerstone of Artificial Intelligence Research over the last few years. It has also been widely applied to remote-sensing problems, as we will see when we look at some examples. At the completion of this work, we will then do a detailed comparison of the major classification techniques we have covered in the course. As we will see by comparison to the classifiers, we have looked at so far the convolutional neural network does not have a standard form or topology instead it is composed of a number of building blocks that can be configured in many ways according to the approach chosen by a particular user. We will develop the tools and show several configurations helpful in remote sensing, especially for taking special neighborhoods or pixels into account when performing semantic mapping. The book by Goodfellow, and the others referenced here has become a bit of a standard treatment for Deep Learning and convolutional neural networks. It's true is at a higher mathematical level then we have adopted in these lectures but nevertheless, for those with the right background it should be consulted to fill in the gaps in the theory. Note, especially it's warning listed here about non standardization in convolutional neural networks. Before we embark upon the development of the convolutional neural network, it is important to reflect on the fact that for many decades, image analysts in remote sensing have been critically aware of the matter of a spatial context. That is, when considering the liable for a central pixel in the diagram on the slide we know for many scenes that there is a high likelihood that the surrounding pixels will be from the same class. That is especially the case for agricultural regions and many natural landscapes, and yet the classifiers we have been dealing with up to now have ignored that property. In that sense, they are simply called point or pixel specific classifiers because they focus just on a pixel independently of its neighbors. Over the use though, there have been many suggestions about how to treat spatial context, some of the more successful approaches are shown here. Echo was perhaps the earliest, developed in the mid 19 seventies, it works by growing homogeneous regions in an image. It then classifies that as objects or regions as a whole, it applies point classification methods for pixels that are not found to be part of an object, such as those on the boundaries between regions. In the late 1970s, the method of relaxation labeling was developed, it takes the results of a point classification at expressed as posterior probabilities of class membership such as in the output of a maximum likelihood classifier. Those posteriors are updated iteratively by reference to the posteriors on the neighbors, linked by a set of joint probabilities. Finally, measures of texture can be used to characterize the neighborhood about a pixel. Local texture is then used as another feature in a point classification method along with the spectral measurements of a pixel. As we will see soon, the convolutional neural network is another technique that embeds spatial context in its decisions. To employ the neural network spatial context sensitive applications, we have to use it in a slightly different way than we have up to now. Let's commence this discretion by recalling the topology we had been dealing with so far, in which the inputs are the individual components of the pixel vector. Suppose now though, we make the seemingly bold move of inputting all the pixels of an image in one go so that we have enough input nodes to accommodate the full set of spectral measurements for a full set of image pixels. For practical image, that will be a very large number of inputs, we still have a number of hidden layers and for the moment, the network is still fully connected, thus there will be a huge number of unknown white vectors and offsets to be learned through training. One immediately obvious problem with feeding the network in this manner is that the spatial interrelationships among the pixels appears to be lost. Even though this is really just a problem of how the pixels are addressed, it is more meaningful to arrange them as shown in the next slide. Suppose we present the image to the network as a square or rectangular array with the pixels in their correct special relationships, this doesn't change anything about the network other than arranging the nodes or processing elements into an array rather than a column format. For convenience, we have shown the hidden layers to be the same size and shapes as a input layer, but in general they could be any size. Note that the output layer is still one-dimensional since it represents a set of classes. With such an arrangement, the number of potential connections is enormous. Let's do a calculation of the number of unknowns between just the input and the first hidden layer. Remember that the input to each processing element in the hidden layer is z equals w transpose x plus Theta. The dimensionality of the weight vector will be equal to the number of elements in the input layer, which is N times N for an N-dimensional image. Also, there are as many weight vectors as there are nodes in the hidden layer. If we assume, for the sake of this calculation, that the hidden layer has the same dimensions as the input layer, that means altogether, we have to have N to the fourth different weights, values for which have to be found during training to make the network usable. In a similar fashion, there will be N squared values of Theta. If we had N equals 100, which would be a very small image in remote sensing, then there are more than 100 million unknowns, that would require an extraordinarily large amount of training data. Added to this, is the fact that we have multiple bands and images usually much larger than 100 by 100. Clearly, a simpler approach needs to be found, but one in which spatial inter-relationships among the pixels are still represented effectively. In this slide, we just simplify the diagram by removing the explicit input layer and just let it be represented by the image itself, perhaps with scaling, if that is found to be beneficial in some applications. In this simplified representation, each image pixel is connected to all the nodes of the first hidden layer. Also, we are still focusing on just a single band of data. We will come to multispectral images later. Here, we show the major deviation of the convolutional neural network from the fully connected neural network we have been considering so far. Instead of implementing all connections, that is as in a fully connected network, we are selective in the connections we make between the layers. In particular, we restrict the connections to a node in the hidden layer to be just those of a neighborhood of non-pixels from the input image as shown. Because of the geometry, the grip of three by three pixels is centered on the one which is in the second row and second column. The processing element in the hidden layer is also that in the two, two position, as seen in the slide. In contrast to the need to determine N to the fourth plus N squared weights and offsets overall, there are now 10 unknowns; that is nine weights and one offset. So 10 unknowns to determine per hidden layer node. Overall, therefore there are, in principle, 10 N squared unknowns defined, a considerable reduction but still a large number if N is large. We do the same thing for the 3 by 3 group, which is one column to the right. Now we take a decision that significantly reduces again, the number of unknowns to be found in training. Rather than using new set of weights and offsets, we assume we can employ the same set as for the previous slide. This is called weight reuse. While that sounds like it will reduce substantially the pair of the network to learn complicated spatial patterns in the image, it gives surprisingly good results in practice. There is also a rationale to this decision, which we will say soon. Continuing though, we then do the same thing for the next pixel group along the row, and then for all rows until the whole image is covered. While this example suggests that the actions happen sequentially, in fact all the operations are in parallel. They are just sets of connections. This is important to recognize. As we have realized, there is a problem with the edge pixels. Given the large numbers of pixels in an image, we could ignore the edge problem. But sometimes, an artificial border of zeros is created so that the edge processing elements in the hidden layer can receive inputs and thus preserve dimensionality if that is important. Even though many of the connections of a fully connected neural network have now been removed, it turns out we can still use backpropagation, surprisingly, to train this new sparser network. So thankfully, we do not need to develop a new training procedure. Let's summarize where we are at this stage. In looking at the neural network, the convolutional neural network, we are partly driven by the desire to take spatial context into account when labeling a pixel. Again, in the convolutional neural network, all the image is fed to the network in one go, but the numbers of node-to-node connections is greatly reduced and thus, so the number of unknown parameters to be found during training. The first question here asks you to think about the importance of spatial context. The last two questions are particularly important when thinking about the use of convolutional neural networks 

### Module 2 Lecture 15: Deep learning and the convolutional neural network, part 2

In this lecture, we take the development of the convolutional neural network further, still focusing just on a single band of data, but considering the evolution of its topology. The concept we adopted in the last lecture for the connections between layers is similar to the common process of convolution used to filter an image to detect spatial features. We haven't covered that material in this course, but it is moderately straightforward. In spatial convolution, a window called a kernel, is moved over an image row by row and column by column. A new brightness value is created for the pixel under the center of the kernel by taking the products of the pixel brightness values and the kernel entries, and then summing the result. That is exactly the same operation implemented by processing element in the hidden layer of the convolutional neural network just before the offset is added and the activation function is applied. It is because of that similarity that the partially connected neural network just described is called a convolutional neural network. However, in the convolutional neural network, the kernel is usually called a filter, and the set of input pixels covered bother filter is called a local receptive field. Note that any size filter and receptive field can be used. Even though we are exploring the smaller number of connections as a way of simplifying the network, and thus the number of unknowns that needs to be found during training, it is of interest to think a bit further about the practical significance of choosing a spatial neighborhood kernel of weights for that purpose. While important neural analysis of spatial context, this has particular relevance to picture processing and object recognition fields in which the convolutional neural network have been used extensively over the past five years or so. In spatial filtering, say for detecting the edges in an image, the kernel, or filter, entries are selected by the analyst for that purpose as seen in this very simple example. A three by three filter can be used to find the edges in an image. In the convolutional neural network, the kernel entries, that is the weights prior to the application of the activation function, are initially chosen randomly. However, by training, they take on values that match the image features that are characterized by the spatial nature of the training samples. If the training images strongly feature edges, it is expected that the weights will tend towards those of an edge detecting filter, for example. The strength of a convolutional neural network is that, with sufficient numbers of layers, it can learn the spatial characteristics of an image. That is why it is an important tool for performing context classification and for picture processing in general. We now introduce some more operations used in convolutional neural networks along with their associated nomenclature. The first is the concept of stride. When we looked at fading just nine outputs from one layer into a single processing element of the next layer, we did so with single pixel shifts along rows and down columns. Some authors choose to have larger shifts. The result of which is that the number of nodes in the next layer is reduced. The number of pixels shifts is what defines stride. This slide shows a stride of two. Another topological element often used is to add so-called pooling layers as seen on the right-hand side of this slide. This strengthens the dependence on neighborhood spatial information, and reduces further the number of parameters to be found through training, particularly when more than a single convolutional or hidden layer is used. Pooling is sometimes called down-sampling. We now have a decision as to how to proceed further and ultimately construct an output for the convolutional neural network. There are four common options. First, we can keep going by feeding the output of the pooling layer into another convolutional layer to provide a deeper network. We can, in principle, have as many layers as we wish. Just like with the fully connected network, we can have as many hidden layers as we like. Secondly, we could feed the output of the pooling layer into a set of output layer processing elements and thus, terminate the network. Thirdly, we've got to have the output of the pooling layer act as the inputs to normal, fully-connected neural network. In this case, the convolutional neural network acts as a feature selector for the fully-connected network. This is a common approach especially in remote sensing. Finally, we could have the output of the convolutional neural network generate a set of class probabilities. In the next slide, we are going to examine the last two options. Here, we show a network with two convolutional layers and one pooling layer feeding into a much smaller, fully connected neural network. At the top, we considered a couple of lectures ago. In effect, the convolutional neural network is acting as a feature selector for the fully-connected network. Note that we have introduced another term, flattening. That is just the process of straightening out the matrix into a vector as needed for the neural network input. Note also here, the last convolutional layer is not followed by a pooling layer. The output of a convolutional neural network can come from either layer type. After flattening, rather than feeding the results into a fully connected network, another very common option is to use a convolutional neural network outputs to generate a set of pseudo probabilities called softmax probabilities. They are defined in the slide. The convolutional neural network outputs are exponentiated and normalized as shown so that the set of softmax values replicate a set of posterior probabilities. Finally, the sigmoid activation function is usually replaced by a simpler activation function called the ReLU, the Rectified Linear Unit, which has the characteristic shown here. This choice speeds up training by improving the efficiency of the gradient descent operation used in back propagation. Note that the use of stride and pooling successively reduces the number of unknowns to be found by training. Also note that convolutional and pooling layers can be cascaded. The second question here leads to one of the design equations used with convolutional neural networks. 

### Module 2 Lecture 16: Deep learning and the convolutional neural network, part 3

In this lecture, we confront the problem of multidimensional images, color pictures made up of the three color primaries, and multispectral and hyperspectral images in remote sensing. In this slide, we see the three color primaries of a color picture. Alternatively, they could be three bands of a multispectral image. We describe the image pixels as shown by the three equations on the top of the slide. On the bottom, we share the corresponding three filter entries. In both cases, we have used three indices. The first refers to the individual band, while the others are the pixel position index. The simplest way to treat the three band image, is to carry out three separate convolutions as shown by the equations on the top of the slide. Generally, only a single offset, theta, is used. The three convolution calculations are added, to which the offset is also added, and then the activation function is applied. We now have three times the number of whites to learn by training. While this is the approach most often adopted for color pictures, we will see later, how multispectral and hyperspectral images are treated. Here we show another variation, often used in the convolutional neural network. Several convolutions can be performed in parallel in order to extract more spatial information from an image. As noted, the filters can be of the same or different sizes. Because of the complexity introduced by the various options we have discussed, it is difficult to come up with a standard form of diagram with which to represent the convolutional neural network. Most authors use their own forms of diagram. But the representation shown here is common to many and simple to understand. He we show convolutions in parallel, as just discussed on the previous slide. We also show several layers, each of which is composed of a convolution operation followed by pulling. Of course, the pulling operations are not essential, but are included here for completeness. Finally, we show the flattering operation often used at the output. As indicated, some authors even have crossed connections between the parallel paths. But that can defeat one of the benefits of the convolutional neural network by having several separate parallel paths. The network can be programmed to run on a multiple process and machine. We now come to an important practical consideration, similar to that we met with the maximum likelihood classifier, when considering the Hughes phenomenon. And that is the problem of over-fitting, which is illustrated on this slide. The concern arises because we have so many weights and offsets to be found through training. And the availability of training data determines how effectively those unknowns can be found. We must have sufficient training samples available to get reliable estimates of the unknown parameters. Otherwise the network will not generalize well. In other words, it will not perform well on previously unseen pixels. It is not sufficient to have a minimum of samples to estimate the unknowns, otherwise over-fitting will occur. This is illustrated in the example from curve fitting shown in the diagrams on the slide. Fitting a high order curve through just three points, will guarantee good fits for those points. But the behavior between the points can be way out in terms of being able to represent intervening points not used in generating the curve. If many training samples are used, then the function found interpolates or generalizes well, as indicated on the right-hand diagram. Clearly, we need many more training pixels than the minimum, to ensure we do not struck the same problem when training the neural network. Consider now the numerical complexity of analyzing hyperspectral image data. So we can make use of both spectral and spatial properties. Several approaches have been used in practice, as we will see shortly in some examples. One is to analyze the spectral information content alone. Another is to analyze the spatial information content alone, that is spatial context. Another is to do both together, but there is a processing challenge. We could treat the problem of processing hyperspectral data with a convolutional neural network by allocating one convolutional filter to each band, as we did previously for the three band color picture. But that requires about 200 times as many weights as for a single band image. For an image with 200 bands, and 3x3 kernels, the total number of unknowns, that is weights plus offsets, connecting the input image to the first convolutional layer is 2,000. Noting that the same weights are used in each filter right across a particular band. This, of course, gets multiplied upwards by the number of filters used in the convolutional layer. Often we take the path of reducing the spectral dimensionality of the hyperspectral image before applying the convolutional neural network. Although that partly defeats the purpose of using hyperspectral imagery in the first place, transforms such as the principle components transform, do allow us to concentrate the variance or information content in a small number of components. Three as shown here, but more might be necessary if we wish to retain, say, at least 95% of the image variance. If we want to analyze hyperspectral data for spectral properties alone, we can use the convolutional neural network defined the label for each pixel, based just upon its spectrum, and thus implicitly the correlations between bands. This, of course, ignores any benefit of spatial context. Here we summarize how multiband images can be handled right through to dats as complex as hyperspectral imagery. Importantly, the need to avoid over-fitting must be kept in mind at all times. The first question here asks you to propose a simple formula based on the discussion in this lecture on using principal components analysis. 

### Module 2 Lecture 17: CNN examples in remote sensing

We now present two examples that illustrate much of what we have discussed in these lectures and which help us to introduce some additional concepts. These examples illustrate how convolutional neural networks have been used to handle hyperspectral data based on Spatial properties alone and a combination or spectral and spatial properties. Our first example is taken from the paper indicated. It presents examples of hyperspectral classification using several data sets based on just the spectral properties of a pixel. Here we look at a classification of the Indian pines data set, which we saw before with the support vector classifier example. The data was recorded by the average hyperspectral sensor over a region in Indiana, USA. It consists of 220 spectral channels in the range of 0.4 to 2.45 micrometers. In treating this image, the authors chose to remove some difficult classes. They retained the class as shown in the table, which also indicates the numbers of training and testing pixels used. The authors chose to use a single layer convolutional neural network as a feature selector product classification by a fully connected neural network. They applied 20 Spectral Filters in parallel and used a fully connected network with a hidden layer of 100 nodes. Note how large their filters are. Altogether, there are 81,408 unknowns to be found from the training data. From the previous slide, there were 1600 trillion pixels at 220 channels per band. That gives 352 thousand training samples, which is sufficient. This slide shows the results in the form of a thematic map and also shows the accuracy achieved compared with that generated with a support vector classifier. The accompanying ground truth map, that is the map of correct labels, allows one to assess how good the final thematic map is. Of note though, is the speckled appearance of some classes indicating that the convolutional neural network misclassified and number of pixels. Hadn't incorporated spatial filtering to, we would expect to see a much cleaner thematic map. The second example we will consider uses a two channel convolutional neural network to account for both spectral and spatial properties of hyperspectral scenes. One channel is diverted to spectral properties alone and functions very much as in the previous example. The other channel handles the spatial analysis. Both channels develop feature subsets that are then concatenated and analyzed by a fully connected neural network. The example is taken from the paper indicated in the slide. We are going to concentrate on this Salinas, California image exercise. The image segment consists of 512 by 279 pixels with 3.7 meter spatial resolution. It has 224 recorded bands, but the authors reduced those to 200 by removing channels with poor quality. The ground truth image shows that there are 16 classes with the numbers of pixels indicated. The authors chose to train the network using different percentages of ground truth pixels. We show the results here for the training data being 25 percent of the total labeled ground truth pixels. They used all the available ground truth pixels to test the generalization of the network, that is the classification performance. Here we see the convolutional neural network topology or architecture used by the authors. It consists of a spectral path at the top and the spatial path or channel at the bottom. Notice that the spatial path has 30 filters of size three by three and the spectral Path has 20 filters of size 20 by one. Each pathway has one convolutional layer and one pooling layer. The input to the spatial path consists of a spatial neighborhood about the pixel currently under consideration during training or in classification. The outputs from the two paths are flattened, concatenated and they'd fade into a fully connected neural network with two hidden layers, each with 400 nodes thus the two path convolutional neural network is acting as a feature selector for the fully connected neural network. Note that the upper layer has 16 nodes representing the 16 classes in the Salinas image. The outputs are in the form of class conditional probabilities computed with the soft max function. There are two important aspects of this example which needs to be emphasized. The spatial layer is required to capture the neighborhood or spatial properties of the pixel. The neighborhood patch of 21 by 21 pixels centered on the pixel of interest is used. The neighborhood patches created by averaging over all the spectral channels in that neighborhood. The authors also used transfer learning. That is a technique based on the concept that networks previously trained on different images but with the same sensor will most likely perform acceptably on the image of interest. This is based on the assumption that the spatial properties are similar from image to image. The authors trained the convolutional neural network layers on a different average image and then use the weights so found to initialize the convolutional neural network weights for trading on the Salinas, saying. This is not necessary in general but is a common approach based on the concept that we, as humans adapt our learning from past experience. The results shown here indicate the benefit of both spectral and spatial context, achieving an overall accuracy of 98.3 percent which is indeed very good. It is important to note that the authors runs extensive trials to find the best topology for the network. That is the numbers of convolution layers and numbers of filters, the numbers of nodes in the hidden layers and so on. Which indicates that the preparatory stages in using a convolutional neural network can be quite extensive. The idea of using neighborhood patches seems first to have been introduced by a contests in the paper referenced in this slide. Those authors used far by far patches that maintain the full spectral dimension for the patches so that spectral information, as well as spatial neighborhood of a pixel was carried by the patch. However, in order to constrain the overall data volume of the input, the authors carried out a dimensionality reduction first using a principal components transformation. This slide gives information on where convolutional neural network software can be found. Two important points are emphasized in the summary. First, patches or neighborhoods can be fed into a convolutional neural network to carry spatial context into classification. Secondly, transfer learning can be an effective and efficient way to initialize our convolutional neural network and even a fully connected neural network. These questions ask you to think carefully about some of the quantitative aspects of using a convolutional neural network. 

### Module 2 Lecture 18: Comparing the classsifiers

Having now completed our examination of the most popular classifiers used in remote sensing. It is now benefit to compare them both in terms of performance and in relation to the user if it required. We want to compare the attributes, so we know where their relative strengths and weaknesses lie. And so that we always choose the most appropriate method for the task at hand. Of the range of algorithms we have looked at, the following three are representative set for comparison purpose and other ones we have spent most time on. They are the maximum likelihood classifier, the support vector machine and convolutional neural networks. Let's commence by summarizing the maximum likelihood algorithm. Recall that the decision rule for allocating a pixel to a class is expressed in terms of discriminate functions. Each class is defined by its mean vector and covariance matrix, and if available, the class prior probability. And is represented also by those properties in the discriminant function. Here we summarized the attributes of the maximum likelihood classifier. The support vector classifier, finds a separating hyperplane that maximizes the margin between two classes of data. While minimizing the error caused by pixels that fall on the wrong side of the hyperplane. It uses kernels in place of dot products effectively to project the data into a higher order space, so that data which is not linearly separable can be handled. The attributes of the Support Vector Machine is summarized here. That in particularly that it has to be used in a decision tree to make it capable of handling multiple classes. The Convolutional Neural Network is a modern variant of the original multilayer perceptrons. And consists of a number of layers, each usually involving sets of filters that perform convolution, activation and pooling. Those layers can then be followed by fully connected neural network and or a softmax operation. And this slide summarizes the attributes of the convolutional neural network. In this table, we bring the most important attributes together so that the three principle algorithms can be compared. In summary, the maximum likelihood classifier is much simpler to construct in trying. But is limited when presented with data of high spectral dimensionality. By comparison, the support vector machine and the convolutional neural network are more challenging to configure and train. But the support vector machine is good for handling data of high dimensionality. It does however require a decision tree framework to handle more than two classes. The convolutional neural network naturally handles special context and, like the maximum likelihood classifier, is a multiclass algorithm. It can also handle data of high dimensionality. Here we summarize what the user needs to look for in selecting an algorithm for thematic mapping in remote sensing. Again, these test questions draw attention to the types of application the analyst may have to handle leading to a choice of the most appropriate classifier algorithm. 

### Module 2 Lecture 19: Unsupervised classification and clustering

We now turn to the topic of Unsupervised Classification, in which we are still interested in thematic mapping. But without the benefit of having labeled training data available beforehand, we won't develop the topic in this series of lectures based on the procedure called clustering. Indeed, most of what we will talk about concerns clustering algorithms. But we will present some unsupervised clustering examples later in the lectures. We will make unsupervised clustering or unsupervised classification, again, in Module 3 of the course. We start by being confronted with the situation in which now obvious training data is available. Yet, we still want to do thematic mapping of remote sensing image data. Cluster Analysis forms the backbone of what we are going to do. Clustering looks for groups of similar pixels assessed on the basis of this spectral properties. The groups we're searching for are in fact clusters in spectral space. We can often identify the pixel labels produced by clustering through the use of spatial clues in the image and by using the cluster means as surrogates for spectral reflectance information. We will see that in the examples to follow. Reference data like maps and air photos also give us hints as to what the class is in a cluster map might represent. Here we look at unsupervised classification as a two-stage process in which clustering takes us from the spectral domain to a map of symbolic labels. The challenge for the analyst is to turn those symbols into meaningful ground cover labels. We get the cluster map in the same way we get a thematic map in supervised classification. Here the clustering algorithms place pixels into clusters based on spectral similarity. We then assign symbols to the clusters and use those symbols in place of the pixel itself in the image that's producing a cluster map. By examining those pixels and their spatial layout, the analyst turns the symbols into ground cover class labels. Clustering algorithms place pixels into clusters based on their similarity, as we said before. As we indicated in the previous slide, the most common measure of similarity is based on the spectral measurements of the pixels. Two pixels with very similar measurement vectors are likely to belong to the same class and thus the same cluster. The simplest way of assessing similarity is to use a metric which measures the spectral distance between pixels. The most common of which is the Euclidean distance between the pixels in spectral space. For in-band data, Euclidean distance matrix is d as a function of X1 and X2, which is given as the norm of the difference between the two vectors X1 and X2 and which in fact, in the last expression is the square root of the sum of the squares that we are most familiar with. There are other distance metrics in use, including the more efficient but perhaps less accurate city-block distance, defined as D, X1 and X2 being the sum just of the absolute differences between each of the elements of the pixel vectors. It is similar to walking between two locations in the city where the streets are laid out on a rectangular grid. It is sometimes called the Manhattan distance for that reason. But in these lectures, we will concentrate on Euclidean distance. The set of clusters for a given image is not unique, even though we have a metric for spectral similarity. Here we see two plausible clusterings of eight pixel vectors in a two-dimensional spectrum space. Which one is correct? To help assess which set of clusters best represents the pixels in an image, we need a quality of clustering criterion. A common one at which is the sum of squared error measure or SSE, with the formula shown here. The SSE checks the distances of all the pixels in a given cluster from the cluster mean and then sums those distances within that cluster. It does so for all clusters and then sums the results. In other words, it is an accumulative measure of distances of the pixel vectors from the cluster means. The smaller it is, the better. Since then the clusters are compact. Other quality of clustering measures are possible. Some look at the average compactness of clusters compared with the average distances among them. In principle, we should be able to develop a clustering algorithm by minimizing the SSE for a given data set. But that turns out to be impractical since it would require examining an enormous number of candidate clusterings of the available data to find that which has the smallest SSE. In practice, some heuristic methods have been developed that work well, two of which we will look at here. The first is called the k-means or migrating means algorithm. It is perhaps the most commonly used, particularly for remote sensing problems. It asks the user first to specify beforehand how many clusters to search for, and secondly, to specify a set of initial cluster mean vectors. Clusters are located in spectral space by their mean vectors. The algorithm starts by the user guessing the set of cluster centers, initially. The image pixels are then assigned to the cluster of the closest mean, after which the set of means is re-computed. The pixels are then assigned to the nearest of the new set of means and so on until the means and the assignments do not change. This slide shows algorithmic-ally the steps in the k-means approach. In the first two steps, clustering is initiated by specifying a starting set of cluster mean vectors, both in number and position. In Step 3, all the available pixel vectors are then assigned to the cluster of the nearest mean. The mean vectors are then re-computed in Step 4. Then a new assignment of pixel vectors is carried out based on the re-computed means. The means are then re-computed again. During this process, pixels will often move between clusters, iteration by iteration, because of the change in positions of the means. Ultimately, we expect that the stage will be reached where the pixels do not migrate any further between the clusters, say that the situation is stable and we conclude that the correct set of clusters has been identified. That is checked by seeing whether the full set of mean vectors no longer changes between iterations. In practice, we may not be able to wait until full convergence has been achieved and instead, we stopped clustering when a given high percentage of the pixel vectors no longer shifts between cluster centers with further iteration. Before we implement the K-Means technique in the next lecture, we hear text stock of what we are trying to achieve. Our ultimate goal in many remote sensing situations is unsupervised classification, which we pursued through the application of clustering techniques. Even though we haven't yet seen an example of the application of the k-means approach, we can still think about some practical matters concerning its operation and especially how we choose the initial cluster centers. That choice will affect the speed of convergence of the algorithm and the actual cluster set found. 

### Module 2 Lecture 20: Examples of k means clustering

In this lecture, we look at two examples of K-means clustering. The first uses a small dataset to show how the algorithm operates. The second is a remote sensing example to share what's operation in unsupervised classification. We now implement the K-means algorithm using the two-dimensional dataset shown here. We will see this dataset a couple of times in these lectures. Looking at the pixel locations in this data, it seems there are two or possibly three clusters. In practice, that might not be so obvious, so a guideline is needed in terms of the initial choice of how many clusters to find. Because we can merge clusters later, it is good to estimate on the high side, recognizing however, that the more clusters there are, the longer the algorithm is likely to take to converge. A guideline which has been used in remote sensing, is to estimate the number of information or ground cover classes in a scene and then search for about 2-3 times that number of clusters. The choice of the initial clusterizations is important, because it can influence speed of convergence and the ultimate set of clusters. It is important that the initial set be spread well across the data. Several guidelines are available. One is that the initial cluster centers be spaced uniformly along the multi-dimensional diagonal of the spectral space. That is a line from the origin to the point of maximum brightness on each axis. Better still, we could choose the multi-dimensional diagonal that joins the actual spectral extremities of the data, that requires a bit of [inaudible] processing to find the lower and upper spectral limits in each band, but that is quite straightforward. Another approach is to space the initial cluster centers uniformly along the first principal component of the data. That will work well with a highly correlated data sets, but might be less useful if the datasets show little correlation. Diagrammatically, this slide shows the evolution of the K-means method, iteration by iteration on a small data set. Here we're searching for two clusters. The bottom row shows four iterative assignments of the pixels to the clusters along with the corresponding SSA values. After the fourth iteration or assignment, there is no further migration of the pixels between the clusters, that incidentally those pixels which changed clusters in the first two steps. The top right-hand diagram shows how the means migrate with iteration. That tells us why the algorithm is sometimes called the method of migrating means. The ISODATA algorithm is a variation of the simple k-means approach. It adds two or three further possible steps as outlined in this slide. First, we can check to see whether any clusters contains so few pixels as to be meaningless. If the statistics of the clusters are important, say for use in a later maximum likelihood classification, then poor estimates will be obtained if the clusters do not contain a sufficient number of members. Secondly, we could see whether any pairs of clusters are so close that they should be merged. In Module 3, we will look at similarity measures for use in classification. They will give an indication of whether classes and clusters are too similar spectrally as to be useful and therefore should be merged. Thirdly, we can check whether some clusters are so elongated in some spectral dimensions that it would be sensible to split them. Elongated clusters are not necessarily a problem, but if they are, then comparison of the standard deviations of the clusters along each spectral dimension will help reveal the elongated nature. We now look at the application of clustering to unsupervised classification using a five channel dataset of an image recorded by the HyMap sensor near the city of Perth in Western Australia in January 2010. The table shows where the five bands are located in the spectrum. From our knowledge of spectral reflectance characteristics, that choice seem sensible in being able to differentiate among the cover tops we most expect to see. The image display uses just three of the bands selected to give the standard color infrared product in which vegetation is emphasized in rate. A remote sensing image analysis package called multi-spectral from Purdue University, was used for this exercise. Six clusters were specified with no provisions to include taken close or elongated clusters. However, if any cluster was found to have fewer than 125 pixels, it would have been eliminated. None were found in this exercise. Once clustering was complete, the unsupervised cluster map shown on the right-hand side of this slide was produced. The clusters represented by different colors by the visual patterns of the classes in the image. It is easy to associate the brown and orange clusters with highways, road pavements, and bare regions, the yellows with buildings, and the shades of green with various types of vegetation. Clearly the dark blue cluster is water. We strengthen this interpretation further in the next slide. In the table here, we see the mean vectors of the final set of clusters, which with the spatial clues in the map itself, allow us to associate the cluster colors with ground cover classes as indicated in the K to the cluster map, which of course is now a thematic map. Here we plot the cluster means by wavelength and see that they follow the spectral reflectance curves of the ground cover class labels assigned to the clusters. This is further information that has been used to identify the clusters. Insofar as it is possible, it is always good to look at unsorted representations such as this to help understand the identity of the clusters that had been found by the algorithm. It is instructive to see where the cluster centers lie in spectral space. While we can't envisage the third-dimensional space, we can look at two-dimensional scatter plots using two of the band's most significant to vegetation, they need infrared and the visible red bands as seen here. The final cluster centers represent well the scatter of the pixel vectors. Here we summarize the essential elements of the ISODATA algorithm and how clustering is used for unsupervised classification. The first question, here should allow you to develop a feeling for the importance of the placement of the initial cluster centers. 

### Module 2 Lecture 21: Other clustering methods

In this lecture, we will summarize other methods for clustering and illustrate one of them, so that we can compare its performance with that of the k-means algorithm. Although the k-means algorithm is one of the most widely used methods for clustering, there are other approaches that have been used with remote sensing data. In the next lecture, we will explore a recent clustering method that has been applied to hop spectral data and to big data sets. But here we will look at another long-standing technique so that we can see its performance relative to the k-means algorithm. By doing so, we will demonstrate that the results of clustering are not unique, affect that the use and needs to be aware of and handle carefully when undertaking unsupervised classification. As noted in the slide, other methods that have been used in remote sensing. First, hierarchical clustering, which when applied to the first example of the last lecture, tends to lead to three and not two clusters. It has been used as a basis for clustering in some big data applications and for details of this algorithm, please see my book. Secondly, histogram picks selection, math and climbing or density maximum selection is another technique used for clustering in remote sensing. Thirdly, we have the single pass clustering algorithm, which is the method we are now going to examine. The single pass method is an old technique and had its origins when remote sensing imagery or supplied on sequentially accessible storage media like magnetic type, which with iteration would be a particularly time consuming process because of the need to read and re-watch the type. Despite its formation, it is still sometimes used because of its simplicity and its speed. It starts by assuming that the data is arranged in the usual row and column format. If the image is very large, a random sample is taken with the pixels and the results arranged again by row and column. The algorithm proceeds in the following manner. The first row is used to obtain an initial set of cluster centers in the following way. The first sample is used as the center of the first cluster. If the second sample is further away from the first by more than a user specified critical distance, then it is used to start a second cluster. Otherwise, the two samples are assumed to be from the same cluster, in which case they emerged and their mean computed. This process is applied to all the samples or pixels in the first row. At the end of the first row, the multispectral standard deviations of the clusters we generated, are produced for use in the light of rows. Each sample in the second and subsequent rows is checked to see if it lies within a user specified number of standard deviations of one of the clusters from the first row. If it does, it is added to that cluster and the cluster statistics are recalculated. Otherwise it is used to start a new cluster and allocated a normal standard deviation in each band. In this slide, we show the single-pass method diagrammatically. The left-hand diagram shows how the first four samples are traded in this particular illustration. Only samples 2 and 3 are close enough to be merged. Clearly sample 2, was too far away from sample 1 and was used to start a new cluster. Also, sample 4, being too far away from the two existing clusters is used to start another cluster. The right-hand diagram shows how sample n plus 1 falls within the prescribed number of standard deviations of cluster 2 and becomes part of that cluster. Whereas sample n was too far away and is used to initiate another separate cluster. The single-pass method is fast and does not require the number of clusters to be pre-specified. It does, however, require the user to specify two parameters, the critical distance used with the first line of samples and the standard deviation multiply used in the remaining lines. Also, since it initiates clustering on the first line of samples, it can be biased by the samples in that line. There is no way to moderate that choice. Variations on a single pass algorithm exist, some let the users specify the actual initial cluster centers, while others use a critical distance measure for all rows. The multi-state package where they're going to use operates that way. We're now going to apply the single pass algorithm to the dataset we traded in the last lecture with the k-means method. The MultiSpec package was again used for this. It doesn't use the standard deviation method for the second and subsequent lines but applies another critical distance. The critical distance is used here, were 2,500 and 2,800 respectively for the first and subsequent lines of data. These numbers seem large, but remember, this sensor has 16 bit radiometric resolution. As noted in the previous slide, the algorithm uses a first line of pixels or samples if the image is large to initiate the cluster centers. In this case, the first line is actually the right-hand column of pixels in the displayed image, say in the next slide. Because after clustering the image and custom apps were rotated 90 degrees clockwise to bring them into a North South orientation. Here we see the results of the application of the single passed method to the image we analyzed earlier. As with the k-means algorithm, we can say that the clusters represented by different colors follow the visual patents of the classes in the image. The colors here are different from before, and this time they were just confusion between roughage and trays. In this slide, we say the cluster center is created by the single pass algorithm. How do they compare with the clusters generated by the k-means method? Well, let's say we compare the results of the two algorithms using bi-spectral plots as shown here. That the sparse vegetation, water, and building classes are about the same for both algorithms. Whereas the two approaches have picked up different combinations of bare surfaces, roads, and trays. In practice, clustering may need to be refined by re-running the algorithm with different sets of parameters until a cluster set is obtained that matches the information classes of interests. Here we summarize the essential elements of the single pass algorithm and the fact that unique results are unlikely to occur. The third question here is particularly important. Often in remote sensing, we have the notion that the pixels tend to clump into groups that align well with groundcover classes. That is often not the case. Instead, the spectral domain can look like a continuum with a few density maxima associated with definite classes like water. We will have more to say about that in Module 3. 

### Module 2 Lecture 22: Clustering "big data"

In this lecture, we look at how to do clustering with very big data sets, including hyperspectral images. We are now in the era of big data, particularly with the recording and storage of many high-volume image datasets. In 2014, NASA was managing more than nine petabytes of data, with about 6.4 terabytes a day being added. This accounts for an unbelievably large number of images. How do clustering techniques cope with such large amounts of data? If we want to apply clustering techniques to large images for unsupervised classification, or use clustering to extract information from archived datasets, a process called data mining, then the methods for clustering we developed in the last couple of lectures are limited in value. In this lecture, we will look at a recent clustering approach that is suitable for big datasets. There are others as well, but the one we look at here illustrates the types of method now being explored for use on so-called big data. Just before doing that, consider the time demand of the k-means algorithm. For P pixels, C clusters and I iterations, the k-means algorithm requires PCI distance calculations. For N bands, the distance calculations involve N multiplications each, giving a total of PCIN multiplication to complete a k-means clustering exercise. For 1000 by 1000 pixel image segment, involving 200 bands and searching for 15 clusters, then if 100 iterations were required, 30 by 10 to the tenth multiplications are needed. How can we devise an approach to clustering that is much faster, and is able to cope effectively with large images? But in searching for an improved technique, the k-means technique should not be abandoned. Its simplicity means it is still used with big datasets. But to make it more suitable, there are several alternatives that should be examined. First, the simplest is to use a more powerful computer. But from an operational point of view, it is important to note that most remote sensing practitioners would want to use readily available and not specialized computer hardware. Secondly, a better method for initiating the cluster centers might be found that helps speed up convergence by reducing the number of iterations needed. Several methods have been suggested for that in the big data context. Thirdly, a multiprocessor or multicore machine could be used to speed up the computation by taking advantage of parallel calculations. However, steps need to be taken to parallelize the k-means algorithm, which because of its iterative nature, requires some innovative modifications. Finally, a more efficient version of the k-means algorithm might be possible. We will examine one such technique here. It speeds up significantly the time required to undertake clustering and to allocate a pixel to a cluster class. In a sense, it is a particular case of the third dot point. The method we will look at for fast clustering is called k-trees. We met decision trees in the context of the support vector machine. But now, we want to look at them more generally. We start with some nomenclature. Trees consists of nodes, linked by branches. The uppermost node is called the root, and the lower most nodes are called leaf nodes. In between, there are internal nodes. The nodes are arranged in layers as shown. Progression of a pixel down the tree is based on decisions at the nodes. Those decisions direct the pixel into one of the available branches. In the k-trees algorithm, we allocate leaf nodes to the clusters that we are trying to find. Both in number and position in the spectral space. Although we don't know how many there will be beforehand. Some authors use the leaf nodes to represent the individual pixels within the clusters. With the clusters themselves being the internal nodes in the layer directly above. That does not help in developing the algorithm and just adds an additional unnecessary complication. The k-trees algorithm has one parameter that the user has to specify beforehand. It is called the tree order. Which specifies the maximum number of pixels in a cluster and as we will see in the following slides, the maximum population of any node in the tree. Full details of the K-trees algorithm will be found in this paper by Geva. It is a little hard to understand in the remote sensing contexts since it is written in the language of computer science. We will develop the algorithm for example, using a simple two-dimensional set of data, and using remote sensing terminology. We will use the set of eight vector samples shown here in vector and diagram form, and choose a tree order of three. Specification of the order controls the structure of the tree, as we will see. The tree starts with a single root node and a single leaf node. We then feed in the first sample, say sample C. Feeding in is also called insertion. Since we have no other information, the sample simply flows down to the leaf node, as does the second sample A, shown on the right-hand side of the slide. We will use black letters to indicate samples of current interest and red letters to indicate samples which have already been fed into the tree. A third sample, say G, can be accommodated, but it fills the leaf node since we have specified a tree order of three. A fourth sample, say D, cannot be accommodated in the current tree because the leaf node cannot contain more than three samples by design. That leaf node has to be split. The K-trees algorithm does the split by doing a K-means clustering of the four samples as on the next slide. The K-means clustering in the K-trees approach always looks for two classes, so that the over full leaf node is split into two new leaf nodes. In this example, the vectors A, B, C, and G have to be allocated to two clusters. We show that process in the slide. Because of the initial choice of cluster centers, the algorithm converges in one iteration for this very simple example. When complete, the two clusters have the main vectors indicated on the slide. The main vectors from the clustering step of the previous slide become the attributes or members of the root node, and the internal leaf is split into the two clusters, as shown in the left-hand diagram. The tree now has capacity to absorb more pixels, so our 5th sample F, can be inserted as shown on the right-hand side of the slide. That pixel is checked against the two main vectors in the root node, and it seemed to be closest to M-C-D, so it is allocated to the left-hand leaf node. As in the previous slide, when pixel B is inserted, it is checked against the main vectors held in the root node and found to be closer to M-A-G, as a result of which it is allocated to the right-hand leaf node. The main A-B-G is calculated and used in place of M-A-G in the root node. But when we try to insert the 7th sample H, the capacity of the left-hand leaf node is exceeded and that node has to be split again, using the K-means algorithm. We are looking to separate the pixels C, D, F, and H into two clusters using the K-means approach. Rather than go through the exercise, we assume for simplicity that the solution shown in the diagram here has been found. The means of the new clusters are shown. This leads to the tree now having three leaf nodes, as seen on the next slide. The root node now contains three main vectors and has reached its capacity. If any further leaf nodes are added, then the root node will have to be split. Consider the insertion of the final pattern E to the tree. When entering the root node, it was seen to be closest to the M-A-B-G mean, so it should be placed in the bottom right-hand leaf as indicated. But since that exceeds that leaf's capacity, the leaf has to be broken into two separate leaves using K-means clustering as on the next slide. For simplicity, we assume that the K-means algorithm has found the clusters illustrated here in the diagram to the left with the main vectors indicated. The tree now has four leaf nodes as shown on the next slide. But the root node now needs to be split into two because its capacity has been exceeded. The two new nodes resulting from the split will be internal nodes, and a new root node is created. We split the root node using the k means approach, that the elements now to be clustered are the main vectors stored in the root node. Again, assume the results of the k means clustering are shown here. The new means mcdfh and mabeg are now computed. Since all pixels have now been fit into the tree, we have its final version as seen here. It has three layers with two internal nodes and four leaf nodes. Any pixel vector fit into the top of the trail will make its way down to one of the clusters via the decisions. That is distance comparisons at the root and internal nodes. For example, the vector 33 will fly through as shown by the dotted green line into the cluster, say H. Apart from [inaudible] there cluster. There are two things we want to know about clustering algorithms. First, how long does it take to build the tree? Secondly, especially with unsupervised classification mind, how quick is it at allocating unseen data to a cluster? If we look at the speed of allocation first, we can do so by counting the number of distance comparisons. In the simple case here, that the k trees and the equivalent k means approach require the same number of comparisons. But what about with bigger data sets? If we take the simplest case of H node in a k tree requiring two distance comparisons, the number of comparisons increases by two for each newly added. Which in this case also doubles the number of clusters. By contrast, the number of distance comparisons for the k means the algorithm goes up as pairs of two say for, large numbers of clusters. K trees algorithm is much faster when allocating an unseen sample to an existing cluster. Getting a meaningful comparison of the times to build the k tree, and the k means approach is not straight forward. We can make comments on the numbers of nodes to be built and the checks within them. But the complexity introduced by the effect of different tree orders makes meaningful theoretical comparisons difficult. We will use an example instead, as in the next slide. This example is taken from the paper cited here. Also a by given. The data to be clustered, consisted of vector samples from a three dimensional normal distribution with zero mean and unity variance. A range of sample sizes was used, starting at 1,000 and progressing to 256,000 by successive doublings. The k tree order was chosen as m equals 50. For comparison, a k means clustering was run on the same set of samples. It was initiated with the same number of clusters as found by the k trees approached on the same data set. Ten sets of a tests were done, with the results averaged. Here we see the comparison, which is quite compelling for a given number of samples to be clustered. The k trees approach is much faster than the k means algorithm as seen. As always, there is a trade off. Geva found that the k trees clustering was not quite as accurate as the k means approach. Given that k-means is an iterative procedure which involves all pixels at all stages while k trees is a single pass per sample and segments that are dispatched during learning by its branching structure, that is not surprising. But in general, Geva found the difference not to be a problem in most practical situations, especially given the spade benefit of k trees. Another significant factor in favor of the k trees approach is that it can be adapted to run a multi-core processes and not to require all samples to be holding core memory during clustering. Those additional developments are new and contained in the paper by Woodley and others sought at in this slide. We are now at the end of our lectures on clustering. It would be good to compare this summary with those of the previous three lectures in order to reinforce overall the important aspects of clustering, especially when used as a tool for unsupervised classification. The first two questions here are important to the development of classification methodologies, which we will pursue in module three. The last question, hearts a particular benefit of clustering with the k trees approach. 

## Module 3

### Welcome to Module 3

We now come to the third and final part of our course. Our last module is in two parts. The first covers some practical matters we need to be aware of when analyzing remote sensing images with classification techniques. One relates to how we cope with the number of recorded bands, especially when that number is large. We also look at the very important topic of how to assess errors in the thematic maps generated by a classification exercise. We then look at methodologies for the application of classification methods to generate thematic maps, noting particularly the benefits of mixing techniques such as unsupervised and supervised learning. The second part of the module is a coverage of image in radar as a remote sensing tool. Radar is one of the key imaging technologies along with visible and infrared imaging that is most commonly encountered. That material introduces some of the innovative aspects of radar, such as its ability to produce topographic maps, monitor changes in topography with time, and the prospect of resolving detail within vertical features such as forests. The module and indeed the whole course concludes with a brief look at what is possible in landscape analysis if different imaging modalities such as radar, optical, and thermal imaging are used together. So at the end of this module, you should understand the need for feature reduction, feature selection as one feature reduction tool. How we go about hyperspectral analysis by library searching the very important topic of classifier performance and map accuracy. How we sample format accuracy assessment, classification methodologies, the fundamentals of imaging radar, then radar interferometry and radar tomography, finalizing with the concept of information fusion. 

### Module 3 Lecture 1: Feature reduction

We now come to two practical aspects of machine learning in remote sensing. The first looks at the features we use in performing a classification, while the second addresses the very important topic of how to assess the accuracy of a thematic map. As part of this work, we will look at methodologies for use in the various machine learning techniques we developed in module two. But we start now with looking at the topic called feature reduction. Feature reduction techniques allow us to limit the number of features to be used in a classification. Most often, those features will be the spectral recordings or bands from the remote sensing instrument. But sometimes, there will be special features, particularly if the image data has high spatial resolution. We will not look at methods for reducing the number of special features, noting that convolutional neural networks do that well. Instead, here, we will develop techniques for reducing the number of spectral features. Why would we want to limit the number of features or bands? The answer is simple, it is because the number of training samples required per class to train a machine learning algorithm is lower for fewer features. With more features, more samples per class are required for accurate training. That is a major consideration for hyperspectral image data sets. Feature reduction is carried out to avoid the so called curse of dimensionality, or the Hughes phenomenon, when we train a classifier. There is a second consideration, with more features, training and classification times increase, in some cases more than linearly, so that classification cost also increases. Reducing features therefore reduces costs, provided the cost of the feature reduction step does not outweigh the savings gained in classification. This raises an intriguing question. If we have to go to all the trouble of reducing the feature subset in order to develop a reliable classifier, why not just record a small number of bands in the first place? To answer that question, we need to examine the rationale behind hyperspectral imaging. Hyperspectral imaging is often referred to as imaging spectroscopy. Indeed, that was its original name. As you probably know from your studies in science, spectroscopy means being able to identify something by reason of its spectral characteristics. Examples with which you might be familiar include mass spectroscopy, visible spectroscopy, and electron spectroscopy. Sometimes a spectrum results from the absorption of radiation, and sometimes it is a result of reflection or emission. Imaging spectroscopy is a development of the familiar field of visible absorption spectroscopy at the pixel level. For each pixel, we record the full reflectance spectrum of the substance using reflected sunlight, usually over the wavelength range of about 0.4 to 2.5 micrometers. We measure the sunlight reflected to the sensor on a remote sensing platform after a component of the sunlight has been absorbed by the surface material. The reason our field is called imaging spectroscopy is because such a complete reflectance spectrum is recorded for every pixel in an image. This slide summarizes the essential nature of imaging spectroscopy. On the left, we see an image which has been recorded in a very large number of wave bands, typically 200 or so. For a particular pixel, those bands represent samples of the reflectance spectrum of the corresponding region on the ground. Or more correctly, the reflectance spectrum of the surface material corresponding to the pixel. From those samples we can reconstruct the reflectance spectrum of the pixel as shown on the right hand side of the slide. Notice that the reconstructed spectrum, particularly if there are sufficient samples, shows various important diagnostic features such as the dip corresponding to chlorophyll absorption in the red region and the water absorption bands in the mid infrared. There are other finer absorption features too, especially for soils and minerals. They do not show up on this vegetation example. One of the great benefits of recording the full reflectance spectrum for a pixel is that we can use Scientific knowledge to identify the pixel rather than depend upon supervised learning and machine classification. Expert spectroscopists can undertake that analysis from their knowledge of the spectral response properties of materials. Well, that is a viable approach, most often in hyperspectral remote sensing. The recorded reflectance spectrum of the pixel is compared with the library or prerecorded spectra, as in the next slide. This slide illustrates the approach to image interpretation based on library searching. The image captured by the sensor consists of pixels for each of which are full reflectance spectrum has been recorded. That spectrum is in compared against the library of spectra previously recorded in the laboratory, allowing a label to be attached to the pixel. Often that can be facilitated by using expert system techniques in which the knowledge of an expert analyst is encoded in sets of rules that are applied to the data. Although spectroscopic analysis is regularly applied in the earth sciences using recorded hyperspectral image data in many applications, we still wish to use machine learning methods for labeling a pixel. This approach is convenient and does not rely on expert knowledge or recorded spectral libraries. But we then have to face the problem that there are too many bands recorded to allow all of them to be used as features in a supervised classification exercise. That is, be cause it is too difficult to obtain enough training data per class to allow our classifier parameters to be estimated reliably. We now therefore need to find methods that will allow us to identify a subset of the recorder bands that are still sufficient for the development of an accurate classifier. Before proceeding, there are some dimming clutcher that we should keep in mind. First features is the name given to the input measurements to our various classification algorithms. Most usually they're just the recorded bands or some transformed version of the bands after, say, the application of the principle components transform. In object detection, there may be special descriptors. In some applications, the feature set may include by spectral and spatial descriptors. We are now embarking on a search for methods to reduce the number of features to use, not unreasonably called feature reduction in general. One approach to feature reduction is to select subsets which still function effectively that is called feature selection. Feature selection is the most common form of feature reduction, but other approaches also exist, as we will see in the following material. Feature reduction has been a focus in the machine learning world ever since the first algorithms were developed. Many techniques have been proposed over the years in remote sensing, especially since hyperspectral image there they became available. In these lectures were going to look at some of the more common feature reduction techniques. For those of you who wish to take this study further, the paper referenced on the slide is a comprehensive overview of the field. Essentially, we are going to look at three different approaches. The first will depend upon an analysis of the correlations among the recorded bands of data. We will see that those correlations allow us to consider groups of bands and will let us ignore correlations from group to group. The second approach we will look at involves transforming the spectral data in such a way that we can ignore the least significant transformed bands. The third approach we will look at involves measures that got us in discarding some of the original bands of data without significantly compromising the quality of classification. These techniques fall into two groups. One assumes that the classes can be described by probability distributions, while the other does not require any such assumption. Let's take stock of what we have said so far before we embark upon the actual procedures for feature reduction. Remember hyperspectral data can have as many as 200 or so bands. The field of imaging spectroscopy depends on reconstructing their reflectance spectrum for a given pixel from the set of hyperspectral measurements. Often the reflectance spectrum is identified by reference to a library. A prototype spectra. It is important for that to be effective that the recorded spectrum is radiometrically calibrated. Most of our focus will be on analysis by machine learning methods. To be effective, they require the number of features to be reduced. In the second question here, pay particular attention to the correlations of each of the four bands, with all three of the other bands. 

### Module 3 Lecture 2: Exploiting the structure of the covariance matrix

In this lecture, we are going to examine the structure of the covariance matrix and thus the correlation matrix to see how their properties help us undertake feature reduction. We have met the covariance metrics several times in the past. It is the starting point for the principal components transformation and along with the main vector, defines the class signature when we undertake maximum likelihood classification. In the latter context, we know that the accurate computation of the class covariance matrix can be a problem for hyperspectral data if we do not have enough training samples per class. Remember, we have to have enough independent training samples in each class in order to estimate reliably the elements of the covariance matrix. Generally, that is not a problem with the principal components transformation because then the covariance matrix is computed using all of the available training samples and not just those for an individual class. We can examine the covariance matrix for the data as a whole. We will do that now and note that it has some interesting structural properties. Rather than the covariance matrix itself though, we will look at the correlation matrix instead. Which remember, is derived from the elements of the covariance matrix. Let's look at an example of a real correlation matrix. Here we will examine the metrics for the Jasper Ridge, USA image, which has 196 bands. The correlation matrix for this image will be of dimensions 196 by 196. This is far too large for us to write down. We can, however, represent it in an image form as shown to the right. Remember, correlations are in the range of minus one to plus one. A correlation of zero means just that. There is no correlation between the pair of bands. Whereas a correlation of plus one means total positive correlation. While minus one means total negative correlation. If we choose a gray scale where black represents zero correlation and white means total positive or negative correlation, we get a very interesting representation of the correlation matrix. Note that there are regions of high correlation distributed down the principal diagonal of the matrix. That means that the bands in those blocks are highly correlated. Where there are black box, the associated bands have little correlation among them. There are some groups of bands which leave the correlations off the diagonal, suggesting some interrelationships of bands in the middle infrared and between the middle infrared and visible regions. In general, though, we can assume that most of the correlation activity is down the principal diagonal. What does that now suggest for feature reduction? Here we neglect the off diagonal correlations and represent the correlation matrix just by the blocks that form down the diagonal. That matrix now becomes a block diagonal matrix which has a number of very interesting properties. If we write the block diagonal matrix in the form shown in the center of this slide, composed of H separate blocks down the diagonal, a number of simple and useful properties arise as we now see on the next slide. First, the determinant of a block diagonal matrix is the product of the determinants of the individual blocks. That means the logarithm of the determinant is the sum of the logarithms of the individual determinants. Secondly, the trace of the matrix is the sum of the traces of the individual blocks. Finally, the inverse of the correlation matrix can be computed using the inverses of the individual blocks as shown on the bottom of this slide. Now what happens to a column vector which has the same dimensions as the covariance or correlation matrices? This could be, for example, the main vector which corresponds to the covariance matrix. Clearly, it can be represented as the concatenation vertically of a set of smaller vectors corresponding to the blocks of the covariance matrix. That leads to a particularly important matrix identity, which says that the vector, matrix vector construct that we use regularly in image processing can be decomposed into the sum of the same expression computed over each of the blocks. We now have all the material needed to show how block diagonalizing the covariance matrix simplifies the principal components transformation and the maximum likelihood procedure. Just before we do that, we can note a further simplification. Rather than utilizing the actual blocks of high correlation down the diagonal, we could just restrict our attention to the correlations that exist among immediately adjacent bands. For example, as shown in this slide, we could just use the two by two or three by three blocks indicated. While probably not as accurate as retaining the true block diagonal structure of the matrix. Such a simplification reduces dramatically the number of bands that had to be considered when training a classifier, as we will see shortly. Irrespective of whether we use that maturation simplification or stay with the original block diagonal form of the matrix, we now look at how the block structuring of the covariance matrix affects the formula for the principal components transformation. Remember, to compute the eigenvalues of the covariance matrix, we have to solve the characteristic equation shown here in determinant form. When the covariance matrix is blocked diagonalized, the characteristic equation is a product of a set of equations of smaller dimensions. Thus the roots of the equation are the eigenvalues of the component block matrices. Likewise, we can find the eigenvectors and the principal components transformation matrix by computing everything on a block basis and then combining results. Now let's see how blocked diagonalizing the covariance matrix and equivalently, petitioning the main vector affects the maximum likelihood classifier using the rules above for block diagonal matrices. The discriminant function for the Gaussian maximum likelihood classifier can be shown to reduce to the expression in the middle of this slide. Although using the block diagonal approximation to the covariance matrix ignores correlations among bands that are widely separated, it does have significant benefits in terms of reducing the dimensionality that has to be considered when applying the maximum likelihood classification rule. The largest matrix for which the elements need to be estimated using training data is that of the largest block. In general, that will be significantly smaller than the original matrix, meaning that many fewer training pixels are needed for reliable estimates, rendering the maximum likelihood rule useful for hyperspectral thematic mapping. We now look at an example of the performance of the maximum likelihood classifier with block diagonalization taken from the paper listed here. Two AVIRIS images were used based on the five different tests listed at the bottom of the slide that the last test applies just the minimum distance rule. Full details of these tests will be seen in the paper that I have referenced. The results of the tests are shown here in graphical form, so they can be compared. The classification accuracy shown is the average performance on training and testing data. Note how good the results are, particularly when the actual block diagonal form is used. What these results don't show, but which is included in the original paper is that there is a significant improvement in classification time with the smaller blocks. That is because the classification time for the maximum likelihood rule is quadratically dependent on the number of bands used. There are four key messages from this lecture. Covariance and correlation matrices can be represented as images. Strong band-to-band correlations appear in blocks, largely down the principal diagonal of the covariance or correlation matrix. The block diagonal structure can be exploited to simplify image processing operations that require computation of a covariance matrix. The block diagonal form of the covariance matrix allows the maximum likelihood classifier to be applied to hyperspectral image data. The last two questions here focus your attention on understanding what the image representation of the correlation or covariance matrix tells us. 

### Module 3 Lecture 3: Feature reduction by transformation

In this lecture, we are going to look at what can be done when using data transformation as I feature reduction tool. This has been a common approach for many years, particularly using the principle components transform. Remember that the principal components transformation maps image data into a new, uncorrelated coordinate system or vector space, it produces a data representation in which the most variance is along the first axis. The next largest variance is along the second mutually orthogonal axis, and so on. The later principle components would be expected, in general, to show little variance. They could be assumed to contribute little value to separability and thus could be ignored, thereby reducing the essential dimensionality of the vector space, and leading to more effective classification. As noted earlier, this is a widely used approach. It requires a computation of the covariance matrix, but using the data set as a whole. So there is generally sufficient training data available to be able to estimate the global covariance matrics accurately. We will illustrate the approach, but working through an example. Consider the two class, two- dimensional example shown here. If you look at the distribution of the data by class, you can see even now that the two classes should be separable in an axis aligned at about 50 degrees to the original axis. In other words, the first principle component. Now, let's verify that. Using the standard formulas, the global mean vector and covariance matrix are as shown here. We then compute the eigenvalues of the covariance matrix and thus the corresponding eigenvectors. Note that the eigenvalues suggest that a large part of the data variance will be along the first principle component. From the eigenvectors, we get the principle components transformation matrix from which we generate the formula for the first principle component as a function of the original axis. That is shown plotted in the diagram along with the projections or components of the data onto that axis. As expected, the classes can't be separated in the first principle axis, meaning that the second axis can be ignored. Subsequent classification can therefore be based just on the first PC alone. Thus, in this simple example, the number of features has been reduced from two to one. There are some situations, however, where the principal components transformation will not work as a feature reduction tool, such as that illustrated here. Feature reduction using the principle components transform, only works if the classes are separable along a reduced set of principle axes, as in the example just considered. In the example here, we could not use principal components analysis for feature reduction because of the manner in which the classes are distributed. There is, however, a transformation that can be used in such a situation. It is called Canonical analysis. If we could find an axis, such as that shown in the bottom diagram then, again, we could represent the data with a single feature, rather than the two original bands. That axis is defined such that when the classes are projected onto it, they appear as far apart from each other as possible, while having the smallest spread. If, along that preferred axis, we define the '' among class variance"" and the average "within class variance" as shown in this slide. Then we want to find the axis rotation such that we maximize the ratio of those two variances. In order to find this expression and the actual transformation, we have to express the problem in matrix form, so that it applies to any number of features, we need also to express it in terms of the variances in the original features. In this slide, we define the key properties of importance when finding the so called Canonical axis. They include the global mean and the two variances expressed in matrix form as required. Now this looks a bit complicated, but it follows an approach almost identical to that of the principal components transformation. Let y equals day transpose x be the required transform that generates a new set of axis y. In which the classes have optimal separation. By the same procedure that was used for the principal components transformation, we can show that there within class and among class covariance matrices in the new y coordinate system can be expressed in terms of those in the original x coordinates as shown in the center of the slide. As with principle components, the row vectors of the transpose define the axis directions in y space. Let's small day transpose. Be one particular vector. Say that one that defines the first axis along which the classes will be optimally separated. Then the corresponding with in class and among class variances will be as shown on the bottom of the slide. We now set up the measure shown as equation A, which we have to maximize, that is, differentiate with respect to the axis direction D. We then solve the generalized eigenvalue equation shown subject to the normalizing constraint at the bottom. That solution generates the Canonical axis for us as we see in the analysis on the next slide. Here we see a two class data set which is not separable in the first principle component access. But will be soon to be separable in the first Canonical axis using the computations shown on this slide for the covariances. Substituting the covariants calculations into their characteristic equation at the top of the slide yields the eigenvalues of 2.5, four and zero. Having a zero eigenvalue tells us that there is only one Canonical axis in this example. Using the non zero eigenvalue in the characteristic equation at the top along with the normalizing condition leads us to the new axis shown in the equation at the bottom of the slide. That access is plotted on the original diagram in which it is saying that the two glasses are now separable using just one transformed feature. Feature reduction can be carried out by transforming the original spectral bands to a new coordinate system in which some transform bands can be discarded without affecting the separability of the classes. The principle components transform is the most popular method to use. Even though it is based on the whole data set, that is the global mean vector in global covariance matrix, provided the classes themselves are approximately distributed along the first few principal axis, it shows good results. If the classes do not distribute along the principal axis, then principle components analysis is not a good technique to use for feature reduction. It is better than to use a transformation associated with Canonical analysis. But principle components and Canonical analysis depend on covariance information. In the former, it is a global measure and thus easier to estimate accurately with limited training data. Whereas for Canonical analysis, class specific covariances are required and thus the method has limitations with high dimensional data such as hyperspectral imagery. These questions direct your attention to an alternative measure for computing class separability. 

### Module 3 Lecture 4: Separability measures

This lecture traits the third approach to feature selection, in which we set up measures to assess the importance to a classification exercise of the original sets of bands. This is the more classical approach. It entails setting up measures that help us to evaluate the relative importance of each of the bands, allowing us to discard some when we want to undertake thematic mapping. Generally, these approaches depend upon a measure known as separability. It can be based on an assumption that the classes of interests, can be described by probability distributions, or it can avoid the use of distributions. Obviously, the first approach is restricted to image datasets with small numbers of bands. Because of the need to estimate the statistics of the probability distributions, such as the Gaussian model that is almost always use. It is the old problem of not having enough samples per class, in order to estimate the distribution parameters reliably, provided the number of bands does not exceed about 10, those methods should be suitable. The second set of methods was developed to help us and detect feature reduction with high dimensionality datasets, such as hyperspectral imagery. That are generally more complicated to develop but are more widely applicable. Measures of separability tell us how distinct or spectrally different two thematic classes are, remember, classes are defined by sets of features. Our objective here is to see whether we can use fewer features and yet still carry out an acceptable classification. We use separability, to tell us where the class is based on choice and subsets of features, are distinct enough to allow good classification results to be achieved. Generally, we adopt a process such as this. First, we start with the full feature subset and a separability metric, compute how distinct the classes are. Next, we then remove a feature or sometimes sets of features and say by how much separability has been reduced. If separability is not badly affected, we could live at that feature. Otherwise we keep it and test other features. We then benefit from reduced classification costs and avoidance of this phenomenon. But the search is an exhaustive one, requiring all features to be assessed for a full evaluation. Divergence is one of the earliest and simplest measures of separability. It is based on quantifying the overlap of two class distribution functions, as illustrated in the diagram on this slide. Suppose we have two classes, omega1 and omega2, that are separable in the two features, x_1 and x_2. Being separable means they can be easily mapped as different classes by a classification algorithm, leading to high classification accuracy. If we discard feature x_2 and return only x_1 as shown on the bottom of the slide, we see that the class is then overlap and thus cannot easily be separated with high-accuracy by our classifier using just that feature alone rather than the two original features. Divergence tells us the penalty we pay by way of loss of separation, if we reduce the number of features by discarding one or more. Without going into the derivation, the formula here shows the computation of diversions for a pair of classes, are shown to be modeled by Gaussian or multidimensional normal distributions. As noted on the previous slide, we use divergence to see how the similarity of classes is affected, if we remove bands. What we're looking for is whether the separability of the two distributions is made never simply worse by the removal of features. If it is, then those features are important when running a classification and should not be ignored. On the other hand, if removing a feature doesn't drop the separability by very much, then it could be removed when doing a classification without significantly affecting accuracy. Divergence has a number of important properties. First, it is always positive. Secondly, it is zero if the distributions are identical. Thirdly, it never decreases with the addition of features. That means it is never high, if features are removed. Another measure of separability, is the Jeffries-Matusita distance, which is again a measure of the average distance between two distributions, i and j, for normally distributed classes. It has the form shown, involving an exponential function of a distance measure called the Bhattacharyya distance. The exponential term will be seen soon to be very important. While JM distance bears some similarity to divergence, it's significant difference is the exponential term. To see the effect of that, we sketch on the next slide, how divergence and JM distance would vary as a function of the distance between two distributions measured by the differences in the main vectors. By looking at the formulas for divergence, and JM distance, the behaviors shown on the two diagrams here can be seen. Divergence increases without bound as the distance between the classes increases. Whereas the JM distance curve saturates. Which behavior is the more useful. If we think about how the probability of correct classification might change as the two classes move further apart, it is more likely they behave like the JM curve, as we will see on the next slide. That is because the overlap between distributions drops quickly at first and then approach zero. Once the distributions are well separated, and thus classification accuracy is close to 100 percent, further separation will have little effect on the classification result. Here we see the probability of correct classification compared with the behaviors of divergence and JM distance. This suggests that the JM curve is more realistic all because of the saturating behavior of the exponent in its formula. However, because JM distance requires a computation of matrix inverses of the sums of all pairs of classes, it is time consuming if it is to be used to check the average similarity of many spectral or information classes. By comparison, divergence is much quicker to compute because the inverse of each class covariance matrix is computed only once, even when checking the average similarity of a set of classes. A third separability measure was developed heuristically to take advantage of the inherent benefits of both divergence and JM distance called transformed diversions. It embeds the usual divergence expression into a saturating exponential function, so that its overall behavior emulates that of the probability of correct classification. It is also faster to compute than the JM distance. Transformed divergence is perhaps the most commonly used measure of separability in remote sensing when classes can be described by the normal probability distribution. As we noted earlier, that generally applies when the number of bands or features does not exceed about ten. One of the important uses of separability measures is to be able to assess beforehand whether the spectral classes are well enough separated to ensure a certain level of classification accuracy. While the actual performance of a classifier depends on many factors, including how well the analyst has defined the spectral classes, collected reference data and trained the algorithm. If the classes are not well separated then there is an upper limit on the classifier accuracy that can't be achieved. The formula shown on this slide provides an upper limit on the probability of correct classification in our two class case as a function of transformed divergence, and this is plotted on the next slide. Here we see the theoretical upper bound of classification accuracy versus transformed divergence, along with some empirical classification results computed from 2,790 trials. As seen, unless transformed divergence is close to its maximum, binary classification accuracy falls well short of the theoretical upper limit. We met clustering algorithms in the last module. They are the basis of unsupervised classification and form a component of a combined unsupervised or supervised classification methodology that we will visit shortly. One of the last stages in clustering is to evaluate the size and relative locations of the clusters produced. If the clusters are too close to each other in spectral space, they should be merged. Well, what does too close mean? If cluster mean and covariance data is available, we can make merging decisions based on a pre-specified transformed divergence. By establishing a desired accuracy level for the subsequent classification, from which the corresponding value of transformed divergence can be specified, cluster pairs with separabilities below that value should be merged. That completes our treatment of measures of separability based on distribution functions. In the next lecture, we will relax that requirement and consider separability measures that can be used when it is not possible to estimate second order parameters, such as covariances, that is usually the case with hyperspectral data. For any classifier, it is good to use as few of features as possible in order to constrain the classification time and thus costs. Feature reduction is important in maximum likelihood classification in order to avoid the Hughes phenomenon, in which not enough training samples are available to develop reliable estimates of the elements of the covariance matrix. Divergence, Jeffries-Matusita Distance and Transformed Divergence are three methods commonly used when classes can be modeled by probability distributions. JM distance performs well but it is time-consuming to evaluate, whereas divergence is faster to use but doesn't work well when classes are widely separated in spectral space. Transformed Divergence combines the benefits of both approaches. Separability measures allow us to place an upper bound on classification accuracy. Remember to consult the solutions if you are having problems with any of the quiz questions at the end of each lecture. 

### Module 3 Lecture 5: Distribution-free separability measures

We now look at separability measures that can be used when it is not possible to represent classes by probability distributions. We will look at two different measures. The reason we need to develop separability measures that can be used with [inaudible] class models is so it can handle data of hyperspectral dimensionality. In such a case, there are insufficient training samples for the generation of reliable covariance matrices. Separability measures are still possible in these circumstances, but they are a bit more complicated to develop since we can't rely on measures that look at the difference between say, two normal distributions. ReliefF is one of the commonly used measures for feature selection, that is, for evaluating existing features, and seeing which of them could be discarded. This measure gives features a weight, which is adjusted by reference to the classes of data being used. Those features with weights below a user-specified threshold are discarded. We will develop the technique by reference to the two class, two-dimensional dataset shown on this slide. The classes have been drawn intentionally so that one feature x_1, does not aid separation while the other x_2, does. The process commences by selecting a pixel at random from one of the classes. We then find its nearest neighbors in the same and in the other class as shown. We now want to derive a measure that gives more weight to feature x_2, than feature x_1 with respect to those chosen pixels. Remember, we have two features, x_1 and x_2. We now define a weight for each feature that tells us how important it is with regard to class separation. Call these weights Omega_1 and Omega_2, respectively, but please don't confuse those symbols with the same symbols we use for classes. Here they refer to weights. The weights are initially set at zero and then updated using the pixel we have chosen randomly. We will shortly choose further random pixels to give us a better measure of the weights, but for the moment, just concentrate on the one in the diagram. For the weight corresponding to the i_th feature, we use the updating rule shown. In the rule, x_i is a feature of the randomly chosen pixel, x_ i superscript s is the corresponding feature of the nearest neighbor from the same class, and the x_i superscript o is the corresponding feature of the nearest neighbor from the other class. D is a distance measure. Applying the updating rule to the diagram shown, we see that the adjustment will be small and negative for feature x_1, but large and positive for feature x_2. That means the weight for the second feature increases, indicating its relative importance, while that for the first feature drops. The same process is carried out several times using a set of m randomly chosen sample pixels and updating the weights each time. The distance values are normalized by the number of samples m. If there is a reasonable distribution of pixels in each class. A little thought will show that the weight for the feature x_2 will go on increasing relative to that for x_1. A helpful modification is to use a set of the K nearest neighbors to the randomly chosen pixels as indicated in the diagram. In this case, the updating rule becomes as shown by the formula which indicates normalization by both the number of trials and the number of nearest neighbors. Again, the process is initialized with all weights Omega_i set to zero. Each weight is then updated by selecting m random samples using the rule above. At the completion of the process, those features x_i with weight values above a threshold are kept for subsequent classification. The ReliefF method, as originally formulated, applies to just two classes. It is readily extended to the more usual multi-class situation by adding in other class term, for each of the classes other than that from which the random sample is taken, leading to the more complete formula shown on the slide. The x_i superscript ok and they are features of the kth nearest neighbors of the current random pixel in each of the other classes. The probability expression p(o) over 1 minus p(s) whites the contributions from the other classes in proportion to their prior probabilities. In all these formulas, the distance measure can be any convenient metric. Euclidean and city block distances are the most commonly used. Another feature selection technique that avoids the need to use probability distribution models for classes, is non-parametric discriminant analysis or in the, like canonical analysis, it sets up measures of the distributions of pixel vectors within classes and the dispersion of the classes themselves in spectral space. It then sets and access transformation, which provides maximum separation between the classes based on those measures. Instead of using covariance matrices, though, it employs the concept of scattering matrices in the manner now to be developed. This is a bit complicated conceptually, but not hard mathematically. If you found the concepts difficult, it will affect your understanding of the equations. Since this procedure is only one of a number of distribution free methods, you may wish to pass over the material if you find it too complicated. But if you wish to employ it in practice, software is available that will guide you through issues. In its simplest form, NDA examines the relationship between the training pixels of one class and the nearest neighbor training pixels from another class. At the bottom of the slide, we show an expression for x, which represents pixel drive from classes, that is the nearest neighbor of pixel i from class r as shown in the diagram. Pay particular attention to how the subscript has been set up. It says j is a member of class s, which is a nearest neighbor of i, which is a member of class r. We can describe the distribution of class r pixels with respect to their nearest neighbors in class s by a covariance like calculation. However, because we're now not describing the distribution of pixels about a class main, which is a parametric description, it is better to use a different term than covariance matrix. To talk about the scatter of pixels with respect to each other, we use the term scatter matrix. The scatter of all of the training pixels from class r about their nearest neighbors in class s, is defined by the scattering matrix definition shown in the equation, which actually computes the expected value of the distance between those pixels times the transpose of that distance, giving a matrix effectively of the distance squared. In this expression, x subscript i is, remember r is the i pixel from class r. The amica r conditionality reminds us that the calculation is determined by pixels from class r. We then do a similar calculation for the scatter of the training pixels from class s about their class r nearest neighbors and then average the two measures. Usually, the average is weighted by the prior probabilities or relative abundances of the classes as shown in the first formula on the bottom of the slide. Often in the a is not just the nearest neighbor, but instead a set of k class is training pixels as the nearest neighborhood for each class r training pixel. The local mean over that neighborhood is then use in the calculation of the between class scattering matrix, giving rise to the second formula for the between class scatter matrix which includes a main vector term for the k nearest neighbors in class s shown on the next slide. The main vector in the previous calculations is given by the expression shown here. Note that if K, the size of the neighborhood, is the same as the total number of training pixels available in class s, the local mean becomes the class mean, and the between class scatter matrices resemble covariance matrices, although taken about the mean of the opposite class rather than the mean of the same class. This approach has to be made applicable to a multi-class situation. In doing so, we note that there are as many weighted means of the pixels from the other class as there are other classes. This is illustrated in the diagram for the case of three classes: r, s, and t. It is easier to express the expectations in algebraic form so that for c total classes the among-class scatter matrix is as in the formula given. In this expression, the inner sum computes the expected scatter between the N_r training pixels from class r and the mean of the nearest neighbors in class c different for each training pixel. The middle sum changes the class c, still relating to the training pixels from class r. The outer sum changes the class r for which the training pixels are being considered. The latter computation is weighted by the prior probability for the class. Having derived an expression for how the classes are scattered with respect to each other, we need to look at how the pixels scatter within the classes. We define the mean vector of a class based on the k nearest neighbors of the i_th pixel in class r from the same class as shown. This leads to the within-class scatter matrix in the second expression. How do we use the two scatter matrices: S_W and S_A to carry out feature reduction? We are looking for a new set of axes in which S_A looks as large as possible while at the same time, S_W looks as small as possible. We define the axis transformation in the usual way by the matrix equation Y equals D transpose X, and look for a y coordinate system in which we can maximize the joint measure, which we'll call J. That's defined as the trace of the within-class matrix inversed times the among-class matrix. Once the new axes have been found, we will have a maximum class separation in the first, followed by the next best separation in second, and so on. We then retain a subset of those transformed bands or axes as the feature-reduced representation of the data that we wish to classify. For high dimensional imagery, we need measures of separability that do not depend on the need to compute a covariance matrix. The method known as ReliefF devises a weighting scheme. Each feature or band is allocated a weight to indicate its significance. Bands with weights less than a user-specified threshold can be discarded when undertaking a classification. Non-parametric Discriminant Analysis, NDA, parallels canonical analysis in that it seeks a new set of axes in which to represent the pixel vectors, such that classes have maximum separation in the first new axis, next best separation in the next axis, and so on. Non-parametric scatter matrices are used in the calculations. After transformation, those low order features which do not help class separation are discarded, resulting in a data-set with smaller dimensionality. Please consult the solutions if you need to in order to help you consolidate your understanding of distribution free feature reduction techniques. 

### Module 3 Lecture 6: Assessing classifier performance and map errors

With this set of lectures on accuracy assessment, we come to the end of our material on general remote sensing before then completing the course by set of lectures on remote sensing with imaging radar. Assessing how I classifier performs and the accuracy of the thematic map it generates is one of the most important topics in operational thematic mapping in remote sensing. We will see that classifier performance and map accuracy are not the same thing. This is a crucial matter in practical remote sensing. If significant economic decisions are going to be based on the thematic maps in class hemorrhages generated by the machine learning techniques we have covered in this course, then we have to be as sure as we can that we are interpreting the results correctly. In this series of lectures, we will look at how we assess the performance of a classifier. The relationship between classifier performance and the accuracy of a thematic map. The number of testing samples that should be used to test thematic map accuracy and the number of testing samples needed to estimate class areas accurately. Recall from a previous work on machine learning methods that classifier performance is assessed by selecting a sample of testing pixels previously unseen by the classifier, the algorithm is applied to that set to check the performance of the classifier. In selecting the set of testing pixels, the analysts has to ensure that biases are not introduced by the fact that the aerial sizes of the classes might be quite different. If care is not taken, more testing pixels in a random sample will come from large classes whereas smaller classes will have fewer pixels with which to test the operation of the classifier. We will assume we have avoided such a bars in what we are now going to develop. The set of testing pixels is often referred to as reference data or less frequently nowadays as ground truth. The same terms are generally applied to training, either. It is common to express the results of testing the classifier by setting up an error metrics as on the next slide. In the past, it was sometimes called a contingency matrix or a confusion matrix. Terms you may still find in use. Here we see an error matrix which has been compiled from a thematic mapping exercise involving just three classes, A, B and C. The cells are populated according to whether the pixels have been placed into their correct class, assessed by comparing the classifier output with the reference data or whether they have been put into the wrong class. That is a classification error has been made. The rows represent the classifier. For example, it put 39 pixels in the class A. But when we look at what the referenced data tells us for each of those pixels, we see that somewhere really from class B and some from class C. The same thing happens for its labeling or pixels as class B pixels and class C pixels. Down the columns, we see how each of the pixels in the reference say there has been handled by class. For example, in the first column, the 50 pixels that are class A in the reference later have not all been put into that class by the classifier. Some have been put into class B and others into class C. If there were no classification errors at all, the matrix would be diagonal with zeros for all the off diagonal entries. In such a case, the classifier and reference data agree on the label for every pixel. In this slide, we just emphasize that the rows tell us about the classifier outputs. Remember, it has liable 39 pixels as class A. 50 as class B and 47 as clas s C pixels. So that is how many pixels for each class we will find on the thematic map. In contrast The column sums, as we noted on the previous slide, are the numbers of pixels in the reference data set for each class. We now introduce some new nomenclature. In placing some class A pixels into the other classes. The classifier has committed errors. So we refer to the off diagonal entries along the same row as errors of commission. The errors down a column, represent pixels from the reference cider that the classifier has failed to label property. We call those errors of omission. We now need to ask ourselves the question of how to determine the accuracy of a classification exercise using the entries in the error metrics. Well, the answer actually depends on whether you are the user of the thematic map produced by the classifier. Or the analyst who ran the classifier to label the pixels which led to the map. They are not the same. A measure of how well a classifier has performed is, how well it has labeled the reference pixels. If we choose class B as an example, the classifier was presented with 40 class B testing pixels and got 37 of them right. That is an accuracy of 92.5%. We call this the producer's accuracy, since it is the accuracy with which the results were produced. However, the user of the thematic map is interested in how many pixels labeled B on the map are correct. There are 50 class B pixels on the map. Only 37 of which are correct. The others are the result of the classifier committing errors on class A and class C pixels. We call this the user's accuracy. For this example, it is 74%. Here we see the full set of user's and producer's accuracies. Along with a commonly used measure of classifier performance, that is the overall accuracy which is computed as the total number of pixels labeled correctly by the classifier. As a fraction of the total number of pixels in the image. Note the range of producer's and user's accuracies compared with the average accuracy. There is an alternative technique used for assessing the accuracy of a classifier, which does not use a separate set of testing or reference pixels. It is called cross validation and involves the following steps. First, a single label set of reference pixels is used for both training and testing. It is divided into k separate equally sized subsets. Next, one subset is put aside for accuracy testing, and the classifier is trained on the pixels in the remaining k- 1 sets. The process is then repeated k times with each of the k subsets excluded in rotation. At the end of those k trials, k different checks of classification accuracy have been generated. The final classification accuracy is the average of the k trial outcomes. A variation to this approach is to use subsets of size one. It is then called the leave one out approach. There will be as many trials as there are reference pixels. Again, after all the trials have been performed, we will have an estimate of accuracy. Our way of summary, note that first correctly assessing the accuracy of a thematic map is an essential step in operational remote sensing. Next, accuracy assessment is carried out using labeled testing pixels but better referred to as reference data. The error matrix summarizes the performance information needed to assess class accuracy. There is a difference between producer's accuracy, that is how well the classifier has performed. And the user's accuracy, that is the accuracy of the thematic map produced by the classifier. Cross validation is an alternative means for assessing classifier performance. And finally, they leave one out approach is a special case of cross validation often found in practice. These questions direct your attention to understanding fully the concepts involved in assessing classifiers and thematic maps. Pay particular attention to the third question about cost overheads of the cross validation approach. 

### Module 3 Lecture 7: Classifier performance and map accuracy

We now wish to extend our analysis of the accuracy of a thematic map generated by a machine learning classifier. In the last lecture we looked at the difference between measures of the performance of a classifier class-by-class. And users accuracy which is related to the accuracy of the thematic map produced by the classifier. We now want to take this analysis further. Are focusing on the true accuracy of the thematic map. We will show that users accuracy only represents the accuracy of the map. If the selection of reference or testing pixels by class reflects the actual ground proportions of the classes. In other words, there prior probabilities. To do this, we need to use some simple probability theory. Let the variance t r and m represent the true labels for the pixels in terms of what is actually on the ground. They variables in the reference stay there. That is the data we have chosen to measure map accuracy. And their labels on the thematic map produced by the classifier. Hopefully the first two t and r are the same. But we will treat them separately for a moment. Let the two symbols Y&Z represent any of the available class labels for our three class example. They can be any of A B or C. Suppose we sampled the thematic map that has been produced by the classifier. If we check the accuracy of those samples against their true labels on the ground, then effectively we are estimating the map accuracy conditional probabilities which are expressed. The probability that the true label is said given that the map says it is it. That is what the user of a thematic map is interested in. More generally, the probability that the true label is, Y given at the map label is Z. Is the probability that Y is a correct class if the map shows it. This allows us to assess incorrectly as well as correctly label the pixels. If we use the reference pixels we have selected to assess accuracy and check the corresponding that labels, then we are in fact computing the classifier performance conditional probabilities. That is, the probability that the map sensor pixels Z. Whereas in fact the reference say that says it is Z. Or more generally, the probability that the map says it is said when the reference say that says it is, Y. If the pixels in the reference data set are chosen in the same class proportions as on the ground. We can assume that the variants r and t are the same. That means we have chosen how reference data set carefully so that the proportions of pixels in each of the classes are the same as, say, proportions in reality. We can then use r in place of 2 in our equations so that for example. The accuracy of the map from the previous slide is the probability the reference says Y. If the map says said, which is equivalent to the probability that the true label is, Y if the map so said? This is still a probability that the true label for the pixel on the ground is Y. If the classifier, then it's a thematic map, says it is in. Because we are assuming r&t are the same. We can now use Bayes theorem to relate the accuracy of the map to the performance of the classifier. So in other words, the probability that the reference datasets, Y given the map, says it is equal to the probability that the map says said if the reference letter says Y times probability that the reference say that is Y divided by the probability that the map label is Z. Now in this expression Pr = y. Represents the Lockley Hood that class why pixels exists in the region being image. This is in fact a property of the scene on the ground. In contrast he of =D. Is the probability that class Z pixels appear on the thematic map? And it is given by the. Equation shown at the bottom of the slide where we sum joint probabilities. Well, let's take stock of where we are at this stage of the analysis. The classifier labels pixels as m, whereas that labels in the reference later are r. Thus, a classifier generates the probabilities that the classifier output will be Y if the reference data is Y, shown as the conditional probability p m equals Y given r equals Y. Whereas we as users are interested in the probability that the reference data is a Y pixel if the map or the classifier says it is. That is a conditional probability, p r equals Y given m equals Y. We will now apply those concepts using our three class example from the previous lecture. Here, we'll show the error matrix we are working with on the upper left hand side of the slide. The table on the right shows a classifier performance or produces accuracies. The users accuracies and the map accuracies computed from the expressions from two slides back, involving Bayes theorem. If you wish to check the calculations, you will need to compute the full set of classifier performance probabilities. That is, the probability that the map says Z if the reference data says Y using the entries from the error matrix. Note, the effect of the class prior probabilities that is, the real proportions of the classes on the ground on the results. The map accuracies are only the same as they uses accuracies when the set of label pixels used for reference data is in the same proportion by class as the real situation on the ground. The second and third examples show the differences if that is not the case. The last is an extreme by the instructive example. The average map accuracies given here were computed by the formulas developed on the next slide. We now consider the average accuracy of the thematic map generated by the classifier. Based on what we have done so far, the accuracy by class is expressed as the probability that the true class is Y if the thematic map says it is, that is shown by this conditional probability. In computing an average accuracy of all classes, we could be tempted just to average the above expression over all values of Z. However, that gives too much weight to the smaller classes and diminishes the contribution to the average of the performance on the larger classes. It is better, therefore, to compute the map accuracy as the average but weighted by the probabilities of occurrences in a thematic map. That leads to the map accuracy expression shown here in two forms. The last expression is important. It tells such that the true average map accuracy can be obtained from the classifier performance provided the classifier performance probabilities are weighted by the class prior probabilities. The average map accuracies noted in the final column of the previous slide have been computed that way. Some analysts prefer to express classifier performance in terms of the Kappa coefficient. It is a measure intended to avoid any chance agreement between the performance of the classifier and the reference data. If we again use the results of the example from the last lecture, we can estimate the chance agreement. The two sets of calculations on the right hand side of the slide show the performance of the classifier by class and the allocation of pixels by class in the reference data. The probability that they both place a pixel in the same class is given by the products of their individual performances. The probability that both the classifier and the reference data place a pixel at random in the same class, overall classes is the sum shown her0,e 0.106 plus 0.108 plus 0.117 Which is 0.331. This is the random chance of their agreeing on the label for a pixel. On the other hand, the probability of a correct classification determined from the agreement of the classifier output and the reference data is 35 plus 37 plus 41 divided by 136, which is 0.831. In terms of those calculations, the Kappa coefficient is defined as shown here, and for our example has a value of 0.747. In order to give meaning to that number, we use the ranges shown in the table. Kappa is not strongly supported by all analysts because its value is not a unique function of its arguments as shown here. By way of summary note. There is a difference between the performance of a classifier and the accuracy of the resulting thematic map. Map accuracy can be obtained from classifier performance if the class prior probabilities are used. Classification errors on large classes can give significant errors of commission into smaller classes. The Kappa coefficient is a common alternative measure for expressing classifier performance. And finally Kappa suffers because its range of values does not naturally imply levels of performance. Nevertheless, it is often used as a comparative measure when jointly assessing different classification algorithms. In answering the first question here, you may wish to look at an extreme example. For example, consider a classification exercise with just two classes. Suppose in reality class A occupies 90% of the scene on the ground and class B the other 10%. Suppose your classifier is 100% accurate on class B, but has 50% error for class A pixels. What does that imply for the thematic map? 

### Module 3 Lecture 8: Choosing testing pixels for assessing map accuracy

We need now to consider one last topic on accuracy assessment, to complete the theoretical aspects of remote sensing before we move onto radar. When we looked at training a classifier algorithm, we had to have enough training pixels per class to estimate reliably any parameters in the technique we chose. We now turn to the related matter of choosing independent samples from a thematic map, that is referenced data or testing data, in order to get reliable estimates of the maps accuracy. If we don't choose enough, then we will end up with a poor estimate of map accuracy. We will undertake this analysis by looking at how well the testing samples we choose actually sample the thematic map. In this slide, the top diagram represents the thematic map itself. We are going to work out this theory with just two classes, but we will comment on the multi-class situation later. In a sense, we don't have to know what happens with each of the two classes. All we are interested in is whether a pixel in the map is correctly labeled irrespective of its class. By knowing how many pixels are accurate overall, we can get the map accuracy. In this top diagram, the shaded pixels are those which have been labeled incorrectly by the classifier. We can describe the situation by a binary period, which takes the value one if a pixel has the correct label, and zero if it is in error. The second diagram shows a random sample of testing pixels. Because they are chosen randomly and because we do not know which pixels in the map above are correct and which ones are in error, we have to choose enough testing pixels to be sure we sample the map so that our accuracy estimate is reliable. The bottom diagram shows the testing samples layered over the thematic map. We now define a new boundary period which takes the value one if a correctly labeled pixel is sampled, and zero otherwise. First, we look at the distribution of errors described by the variable y_i as shown. That theorist has a binomial distribution. If we take the sum of all y_i, we get the total number of correctly labeled pixels because the incorrect ones contributes zero to the value of the sum. That means the accuracy of the map can be expressed as shown here by the formula P equals one over N times the sum of the y_i periods. N is the total number of pixels in the map. What we are interested in is getting an accurate estimate of P, the map accuracy. Now consider the properties of g_i. It also has a binomial distribution. By definition, little n is this begin. Otherwise, we will be sampling every pixel in the thematic map. The sum of all the g_i is the number of correctly labeled pixels, but in this case as found in the testing set. As before g_i is zero for testing pixels which are incorrectly labeled so that they do not contribute to the value of the sum. The proportion of correctly labeled testing set pixels, which is always what we use as a measure of accuracy, is given by the expression for p. That is, one over n times the sum of the g_i. We want p found from the testing set to be a good estimate of the actual accuracy of the thematic map, P. The question is, how many testing set pixels n, are needed to ensure that? Since the sum of binomials is itself a binomial distribution, the map accuracy estimate p, is binomially distributed. We can assume that g_i and y_i come from the same underlying distribution. They then had the same means so that the expected value of p is equal to the actual map accuracy P. Although the expected value of the map accuracy found from the testing pixels is the main P, the actual value found can be different. Hopefully, it would be in the vicinity of P, but depending on the number of testing pixels taken, it might be quite different. But how different might that be? To answer that, we look at the variance of p about the main P, and that variance is given by the expression shown on the slide. Usually, the number of pixels in a remote sensing image far exceeds the number of testing pixels so that the variance takes the form of the bottom equation on this slide. It is inversely proportional to the number of testing pixels n so that N reduces severance and makes the estimate p, closer to the actual map accuracy P. Repeating the message from the previous slide, the variance reduces with more testing pixels. Thus, more testing pixels gets us closer to the true value of the accuracy of the thematic map. To a good approximation, little p can be assumed to be normally distributed about the mean big P, as illustrated in the diagram. Two standard deviations about the mean contain 95 percent of the population. Thus, with 95 percent confidence, we can say that our estimate of the accuracy of the thematic map lies within about two standard deviations of the true value. Let's say now how it can use that information. With 95 percent confidence, we know our estimate of the accuracy of the thematic map, lies within about two standard deviations of the true value. Now suppose we are happy for our estimate of the accuracy to be within plus or minus epsilon of its true value. That is, we're looking for little p, which is equal to big P plus or minus epsilon. With 95 percent confidence, we know it will be within the range if epsilon represents two standard deviations. Thus, we have the formula for epsilon equal to 2 times the square root of p times 1 minus p divided by n, which gives the number of testing pixels shown on the bottom of the slide. Interestingly, note that it is a function of the true accuracy p. Is that unusual? We'll know. If p was equal to 1. In other words, the map was 100 percent correct. We really don't need to center it at all. Whereas if P were low, the numerator, P minus p squared is large, meaning that more samples are required if the accuracy is relatively poor. As an example, suppose we are happy for an estimate to be within plus or minus 0.04 of a true proportion, which is thought to be about 0.85. That is, we are happy with accuracies in the range of 0.81-0.89. Then at the 95 percent confidence level, n equals 319. That means randomly sampling or randomly selecting 319 testing pixels will allow a thematic map accuracy of about 85 percent to be checked with an uncertainty of plus or minus 4 percent, with 95 percent confidence. Further examples are in the table on the next slide. Here we see explicitly that the required number of testing pixels decreases with the actual accuracy of the thematic map. Our previous analysis was focused on the need to establish a sufficient number of testing pixels so that the overall accuracy of a thematic map could be assessed. What we want to know now is how to ensure we have enough testing pixels per class, to know that each class is accurately labeled. One approach upon the paper referenced here, leads to the numbers shown in the table below. In this case, with an uncertainty in the estimate plus or minus 10 percent at the 95 percent confidence level. When looking at overall map accuracy, we were able to use binomial statistics since only two outcomes are possible, a correctly labeled pixel or an incorrectly labeled pixel. If we wish to look at class wise accuracies for each pixel in a thematic map, a multinomial probability distribution is needed. We will not go into the development here. But note that it leads to the following conservatively high estimate for the number of testing pixels required per class. When we are interested in whether we have the correct class for a pixel. Not just that it's this correctly labeled. We can use the expression shown on the slide. Epsilon is again the level of tolerance we need around the estimate of the class accuracy. B is the upper beta percentile for the course grade distribution. With one degree of freedom. Beta is the overall precision needed divided by the total number of classes. For the background theory here, see that paper by Tortora. Consider this example from the book by Congalton and Green. Suppose we have eight classes in a thematic map. We want to find the estimate of the accuracies within plus or minus 5 percent, and we want our results to be at the 95 percent confidence level. In this case, beta equals 0.05. divided by 8, which is the 0.625 percentile of the distribution, giving b the value of 7.568. That's the total number of testing pixels needed, is 757 as shown. Therefore, we need just under 100 per class. By way of summary, note, just as in training, where we need enough labeled reference pixels to get good estimates of the parameters of a classifier, in testing, we must have enough labeled reference or testing pixels to ensure you can get good estimates of the accuracy of a thematic map. We need enough testing pixels to ensure that the estimate of the map accuracy obtained is as close as possible to the actual accuracy of the thematic map generated by the classifier. We can look for estimates of the overall map accuracy or the accuracies with which each of the individual classes have been mapped. The latter requires more training pixels. It is important to understand the messages behind each of these three questions, if you are to be sure you understand the guidelines for selecting the number of testing pixels. 

### Module 3 Lecture 9: Classification methodologies

Often, we say analysts, sometimes naively using machine learning techniques for thematic mapping. Without thinking carefully about the objectives of the exercise and whether a careful mix-up procedures can produce better results. Here, we want to look at some mixed algorithm methodologies. Too often, analysts just apply a chosen classifier to an image, using labeled training data for each class and expect good practical outcomes. Such an approach works okay for highly stylized images, such as the labeled data sets regularly used to test new algorithms, but they are not optimal when looking at real and heterogeneous image data. In this lecture, we will look at practical methodologies that work well in practice and yet are often overlooked. We start with a review of the properties of supervised and unsupervised classification. Remember, in supervised classification, the analyst acquires beforehand a labeled set of reference data, that is, training pixels, for each of the classes of interests and desirably for all apparent classes in the image. That data is used to estimate the parameters or other constants required to operate the chosen classification algorithm. The algorithm can then be applied to the full image of interest, and testing pixels can be used to assess how well it has performed. The amount of training data will often be less than 1-5 percent of the image pixels. The learning phase, therefore, in which the analyst plays an important part in the labeling of pixels beforehand, is actually performed on a very small part of the image. Once trained, the classifier can then attach labels to all the image pixels, and that is where a significant benefit occurs in thematic mapping. The output from supervised classification consists of a thematic map of class labels, often accompanied by a table of area estimates, and importantly, an error matrix which indicates by class, the residual error, or accuracy of the final product. In unsupervised classification, clustering algorithms are used, typically on a sample of an image to petition the spectral space into a number of discrete clusters or spectral classes. It then labels all image pixels as belonging to one of the spectral classes found, typically using a minimum distance assignment. A cluster map can be produced in which the pixels are given labels indicating which cluster they belong to. Unsupervised classification is a segmentation of the spectral space in the absence of any information fed in by the analysts. That's why it's called unsupervised. Analyst knowledge is used afterwards to attach information class labels to the map found by clustering. This is often gathered by the spatial distribution of the labels shown in the cluster map. Clearly, this is an advantage of unsupervised classification, as we will see in an example. In general, clustering algorithms are computationally expensive to run, compared with most supervised classification methods. We can benefit thematic mapping by bringing together the strengths of supervised and unsupervised methods into a classification methodology. Even though very old and devised when the maximum likelihood rule was the key machine learning classifier used in remote sensing, it teaches us a lot about operational thematic of mapping. We will demonstrate the method with a simple example. It uses the best aspects of the unsupervised and supervised approaches. Unsupervised clustering is used to discover the spectral classes using an image. Supervised classification after having associated those spectral classes with the information classes of interest, produces the thematic map. It is outlined in five steps on the next slide. The unsupervised supervised methodology developed by Fleming and his colleagues works as follows. Step one, we use clustering to determine a set of clusters or spectral classes into which the image results. This is performed on a representative subset of data. Spectral class properties, that is, statistics can be produced from this step. Small classes that might be overlooked by the analysts, such as those consisting of mixtures along class boundaries, and elongated classes like rivers and stream systems, will be picked up. Step two, using available reference data, that is maps, air photos, the image itself, and even the special distribution of clusters in the cluster map, will associate the spectral classes or clusters with information classes. Frequently there will be more than one spectral class for some information classes because they might look different spectrally to the sensor, even though the analysts calls them by the same name. This is an important consideration. Step 3, we can then perform a feature selection to see whether all the features or bands need to be retained for reliable classification. Depending on the classifier algorithm used and the data set. This step is not always necessary. Step 4, we use the supervised algorithm to classify the entire image into the set of spectral classes. The hybrid approach was used initially with maximum likelihood classification, but it can be used with most classifier algorithms. Step 5, label each pixel in the classification with the information class corresponding to its spectral class and use independent testing data to determine the accuracy of the classification product. We now use two simple examples to highlight the value of using unsupervised classification as a means for identifying spectral classes and for generating the signatures of classes for which the acquisition of training data would be difficult. Those results are just then used in a supervised algorithm. To demonstrate the hybrid approach, we use an image segment recorded by the HyVista HyMap sensor over the city of Perth in Western Australia. It is centered on a golf course. The obvious cover types are water, grass that is the fairways in the golf coarse, trees, bare ground including bunkers, a clubhouse, tracks and roads. Apart from a few cases, the distribution of cover types suggests that it might be hard to generate training fields for all classes of interest. Five bands were used, band 7 which is visible green, band 15 which is visible red, band 29 which is in the near infrared and band 80 and band 108 both in the mid infrared range. These last two are the infrared maxima of the vegetation and soil curves midway between the water absorption regions. It was felt that they would assist in discriminating among the bare ground and roadway classes. The three heterogeneous fields shown on the image were used for clustering. Among them, they seem to include all cover types. The three fields were aggregated and the Isodata clustering algorithm was supplied to the combined dataset with the goal of finding 10 spectral classes. Note there are seven information classes in the image. The results for the three cluster regions are shown on the top right-hand cluster map. We can say that the spatial distribution of the clusters matches the spatial distribution of the information classes in the image which is as to be expected although there may be more than one cluster corresponding to each information class. In this slide, we showed the main vectors of each of the clusters found by looking at where the clusters lie in the image and by looking at the shapes of the spectral reflectance characteristics seen by the distribution of the elements of the cluster means, we attached the information class labels shown on the slide. Several important observations can be noted here. First, two of the information classes are each represented by two spectral classes or clusters. Secondly, one spectral class or cluster has been generated from the water vegetation mixed pixels along borders. Thirdly, elongated classes such as tracks and roads have been found as separate spectral classes. It would be a little hard to develop 20 pixels for those classes otherwise. For verification, this slide shows an infrared versus red bars spectral plot for the clusters found. The distribution of classes agrees with where they would normally lie in such a plot. Using the set of spectral classes found from clustering and their mean vectors and covariance matrices which are generated automatically by the Isodata algorithm, we get the symmetric map shown on the left-hand side of the slide. The right-hand, the medic map is colored. Sets of the clusters or spectral classes corresponding to each information class have the same color. Here we show the final thematic map alongside the image to allow a comparison. In this simple exercise, we did not choose testing data so it cannot report quantitatively on the map accuracy. We now look at another simple example of the combined unsupervised, supervised approach. It involves classifying an arid region of Australia employed for growing cotton by the use of irrigation from a nearby river. The task was to assess the area in hectares sown to cotton as a surrogate for the amount of water used. Field agronomists had assessed the hectarage of cotton crops in the region but required corroborative evidence. The image to be classified consists just of the visible red and the first of the two near infrared bands of a landsat multispectral scanner image, recorded in February 1991. Although the region is very dry at that time of the year, apart from the crops there is a gallery or riparian forest along the river, which provides another vegetation class. Supervised classification was carried out using the minimum distance classifier, a very simple algorithm. On the left of this slide, we see a near infrared image of the region to be analyzed. A test semi-image has been identified on which the results are to be evaluated. The Darling river can be seen in the image. The cotton fields are mostly in the test area, which is what in the left-hand image, indicating a high IR response as well and approximately triangular shaped crop is shown in the bottom southeastern corner of the image. This is cotton, as some other scattered fields. The full rectangular selections in the right-hand image subsets, along with our sample of the lower triangular crop were use to resolve the spectral space in the spectral classes by clustering. Here the simple single-pass clustering algorithm was used and each of the five of heterogeneous regions was clustered separately. The results are shown on the next slide. The results of clustering as shown here in bar spectral plot form. In terms of the means of the cluster centers found, there were 34 clusters in total, which were then rationalized down to the 10 shown. That was done by associating the clusters with information classes using black and white and color air photos along with photointerpretation of the image itself. Those 10 grouped spectral classes were considered to be adequate to differentiate the image into its main cover types and thereby avoid any errors of commission, which might lead to poor estimates of the area of the cotton crops. When the minimum distance classifier was applied to the test image, using the 10 rationalized spectral classes from the previous slide, it was found that the cotton crops accounted for 803 hectares. The field agronomists had estimated 800 hectares. In this exercise, it was not necessary to produce thematic map since the important result was the air of cotton in the test region. How can we ensure we have pixels in the training dataset representative of all the information classes in an image and does it matter? For example, consider the river and gallery forest class in the previous irrigated crop example. How can the user select beforehand a representative set of those pixels with which to develop one of the binary classifiers in a support vector machine? If the pixels for that class are not well differentiated and indeed that may be mixtures of water and trees, then hand selection of the training fields might be difficult. Again, the analyst may wish to consider using a hybrid clustering support vector machine approach. The cluster step as with the maximum likelihood and minimum distance classifier examples just presented, would be carried out on representative and heterogeneous parts of the image in order to generate a set of spectral classes with which to work. Those clusters could be aggregated into single information classes before application of the support vector machine in order to limit the number of binary classifiers needed, although in some cases there may be value in retaining some information class sub-sets that improves separability. By way of summary, unsupervised classification based on clustering can be a very effective way of generating training pixels and revealing spectral differences within information classes, that is, spectral classes. Often, more accurate supervised results will be obtained if sets of sub-classes are use for each information class. Rather than use reference data explicitly to train a supervised algorithm, it is used to attach information class labels to clusters. This is a traditional unsupervised classification approach. Even there the analysts might be interested in just a small number of classes, sometimes one, better results will often be obtained if all apparent cover types are represented in the classification, in order to avoid errors of commission caused by large classes spilling over into the small classes. Finally, because the convolutional neural network develops knowledge about a scene as a whole, and to particularly the special nature of information classes and implements exceedingly complex piecewise linear decision surfaces, it is unclear at this time whether a prior unsupervised clustering will help improve performance. Answering this question requires a careful assessment of the spectral reflectance characteristics of the cover types involved. 

### Module 3 Lecture 10: Other interpretation methods

We don't always have to use supervised or unsupervised techniques for pixel identification in remote sensing. Other approaches are used in practice. Most of our analytical focus in the course has been on Machine Learning. There are, however, a number of other approaches used regularly in practice, often founded on scientific as against data analysis principals. Lab searching using pixels spectra in hyperspectral images is one example of that, which we have already seen. One of the most longstanding and effective methods for assessing cover types recorded in a remotely sensed image involve indices, which are metrics computed from the brightness values of the pixels in sets of bands. For example, a simple vegetation index might just be the ratio of a near infrared measurement and the visible red measurement. The rationale for which becomes evident when we look at common spectral reflectance curves, such as shown on the bottom left of this slide. If we apply that vegetation index to all the pixels in an image, it will produce a map in which pixel brightness is correlated with a strong vegetation signature. The indices should be defined as functions of surface reflectance, which is implied by our reference to the spectral reflectance curves. But in practice, the formulas are often based on the digital numbers in the received image product. There are better vegetation indices, such as the two shown here. The most common is the Normalized Difference Vegetation Index or NDVI. Here we see an application of NDVI to know our AVHRR images for which the pixel size is one kilometer. This allows us to develop a continental scale vegetation index map, which demonstrates, in this case, the evolution of drought conditions over the past four years in Australia. The spring sequence is perhaps the most profound during the huge change in green biomass over the four year period. Many other indices are used in remote sensing, including those shown here, which allow compensation for the amount of soil reflectance in a pixel and those for monitoring water conditions, soil, green biomass and so on. Again, by way of summary, vegetation and other indices are the result of band arithmetic. That is, computing new values for the brightness value of a pixel from sums, differences, and questions of the originally recorded bands. Those new values emphasize properties of importance such as greenness. Secondly, used in time-series, indices can follow the development of important phenomena, such as the development of a crop or the evolution of a drought. Answering this question requires you to plot the equations of straight lines that result from the index definitions. For example, from the simple vegetation index, we have near infrared equals vegetation index times red, where vegetation index is the slope of a straight line through the origin. 

### Module 3 Lecture 11: Fundamentals of radar imaging

We are now going to change direction completely and spend the rest of this course looking at the technology of imaging radar as a remote sensing tool. In the first module of this course, we looked at the Planck radiation law. Remember, it shows how bodies at different temperatures emit radiation or energy. As seen on the diagram to the right of this slide, the earth at 300K is a low emitter by comparison to the sun or a burning fire. But let's focus on the curve for the earth itself in more detail in the next slide. Here we see the earth curve expanded. Look particularly at the right-hand side covering the microwave range and note that the emission from the earth at microwave frequencies is much lower than its emission in the visible and infrared range. Although there is a very small level of radiation from the earth at microwave frequencies, we can, for all intents and purposes, assume the earth is dark. We can therefore take advantage of that by irradiating the earth with an artificial source of microwave radiation, just as we use a torch or a flashlight at night when there is little natural light available. When we use radar to image the earth's surface, we do so to the side of the platform, whether that be a satellite, an aircraft, or a drone. The reasons for this will become clear soon. There are two things we then need to understand. First, we need to understand how the energy reflected back to the platform is representative of the properties of the landscape. Secondly, we need to understand how such an arrangement gives us spatial resolution in both the across track and along track directions. Just as with radio, television, and mobile or cell phones, clouds are not a problem with radar imaging. This is a huge advantage because clouds can completely stop remote sensing imaging at optical wavelengths. Also, since the platform carries its own source of irradiating energy, imaging can be carried out anytime of day or night. The energy radiated to the surface is not continuous, but consists of a regular sequence of pulses, as shown in this slide. Importantly, those pulses travel at the speed of light, which is 300 megameters per second or 300 million meters per second. Reflections from targets closer to the platform will appear back at the radar earlier than those from the targets further from the platform, as illustrated by A, B, and C in the diagram here. The pulses are actually the envelopes of a burst of microwave radiation at the frequency or wavelength of interest. Just as with optical imaging, in which the earth responds differently at different wavelengths, the same is true of radar. The frequency inside the pulse will determine the property of the earth that we are measuring. The pulse transmitted in the previous slide is actually repeated on a regular basis called the pulse repetition frequency. The rate at which the pulses are transmitted is synchronized with the forward velocity of the platform so that the strips of ground image by each of the pulses align with each other as shown in this diagram. This provides continuous coverage. It is actually the property of the antenna used to radiate the pulse that gives the so-called footprint on the ground. The angle with which the radiation is transmitted to the side of the platform is called the look angle. Its corresponding version on the ground is called the incidence angle. They will be the same for a low-flying platform and a flat surface. There will be slightly different, however, at spacecraft altitudes where the earth's curvature might be important and also for ambulating terrain. We now need to understand quantitatively, how the radar separates targets, or regions on the ground as different distances as from the platform. In the diagram here, we show two targets A and B separated in the direction and from the platform. We call that the ground range direction, in contrast to the slant direction which is parallel to the beam from the radar as shown. Suppose the two tablets are delta r apart, in the slant range direction. Given that the radiation travels at the speed of light c, and that it travels to and from the platform. The difference in time between the two echoes is given by delta t, which is twice the distance divided by the speed of light. If the pulse duration is tall, as in the previous slide, then the two targets can be resolved provided there are no closer together then c2 divided by 2. We call that limit the slant range resolution, because we cannot resolve between the pulses if they overlap. This is illustrated on the bottom of the slide. Of course, as remote sensing uses, we are interested in the resolution along the ground range direction, rather than that in the slant direction. From trigonometry we can see that to be the formula at the bottom of the slide, there is a sine theta in the denominator. From the last formula, we can make some important observations. First, there is no special resolution directly under the platform. That is, when theta equals zero. That is why the system has to be side looking. Some early aircraft radars of this type were called side looking airborne radars or SLARs. Secondly, the slant and ground range resolutions are independent of the altitude of the platform. That's an amazingly useful property. Thirdly, ground range resolution is a function of incidence angle, and thus will vary across the swath. It is best in the far swath where theta is largest and poorest in the near swaths where theta is smallest. This is opposite to that for optical sensors which have their best resolution closest to the platform. Just as we can see more detail in the near range when we look at the window of an aircraft. Finally, if the antenna radiated to both sides of the aircraft and a single receiver were used, then there would be a right-left ambiguity in the received signal. That can be circumvented using two antennas and receivers, but most often that is not the case. Most systems encountered in practice radiate to one side of the platform. Having looked at how we achieve resolution in the ground range direction, that's from the platform. We now need to see how we achieve spatial resolution along the track of the platform. Although a curious line it is called azimuth resolution. It is determined by the properties of the antenna used on the platform for transmitting and receiving the microwave pulses. In particular, it is the so-called beam width of the antenna in the azimuth direction that sets the azimuth resolution in the manner we will now describe. From antenna theory, the width of the beam transmitted by an antenna is directly proportional to the operating wave length and inversely proportional to the length of the antenna, as shown on the slide. That means that a longer antenna will give a narrow beam width on the ground and thus a smaller strip irradiated action. What we want to know now is the width of that strip because that defines the azimuth resolution. From the previous slide, we had the top equation as the beam width of the antenna expressed in radians, at a range of R subscript nought that projects onto the ground, l dimension of R subscript l meters. That is the width of the illuminated strip and thus the azimuth resolution. Note from this formula, the azimuth resolution gets worse with increasing slant range and better with increasing antenna length. At this point, it is worth doing a simple calculation as on the next slide. Consider an aircraft situation, the slant range is 2000 meters. The radar antenna is three meters long in the azimuth direction and the operating frequency is 10 gigahertz. That corresponds to a wavelength of three centimeters. Substituting those values into the formula, we see that an azimuth resolution of 20 meters results, which is acceptable. However, if the same radar is placed on a spacecraft with a slant ranges 1,000 kilometers, the azimuth resolution will be 10 kilometers, which is totally unacceptable. Clearly, a better method is needed to achieve good azimuth resolution at spacecraft altitudes. What if we use a short antenna and thus irradiated a broad region on the ground as shown in this diagram? Clearly, the azimuth resolution should be very poor. The length of the region illuminated in the along track direction is the slant range to the ground multiplied by the beam width of the small antenna, which is as shown in the equation on the slide. What happens, though, if there's more than one pulse transmitted while the point target is in view of that large right at beam? To answer that, consider the geometry on the next slide. Here we see the platform moving past the target. The radar just encounters the target when the target first comes into the beam and loses it when it leaves the beam. All the time the platform is transmitting pulses, which incidentally are called ranging pulses, and it's receiving echoes. By processing those echoes using a signal processing technique called matched filtering, we are able to make the length of the spacecraft travel look like a long antenna or aperture as shown in the figure. That travel distance is equal to the projected beam width on the ground of the real short antenna. The effective long antenna is called a synthetic aperture, leading to the name synthetic aperture radar or SAR for the technology. What beam width does a synthetic aperture create, and what footprint does that lead to on the ground? Remember the beam width in radians is the operating wavelength divided by the antenna length. For the synthetic antenna, that is as given by the formula in the center of the slide. The two in the denominator comes about because the transmission is from the antenna to the target and then back to the antenna. When projected onto the ground, this gives an along track or azimuth resolution as shown at the bottom of the slide. Truly an amazing result because it tells us that the azimuth resolution in synthetic aperture radar is directly proportional to the antenna length in the azimuth direction and not inversely proportional as before. We will look at the implications of that in the next lecture. By way of summary, we note so far, first that imaging at microwave frequencies makes use of the fact that very little natural microwave energy emanates from the Earth's surface. We can thus irradiate the surface ourselves with a source of microwave energy carried on a remote sensing platform. Secondly, as with optical remote sensing systems, the forward motion of the platform sweeps out a swath of recorded image data of the Earth's surface. Thirdly, imaging radars are side looking; range resolution is best at far swath, that is large incidence angles, and worse at near swath that is, with small incidence angles. Fourthly, good azimuth resolution is obtained by using a signal processing technique called matched filtering, that allows a small real antenna to be used on the platform but achieves a very high spatial resolution equal to half the size of the real antenna; this is called synthetic aperture radar. Finally, in SAR, both the ground range and azimuth resolutions are independent of the platform altitude and operating wavelength, which is the wavelength of the frequency burst modulated by the ranging pulse. Again, this is a significant practical advantage. In the first question here, your attention is drawn to the node to receive sufficient energy that the brightness of a pixel is of a measurable value and well above any noise that might be present in the system. 

### Module 3 lecture 12: Summary of SAR and its practical implications

Let's now summarize where we were in the last lecture and then note some practical implications of synthetic aperture radar. In this slide, we summarize the important geometric properties of radar imaging, including how the width of the recorded swath depends upon the system parameters. We know now how to resolve the landscape into pixels using radar. Our objective from this point on is to understand what the pixels tell us about the landscape. Along the way, we have to consider a number of related matters, including some further properties of electromagnetic radiation and sources on geometric and radiometric distortion in radar images. The first important point to take note of is that the wave form within the ranging pulse does not have a constant frequency, but chirps over a limited range about a center frequency. That chirp range is very small but nevertheless, it is a major determined for achieving high range resolution. Understanding that is beyond this series of lectures, but the book referenced provides the details for those interested. The chirp range is small enough, that we can regard the imaging wavelength as equivalent to that of the center frequency. A second practical matter and one which has significant implications for understanding the properties of radar images, is to do with the polarization of the incident and reflected waves. Power or power density is carried forward as a result of both an electric field and a magnetic field, that oscillates at right angles to each other and to the direction of propagation as illustrated by the field vectors in the left-hand diagram on this slide. There is a fixed relationship between the magnitude of the magnetic field and the magnitude of the electric field. They are always at right angles each other. Thus, we only need to consider electric field from now on. The direction in which the electric field vector points, defines the plane of polarization depending on the transmitting antenna used at the platform, the radiation incident at the earth's surface can be vertically or horizontally polarized as illustrated in this slide. The transmitted signal can either be vertically or horizontally polarized. Irrespective of the polarization of the incident radiation, the scattered signal can also have both horizontal and vertical components. Whether either or both of those components can be received, again, depends upon the properties of the receiving antenna. Generally, the same antenna is used for transmission and reception. Whether it can transmit and receive both polarizations will depend on its design. There are four possible radar configurations. Transmit horizontal and receive horizontal, which is called HH, transmit horizontal and receive vertical, which is called VH, transmit vertical and receive horizontal, which is called HV, or transmit vertical and receive vertical, which is called VV. Note that the first in the double letter convention is they received polarization, while the second is the transmitted polarization. Most systems are the HH or VV, and are called single polarization radars. Whereas more sophisticated radars accommodate all four configurations, and are set to be fully or quad-polarized radars. This slide shows a radar image example over the city of Darwin, Australia in two different polarizations, HH and VV. It was recorded by the ASAR C band imaging radar carried on the Envisat platform. Although much of the image seem similar in the two polarizations, there are some notable differences as indicated. In many cases, polarization is an important discriminator in radar imagery. Here is another Envisat ASAR example over the city of Kalgoorlie, in Western Australia. This time, one of the images is cross polarized as indicated. Again, note the differences between the lock and cross polarized responses. Note the comment on the bottom left-hand side of the slide. Monostatic radar, main choosing the same antenna for transmission and reception. Later we will relax that requirement, but for monostatic systems which are by far the most common, we assume the two cross polarized responses are identical. In this example, we see all four polarization configurations. The image was recorded by the TerraSAR-X Satellite Program. The color composite image was constructed by displaying the horizontally polarized image as red, the vertically polarized image as green, and the two cross polarized responses as blue after they have been added. Bringing all this together, we say that operational radars in remote sensing are defined by three important system parameters. The operating wavelength, the polarization, and the incidence angle, or range of angles. Ever since radar was developed during the Second World War, the range of wavelengths has been described by a letter designation. It is not unique, but the frequency and wavelength ranges shown here are the most usual in remote sensing. Of this set, L, C, and X are the most often encountered. You may wish to take note of the formula on the right-hand side of this slide, which relates operating wavelength in meters and frequency in megahertz. In this slide, we see the technical characteristics of a number of satellite and aircraft radar remote sensing missions. You can convert between wavelength and frequency, using the formula from the previous slide. We've seen that the radar resolution cell or pixel size in a displayed image product, is defined by the ground branch resolution and the azimuth resolution. The ranging pulses are chirps about the operating frequency or wavelength. Although that requires some sophisticated signal processing to resolve targets in the range direction, better sensitivity to weak targets is possible. The important parameters of an imaging radar are the wavelength, polarization, and look angle. Operating wavelengths or frequencies are described by letter designations, reputedly chosen as a means for discussing the operating frequencies of aircraft detection radars during the Second World War. These questions are designed to consolidate your understanding of the geometric properties of a radar image. 

### Module 3 Lecture 13: The scattereing coefficient

We now turn our attention to the properties of the earth itself that are measured by the various synthetic aperture radar programs. In focusing on the properties of the earth surface detected by imaging radars, we start by analyzing a more traditional radar situation in which we detect at point of target. We will then generalize that to distributed habits such as crops, forests, another ground cover types of interest in remote sensing. Consider the situation shown in the diagram in which we start with the concept of an isotropic radiator on the left hand side, which is a transmitter which radiates equally in all directions, a simple light bulb can be thought of as an approximation to an isotropic radiator. In the next couple of slides we will work through the details of this diagram. We look first at the transmission of radiation from the isotropic source to the target. The power transmitted. P sub script t spreads out over the surface area of a sphere at a distance are not the power density over that spherical surface is given by the first equation here, which is just the power transmitted divided by the surface area of the sphere. When a real antenna is used rather than an isotropic radiator, the power density towards the target is increased by the gain of the antenna. The gain is just a number that tells us how much better the antenna is in the preferred direction compared with isotropic behavior. It is dimensionless. Now we have to describe the properties of the target in such a way that we can use it in our calculations. We describe it by an artificial area Sigma, called its radar cross section, which is defined as a cross sectional area at right angles to the incoming beam such that the area intercepts power from the incoming power density and re radiates it isotropically. The power available for isotropic re radiation is given by the first equation on this slide. The power density then produced back at the platform as a result of isotropic radiation from the target is as given by the second equation. In order to describe the actual power received back at the right app platform after having been scattered from the target, we introduce a property of the receiving antenna called its aperture. Which is also an area at right angles to the incoming beam of power density and which extracts power from that theme as seen in the last equation n this slide, the aperture has dimensions of area. The aperture of an antenna is a property we've used when describing its ability to extract power from an incoming wave front of power density. That is, when receiving. An antenna as a receiver can also be described in terms of its receiver gain, G subscript are, which is related to the aperture of the antenna as per the top expression on this slide, if we use receive again rather than aperture, the equation for receive power from the last slide becomes as shown in the center of the slide here. This is a famous equation. It's called the radar range equation. Note that there is suit powered varies inversely with the fourth power of range. It shows explicitly as should be expected that the power level received at the radar is a linear function of the target property. The radar cross section. Note that the equation has been derived on the basis of a point target rather than the distributed landscape which we will consider shortly. By point targets do arise in radar remote sensing. Any isolated strong reflector smaller than the size of a pixel can behave that way. Examples include houses, isolated trees, and targets intentionally located in the scene to help calibrate the radar. We will see examples of those later. We didn't have to see how we can modify the radar equation to account for the imaging of distributed regions, such as natural grasslands, forests, crops, oceans and so on. To do that, we represent the landscape by a collection of infinitesimal elements, as shown in the figure on this slide, each has an effective area of DS and a radar cross section of the Sigma. On the average, the region. Is it really that cross section per unit area of the signature? Yes, this is called the scattering coefficient. It is given the symbol sigma superscript o and it has units of meters squared per meter squared. That means in principle, it is really dimensionless. But it is important in practice to maintain the seemingly strange units of middle square can be described as we will see later. We're now treat each of the infinitesimal resolution elements as though, it were a single isolated point scatterer and use the right our equation which were previously as in the equations shown here within integrate all those contributions over the resolution cell to get the received power from the whole element as saying. The received power is a linear function of the scattering coefficient as it was for the radar cross section before. This is important, because it means that the measured reserved power is in proportion to the scattering coefficient of a surface. The scattering coefficient is usually expressed in the log rhythmic units of decibels and its value is related to a reference level. In this case, the scattering coefficient of one meter squared per meter squared. Base 10 logarithm so used and it is common to refer to the scattering coefficient as sigma nought. Radar cross sections for point targets are also expressed in decidbels reference to one meter squared as seen in the second equation. The disability notation is very useful in practice not just because the scattering coefficient and radar cross section can vary over several orders of magnitude, but also because values can be computed easily as we see on the next slide. This slide shows a range of scattering coefficients expressed in normal form. And indeed, the form along with some sample calculations which shall have the day bay form can be used to compute knew values easily. Because we have radars with different polarization configurations, we add subscripts to the scattering coefficient to indicate which transmit receive configuration was used to measure it. For quad or fully polarized radars, the four scattering coefficients are usually brought together into a sigma nought matrix. In more advanced treatments of radar imaging, we like to refer to the transmit receive situation in terms of electric fields rather than powers and power densities. At derivations here, we're based on the concept of power transmission and reception. But as we saw when discussing polarization, the actual signal is carried as a combination of propagating electric and magnetic fields. Just as we expressed the power view in terms of scattering coefficients, we can do the same with fields and in particular with the electric fields involved rather than scattering coefficients within describe the scattering events in terms of the elements of the scattering matrix indicated here. Incidentally, this matrix equation tells us why the second subscript on the matrix element here and in all treatments is the transmitted or incident polarization and the first subscript is a scattered or received polarization. If they were the other way around, the matrix vector multiplication would not work. Summarizing this lecture. The transmitted right our signal is treated by assuming isotropic radiation multiplied by the gain of the transmitting antenna. Gain is just a number, which tells us how much better the antenna is compared with isotropic transmission in the direction of interest. The target is described by an area called its radar cross section, which is defined by assuming that the power scattered from the target is isotropically reradiated. Distributed targets are described by the radar cross section per unit area, which we call the scattering coefficient both radar cross section and the scattering coefficient are polarization dependent. Both radar cross section and scattering coefficient are usually expressed in decibels. Finally, we can also describe the scattering behavior of a target in terms of electric fields rather than powers. In which case, we define and use the scattering matrix of the target. The first two questions Yeah, are important in understanding why the scattering coefficient of the landscape image by remote sensing radars, is a function of look, or incident angle. The last question shows you that radar image in space, essentially on two inverse square laws applied at the same time. 

### Module 3 Lecture 14: Speckle and an introduction to scattering mechanisms

In this lecture, we want to look at a major consideration in terms of the perceived quality of radar images. They often look speckled in appearance and lack the crisp quality of optical images. In the next lecture, we are going to look at the nature of the scattering coefficient for various Earth surface cover tops. Before that though, we need to be aware of this unusual problem of speckle, how it arises, and how it can be traded. That's what we want to do in this lecture, along with an overview of what is to follow. As just noted, one of the most striking differences in the appearance of radar imagery compared to optical image data is its poor radiometric quality, that is caused by speckle. The figure here shows a portion from an image of a fairly homogeneous region in which speckle is obvious. Speckle is a direct result of the fact that the incident energy is coherent. It is assumed to have a single frequency and the wavefront arrives at a pixel with a single phase. What does that mean?. Incident sunlight that forms the basis of most of our optical remote sensing is not coherent. It is a collection of energy at the different wavelengths corresponding to the wavelength range of the image and band, and there is no relationship between the phases of the different components. The phase angle of a sinusoid, which represents a propagating electric field, is a relative quantity which describes its position in time. In the diagram on the left, the phase angle is described with respect to the time origin. More often though, we talk about the differences in phase between two or more sinusoids as shown in the diagram on the right. A direct consequence of the phase difference between two or more sinusoids is interference. When they add in phase, we get a sinusoid of twice the amplitude, as in the top right-hand diagram. When they add out of phase or have a phase difference of 190 degrees between them we get zero, as in the bottom right-hand diagram. In between, we get somewhere between those two extremes depending on the phase difference between the sinusoids as illustrated. How does that explain speckle? Typically, a pixel will be a set of a very large number of incremental scatterers. We saw that when we developed the radar equation. Their returns combine to give the resultant received signal for the pixel. Such a situation is illustrated in the diagram here. The next brightness of the resolution element or pixel will be the result of all the incremental reflections interfering with each other. An adjacent pixel will have a different set of interfering scatterers, but will have about the same average brightness value if both pixels are of the same cover type. If nothing is done to reduce the speckle in a recorded radar image, the speckle level will be too high to allow sensible interpretation of the image. Before radar imagery is released for use, it usually has undergone some form of speckle reduction. There are many ways to reduce the influence of speckle. Some quite sophisticated filters are used, but often just a simple averaging of several images of the same scene is used. But averaging will unfortunately reduce spatial resolution as well since averaging is smoothing. However, when the radar is designed, it is anticipated that averaging will be needed to reduce speckle, so the designed spatial resolution, usually in azimuth, is higher than needed. After averaging the azimuth and range resolutions approximately match as required by the user. The number of signals averaged in the terminology of radar imaging is called the number of looks. On the next slide, we see an example of speckle reduction by averaging. This is a simple made-up example, but it illustrates the point that averaging will reduce to notice periods or speckle while retaining the average value of the signal, that is the scattering coefficient. Before separate images have been generated from a distribution with a mean of 50.7 and a standard deviation of 15.05. When full looks are averaged, the mean is preserved but the standard deviation has been halved. The speckle variance, which is equivalent to its notice power contribution, has been reduced by a factor of four. In the next lecture, we start our trip with our radar scattering mechanisms. That is, how different Earth surface elements scatter energy in radar remote sensing leading to the formation of an image. Here we've summarized the situation so that we have an overview of what is to come. The most common scattering mechanisms that contribute to the formation of images in radar remote sensing are illustrated here. Unlike the case of optical remote sensing where, because of the very short wavelengths involved, scattering mostly occurs in surfaces. In radar, the situation is much more complex, including the possibility of scattering from elements within the surface. In summary, the actual radar signal is carried by electric and magnetic fields. We concentrate just on the electric fields since the accompanying magnetic fields are directly related to the electric fields. Electric fields are represented as sine waves, since that is how that propagate. As a result, sets of electric fields can experience constructive and destructive interference. The backscattered signal from a pixel is composed of a large set of reflected fields from the myriad of scatterers that compose the pixel. Those fields interfere with each other, the result of which is that adjacent pixels, even though from the same cover type, may have different brightnesses. That causes the image to have a speckle appearance, even though adjacent pixels might have quite different brightness values because of speckle, the average brightness of scattering coefficient over a set of pixels of the same cover type will be representative of that cover type. Speckle can be reduced by averaging, which doesn't affect the average value of pixel brightness but reduces the variance and thus the image will look less noisy. The last two questions here are included to add to your understanding about look averaging in radar remote sensing. 

### Module 3 Lecture 15: Radar scattering from the earth's surface

In this next set of four lectures, we will look at how radar energy scatters from the earth's surface. This treatment is important in understanding radar images and in many ways, is similar to the importance of spectral reflectance curves in optical remote sensing. We will look at surface scattering, volume scattering, strong or corner reflector scattering and scattering from the surface of the ocean. Along the way, we will discover some interesting and unusual facts about how microwave energy interacts with the surface. Consider a beam of microwave energy incident vertically on a surface as shown in the diagram. To analyze what happens, we have to work in terms of electric fields and we describe the properties of the surface in terms of electrical quantities, the dielectric constant and sometimes it's conductivity. Conductivity does not appear often, but dielectric constant is a material property that is central to radar imaging. The dielectric constant of air is one. Whereas most natural media have dielectric constants in the range of about one to 80 or so. The diagram shows three components of electric field, that incident on a surface, that component reflected from the surface and that component transmitted into the surface medium. Note the definition of reflection coefficient on the right-hand side of this slide for vertical incidents. It is determined exactly by the dielectric constant of the medium. We can also define a power reflection coefficient as shown and a transmission coefficient. We don't use transmission coefficient very often in remote sensing. If we consider dry soil at microwave frequencies, the dielectric constant is about four, said that the power reflection coefficient is about 0.11. That tells us that only about 11 percent of the incident power is reflected. The remainder is transmitted into the surface. The surface therefore, would look quite stark in an image. By comparison, for water at microwave frequencies, the dielectric constant is much larger, about 81, so that the power reflection coefficient is about 0.64, telling us that about 64 percent of the incident power is reflected. The surface therefore would have a light tone in a radar image at vertical incidence. Water is a strong determinant of radar scattering, because it has a major impact on the dielectric constant of a substance. The diagram on this slide shows its effect on the dielectric constant or soil. The imaginary part is related to the conductivity of the soil, which tells us how effectively it will conduct electricity. Now, consider oblique rather than vertical incidence, but still with a smooth surface. If a beam of microwave energy is obliquely incident to a surface, as shown in the diagram, the reflection coefficient becomes polarization dependent. The two formulas given here show that fact. One is the reflection coefficient for horizontal polarization and the other, the reflection coefficient for vertical polarization. Reflection from a smooth surface is called specular, since it is mirror-like, because the reflected field is away from the incident radiation, that is from the radar itself, the surface appears black in imagery. Examples of this include, calm water bodies such as lakes and smooth soil surfaces. Now, consider rough surfaces. Most real surfaces exhibit some form of roughness when being imaged by radar as indicated in the figures here. We now wish to understand the backscattering properties of rough surfaces. However, how do we know beforehand whether we should consider a surface as rough? Well, we use the Rayleigh criterion for that purpose. Note that the decision is related to wavelength, incidence angle and the variation in height of the surface. In the previous slides, we looked at scattering from smooth to approximately rough surfaces. Now, what about the other extreme? A surface that is very rough? Such a surface is called Lambertian for which the backscattering coefficient is as shown in the equation here. Notice that it is independent polarization. It is possible to derive scattering models for surfaces with roughness ranges between pure specular and Lambertian. But those derivations are beyond the scope of this course. We do, however, show results on the next slide. This slide shows three surface scattering models which indicate how the scattering coefficient varies as a function of incidence angle and surface roughness. It is important to have a feel for these types of behavior, particularly for how smooth and very rough surfaces behave. Surface scattering is sensitive to polarization. This graph illustrates the HH and VV lack responses of a moderately rough surface, and it's HV cross polarized response. Remember, this spins at the incident radiation was vertically polarized as it was in the VV case. But there were some horizontally polarized scattering, as well as the vertical component. When cross polarized responses occur, we sometimes say deep polarization has taken place. In this slide, we summarize in two diagrams, surface scattering behavior as a function of the common wavelengths found in radar remote sensing, as a function of surface roughness and as a function of surface dielectric constant. All wavelengths show strong dependence on the dielectric constant of the surface material. Given that dielectric constant is a direct indicator of moisture content. This shows the importance of surface moisture in effecting the tone of a radar image. Note that higher moisture contents give a bigger radar response. Variations in surface roughness show more strongly at the longer radar wavelengths. At X band, it might be hard to discern such changes. So the image will have a more uniform tone overall, irrespective of whether the roughness of the surface might vary across the same. But note that in general, a rougher surface will give a bigger radar response. This slide shows a classic radar image recorded by the short-lived Seasat satellite in 1978. The enhanced backscatter, that is, the lighter regions, resulted from increased soil moisture owing to the effect of a storm to the West and the subsequent storm cells that traveled to the North East both late on the day prior to image acquisition. In summary, the reflected field in radar for smooth surfaces is determined by the reflection coefficient of the air surface interface. Secondly, the reflection coefficient is a function of the surface material dielectric constant, which in turn is a strong function of moisture content. Thirdly, when viewed a obliquely as in a radar looking to the side, a smooth surface will appear black in imagery since there is little or no backscatter. Next, as surface roughness increases, the level of backscatter and thus the radar image tone increases. Backscatter from smooth surfaces is a strong function of incidence angle. Whereas backscatter from rough surfaces is a weak function of incidence angle. Apart from specular scattering, backscattering from rough surfaces will have a cross polarized, that is an HP or VH component, which is also a weak function of incidence angle. Finally, longer radar wavelengths are more affected by surface roughness than shorter wavelengths. The second question here, start thinking about the choice of Bragg law parameters for particular applications. 

### Module 3 Lecture 16: Sub-surface imaging and volume scattering

We now want to look at the intriguing possibility of imaging below the surface with radar, along with the mechanism we call volume scattering. Not all of the energy incident on a surface is scattered. As seen in the last lecture, some is transmitted across the boundary and into the surface medium. There it is absorbed by losses, usually very close to the surface. Sometimes though, the losses are small enough to allow significant propagation into the medium, so that the energy is then scattered by buried features, allowing those features to be imaged. The loss of power density with propagation into the surface medium can be described by a simple exponential equation in which the exponent is called the absorption coefficient. It is given by the second formula on the slide, in which we see it is a function of wavelength, the dielectric constant of the medium, and its conductivity via the so-called imaginary part of the dielectric constant. Based on those equations, we can make a number of important observations. If kappa, the absorption coefficient, is large then power density drops quickly with distance into the medium. Kappa is smaller for longer wavelengths, indicating that there is better transmission into the surface material at longer wavelengths. We now define the penetration depth delta as that value of r at which the power density has dropped to 1/e of its value just under the surface. The diagram in this slide shows the penetration depth at L band as a function of moisture content. Note that for very dry media, we can get penetrations of several meters. It is important to note that the signal doesn't stop after the penetration depth. Depth is just a convenient measure of how quickly it falls away. Objects several penetration depths below the surface can still return measurable signals to the surface, noting that absorption happens, of course, on both the forward, or incident, and backward, or reflected, paths. This is a wonderful example of the ability of spaceborne SIR to image below the surface of very dry sands. The top image is an optical color infrared photo, and the bottom is a shuttle imaging radar C composite radar images, with the color assignments shown. The radar energy has penetrated below the sand sheet to reveal an old paleo channel of the Nile River, as shown. Adjacent buried drainage patterns are also visible in the bed rock under the sand sheet. We now turn to the scattering of radar energy from volume media, such as tree canopies, shrubs, and sea ice. Those types of media contain many individual scattering sites that are harder to identify but which collectively contribute to a backscattered signal, as in the diagram shown on the slide. A simple model of this type of behavior has been devised by assuming that the medium consists of a cloud of water droplets. They are the scatterers. Known as the water cloud model, it gives the scattering coefficient of the volume medium as shown by the equation on the slide. Note that it is a function of the thickness of the canopy and the volume metric properties of scattering coefficient and extinction coefficient. Note also that the loss of energy from the signal is a result of scattering by the fine particles of water. We can use that model to simulate volume scattering behavior. The diagram here compares volume scattering with very rough surface scattering in which we see that volume scattering is even less dependent on incidence angle than the roughest of surfaces. Although many surfaces might show a small specular component near the origin for surface scattering, except for this very rough case, there is never any specular component with volume scattering. Here we make two important observations about volume scattering, one to do with polarization dependence, and the other concerned with the frequency or wavelength dependence of the volume extension coefficient. The simple water cloud model of volume scattering is based on a set of small spherical scatterers. It therefore generates no cross-polarized returns. In general, however, volume scattering is highly depolarizing, which means it causes a cross as well as a co-polarized signal. That is because the scatter is are not spherical but have geometric shapes, such as found with twigs and leaves. As with like or co-polarized behavior, that is HH or VV, the cross-polarized, HV or VH, backscatter signal is very insensitive to incidence angle, but is usually much lower in magnitude than the level of the co-polarized response. The extinction coefficient for volume scattering is strongly dependent on wavelength, meaning that energy loss through the canopy is also wavelength-dependent. If the canopy has low loss, then imaging can be performed of the underlying medium, such as the soil surface and trunks underneath the tree canopy, and the water surface in the case of sea ice. In general, tree canopy attenuation is almost negligible at P band, it's very low for L band, it can be moderate at C band, and is generally high for X band. Although not a very good image, this JERS-1 image of a forested region in Australia shows how much higher the volume response of the forest is compared with the surrounding grassland. Both are vegetated, but the grassland at L band is behaving more like a surface scatterer. So in summary for this lecture, radar energy can penetrate very dry surfaces at long wavelengths. The absorption coefficient and the depth of penetration are strong functions of moisture content. Volume scattering is a weak function of incidence angle. In general, there are both cross and co-polarized components of volume scattering. Volume scattering is in general stronger than surface scattering at longer wavelengths. The canopy extinction coefficient for volume scattering is a strong function of wavelength. And finally, canopies appear almost transparent at P band but look like strong opaque scatters at X band. The second question here concentrates on the relevance of penetration depth. Bowler first asks you to think about a scattering mechanism that we will meet in the next lecture. 

### Module 3 Lecture 17: Scattering from hard targets

We now look at some unusual scattering behaviors that nevertheless occur often in remote sensing. Because of the longer wavelengths involved, some materials we image with radar act as hard targets in that they are capable of reflecting a very large proportion of the incoming wave front and thus can appear very bright. Examples of hard targets include metallic elements such as wire fences. Recall the forest example in the last lecture with the adjacent pasture fields. Other hard targets are facets directly aligned to the incoming radar beam such as roof facets and corner reflectors formed by paired horizontal surfaces and vertical structures. Examples include houses, tree trunks, and ships at sea. Because they are such strong reflectors, they tend to dominate the response of an individual radar resolution cell or pixel so that we revert to radar cross section to describe their scattering behavior rather than the scattering coefficient. We will now look at each of these in turn. First, consider simple metallic elements, such as wires appropriately aligned. A wire fence for example can act as a strong scatterer. While the analysis is complicated, we can note that, one, if the fence wire is aligned exactly at right angles to the incoming radar beam, which we call zero incidence angle in this context, and the electric field is in the plane of the wire, then it will reflect the beam strongly exhibiting a high radar cross section. Secondly, if the fence wire is of finite length, then it will also reflect strongly at angles just off zero incidence. Thirdly, if the electric field is not exactly aligned with the wire, there can still be some sizable reflection, particularly if the wire is thick. Finally, if the length of the wire or another elongated metallic element is a multiple of half a wavelength, the element is then called resonant and will give an extremely high response if it is correctly aligned to the incoming beam. Let's look at facet reflection. The roof of a building or another planar structure facing the incoming radar beam is called a facet scatterer. Its radar cross section is given by the formula shown on the slide. We call this it's bistatic radar cross section because the scattering or reflection occurs at an angle two Theta away from the incoming beam. Most of the time in radar remote sensing, we are interested in the case where Theta equal zero so that the radar cross section becomes four Pi times the square of the area of the facet with the dimensions expressed as a fraction of wavelength. Now we come to a very interesting mechanism and one which occurs surprisingly frequently in practice in a number of different forms. It is a manifestation of dihedral corner reflector behavior. The figure on the left shows a dihedral corner reflector. If it is large compared with the wavelength, then it's radar cross section within three decibels over 30-60 degrees is given by the left-hand equation. Now look at the right-hand figure, which is meant to represent the side of the building. The wall of the building has a radar image in the ground plane but the length of that image will depend on the incidence angle as indicated in the figure. It's radar cross section is therefore different from that of a simple dihedral corner reflector because the horizontal surface is not fixed. If we have a standing structure such as a building, then the formula on the right-hand side of this slide provides the appropriate model to use in order to understand its appearance in radar imagery. We can use that same dihedral corner reflector approach as the model for a tree trunk in radar imaging. As shown on the left-hand side here, the trunk has an image in the ground plane. Theoretically, we could use the expression for the radar cross section of a cylinder to help develop a model for the tree trunk ground combination. However, there is a simpler approach. If the trunk radius Y is large compared with the wavelength,it is simpler mathematically to represent the cylinder as an effective flat plate, as indicated in the right-hand diagram, with the equivalent widths as shown. The radar cross-section of the trunk standing on the surface is then given by the equation on the right-hand side. We need to make two modifications to the last expression to allow it to be used in real situations. First, for a tree, the vertical and horizontal materials are not ideal reflectors but are real substances such as wood and soil. We can account for that by introducing the power reflection coefficients for the trunk, that's row subscript T squared and the ground row subscript J squared into the expression for the red at cross-section. Secondly, we have to account for the attenuation of inner canopy that envelops the trunk. We can do that by adding a two-way absorption term, giving as the final radar cross-section of the tree the last formula on the slide. We now have a complete model for the trunk in a thorough situation and can use it to model the backscatter response of a forest in radar imaging. The backscatter curves shown here were produced from the expression for a tree trunk acting as a corner reflector as a function of canopy loss and with the parameters indicated here on the slide. Note that this is the first time we have seen backscattered drop away at small incidence angles. That is because the reflection of the vertical structure in the ground plane shrinks away as the angle becomes smaller. The falling backscatter at the high angles is the result of canopy attenuation, as can be appreciated when looking at the values of the extinction coefficient. Because of the pathway involving the ground in trunk scattering, the radar cross-section is a fairly strong function of the ground dielectric constant, as seen in the formula we derived for the radar cross-section of a tree. It will change, therefore, with changes in the ground material. Particularly if the ground changes from dry soil to water, as when a forest might be flooded. Remember the dielectric constant at microwave frequencies of dry soil is around five or so, whereas for water it is about 81. Therefore, a forest with a water understory will appear considerably brighter than one with a dry soil surface. As shown here, it will be about five decibels brighter. We see an example in the next slide. This slide shows an image of a forest in Australia is across a river and which receives annual floods with snow melts further upstream. The bright response corresponds to flood water from the river under the forest as a result of the corner reflector effect. In this study, the change in backscatter from dry to flooded understory was found from the image to be about 6.8 dB compared with 5 dB, we might expect from the change in dielectric constant of the understory from dry sort of water. However, in this region, the dry understory is unlikely to be sufficiently smooth as to be modeled fully by surface reflection coefficient. Instead, the surface may be more Lambertian, meaning less forward scatter in the dry double bounce model, and thus a larger difference when flooding occurs. So in summarizing this lecture, note that strong scatterers include metallic structures appropriately aligned to the incident radar beam, along with facets and corner reflectors. Next, In many cases strong scatterers will dominate the response of a radar resolution cell. Next, size of houses, trees, ships at sea, and ocean oil rigs will all give strong corner reflector responses. Next, trace without a dominant trunk will not behave as strong scatterers. Next, tree canopy attenuation has to be taken into account when modeling forests with strong scatterers, but not when considering buildings, ships, and oil rigs. Finally, changes in the ground surface can be assessed from radar imagery, even with a canopy, because of the change in power reflection coefficient. The second question draws your attention to the equivalent of canopy attenuation. The third question asked you to pay attention to cylindrical structures like tree trunks. 

### Module 3 Lecture 18: The cardinal effect, Bragg scattering and scattering from the sea

Now look at some further unusual scattering behaviors, one of which is sea-surface scattering. This treatment will finalize our consideration of scattering by looking at some unusual mechanisms that occur because of the coherent nature of the incident radiation. We will look first at Bragg scattering, which occurs when there are periodic structures on the ground, and then we will look at sea surface scattering, which is in fact related to Bragg scattering. Just before that though, we want to look at another feature of the double bounce dihedral corner reflector model for buildings, particularly in urban and city regions where houses and buildings often occur in rows along streets laid out in a regular grid patterns. In one of the quiz questions in the last lecture, you were asked to consider what would happen to the response of a dihedral corner reflector if the incoming beam was not precisely aligned with the face of the reflector. This is generally not a problem for tree trunks as a result of their cylindrical nature, but for flat vertical structures, such as the sides of buildings, if the incoming beam is inclined as illustrated in the diagram, the backscatter drops very quickly. Thus, buildings will only stand out in an image if the direction of radar illumination is at right angles to the wall in the horizontal plane. This leads to what is called the cardinal effect, an example of which is seen in the next slide. The word cardinal here derives from the cardinal points on a compass, that is north, east, south, and west. Here we see a portion of a survey image acquired over Montreal, Canada in 1984 demonstrating the cardinal effect. The bright central portion of the image is where cross streets are aligned orthogonally to the incoming radar energy. Whereas the darker portions to the north of about the same urban density have street patterns that are not orthogonal to the radar beam. We now come to a non unusual scattering mechanism which had its origins in crystallography. The Australia father and son team of William and Lawrence Bragg received the Nobel Prize in 1915 for x-ray diffraction, which has the same basis as what we're now going to consider in radar scattering. The wavelengths that are used in radar remote sensing that is about 3-50 centimeters are not too different from some of the periodicities we see in the landscape, such as row crops. Consider the interaction of an incoming radar front with the surface shown in the diagram on this slide, which is periodic in the plane of irradiation. It could represent, for example, the cross-section of a newly plowed field. When this region is irradiated, there will be moderately strong reflections from regularly spaced parts of the surface as indicated. It is a set of those reflections which constitute the backscatter that occurs from the radar resolution cell or pixel. Note that some returns travel further than others, so when looking at reconstructing the composite pixel response, we have to convert those additional travel distances into phase angles. Thus, the pixel response is made up of a set of signals with different phase angles. Those phase differences will be related to the angle of incidence and the spatial wavelength of the surface periodicity. Sometimes the signals will add constructively, giving a brighter than average return, whereas at other times they will add destructively. We saw the same effect when we considered speckle but here the interference results from periodicities in the landscape rather than randomly distributed incremental scatters. Usually, when we considered the different mechanisms within a pixel which contribute to backscatter, we simply add the backscattered powers. With Bragg scattering, however, we add the electric fields, which then squares the backscatter power density, making the pixel much brighter than if the periodicity were not present. There is one important condition to all this, and that is that the row-like periodicity has to be aligned orthogonally to the incoming wave front, otherwise the phase reinforcement will not occur. When it does, the condition for constructive reinforcement is that the surface and electromagnetic wavelengths are related by the formula shown, which is called Bragg's Law, and the behavior is called Bragg resonance. Here we see an example of Bragg Resonance with circular pivotal irrigated agricultural fields in Libya. The strong returns are much likely associated with Bragg Resonance. The radar of illuminations from the bottom of the scene, so that plowed furrows running across the scene give the enhanced returns. We now turn to how scattering occurs from the ocean and work on Bragg scattering is important here. A flat sea will behave like a specular reflector and will appear dark in radar imagery. To receive measurable backscatter, the sea surface must be made rough by some physical mechanism. The principle means for surface roughening is the formation of waves. There are two broad types of wave, both excited by the action of the wind blowing across the surface, they are distinguished by the mechanism that tries to restore the water surface against the driving effect of the wind. Gravity waves depend on gravitation acting on the disturbed massive water to counteract the effect of the wind, their wavelengths tend to be long, typically in excess of a few centimeters. Capillary waves have wavelengths shorter than a few centimeters, and rely on surface tension to work against the disturbance caused by wind action. They appear to ride on the gravity waves. For both waves, amplitude and wavelength is a function of wind speed, fetch, that is the distance over which the wind is in contact with the surface of the water, and the duration of the wind event. The bottom left-hand diagram here shows the so-called energy spectrum of typical capillary waves. It peaks around the wave number, roughly aligned with C band radar. Importantly though, this diagram tells us that presence among the capillary waves are components, in a farrier analysis sense, that can match the wavelength of an incoming radar beam and thus cause Bragg resonance to occur. From the spectrum, we see there is more energy at smaller wave numbers, and from the expression for Bragg resonance, we see that for a given radar wavelength lambda, we can select a smaller wave number on the capillary wave spectrum if we make the incidence angle small. Thus, oceanographic radar imaging is usually best done with small incidence angles. These images, recorded by SeaSat and SIR-A over a region of the Californian coastline of Santa Barbara, illustrate the importance of incidence angle in sea surface scattering. The seawater detail is much better expressed in the 20-degree imagery than in the 40-degree imagery. Note however the terrain distortion, at 20 degrees and the consequent difficulty in assessing terrain detail. Note also the strong dihedral corner reflector responses from oil rigs in the channel, easily seen at 40 degrees but less so at 20 degrees. Finally, notice what happens in radar images of the ocean. If the capillary waves are damped, oil slicks attenuate, considerably, the amplitudes of the capillary waves, meaning that incident radar energy has nothing to couple into, making those regions on an image look dark as seen here. Note also the effect that ship wakes have capillary waves and thus on the radar response. Summarizing here; First, the dihedral corner reflector effect requires the incoming radar beam to be orthogonal to the reflecting elements. Secondly, the cardinal effect shows how the alignment of straight patterns affects strong reflections in urban zones. Next, Bragg scattering occurs when the earth's surface has regular periodic features, it is also a strong reflection mechanism. Next, sea scattering entails coupling of the incoming electromagnetic radiation in the radar beam with capillary waves on the ocean's surface, it is strongest at small incidence angles. Finally, oil slicks damp the capillary waves and thus considerably reduce radar backscatter from the sea surface. When looking at the second question here, keep in mind that radar reflections are separated spatially in an image if the returns for the radar happen at different times. 

### Module 3 Lecture 19: Geometric distortions in radar imagery

We now come to the last week of our course. In it, we will look at some final topics in radar, including some innovative applications and say something about how different remote sensing imaging data types can be used together. First though, we look at distortions in radar images. The first form of distortion is shadowing. While not exactly a distortion, it does affect interpretation. As with optical images, shadows are caused in radar when signal cannot be received from behind an object. Unlike optical image shadows, however, shadowing in radar imaging is absolute. For nadir viewing optical sensors, it is possible sometimes to detect measurable signals from shadow zones because of atmospheric scattering of incident radiation into the shadowed regions at fairly short optical wavelengths. Radar shadowing is likely to be most severe in the far range and for larger angles of incidence. Whereas it is often non existent for smaller incidence angles. That can be appreciated by looking at the diagram on this slide. The first real geometric distortion we encounter, is that to do with the changing range resolution across this width. It is called near range compressional distortion. Recall that the ground grains resolution is best at far range and poorest at near range. Thus, the near range pixels cover a larger region of the surface than the far range pixels, and yet both are made the same size in the display product as illustrated in the diagram here. As a result, near range features are compressed into smaller than realistic display pixels. We see the impact of that on the next two slides. Consider a region on the ground in which there is a square of grid like features, such as field boundaries. Within each of the square cells, there could be many pixels. Imagine also that there are some diagonal lines as shown. They could be roads connecting across field corners. The right hand figure shows how that region on the ground will appear in recorded and displayed radar imagery. Not only do the near range features appear compressed, but linear features at angles to the flight line appear curved. The combined effect is as if the image were rolled over on the near swath side. The image at the top of this slide shows the effect again, but with a real image recorded by an aircraft radar. The bottom image has been corrected. We now look at the distortion peculiar to radar imaging. It arises because objects are delineated in the range direction by differences in time delay. As a result of that, how would a tall tower appear in the range direction in a radar image? The radar echo from the top of the tower arrives back at the radar before that from the base because it travels a shorter two way path. That causes the tower to lie over towards the radar on the image. To emphasize that, we draw concentric circles from the radar. All points lying on one of those circles, will be at the same slant ranged from the radar, and we'll create echoes with the same time delay. By projecting the circle which just touches the top of the tower onto the ground, we see that the tower top and indeed the whole tower superimposed on ground features closer to the radar, than the base of the tower. This effect is referred to as layover. By comparison, in optical imagery, vertical objects appear to lie away from the imaging device, since they are superimposed on features further from the device than the base. We now look at relief displacement, which is similar in origin to layover. Instead of a tower though, consider a vertical feature with some horizontal dimension, such as the model mountain shown in the diagram. Using the same principle of concentric circles to project the vertical relief onto the horizontal ground plane, several effects are evident. The front slope is foreshortened, the back slope is lengthened. Together, these effects suggest that the top of the mountain is displaced towards the radar set. In ground range format, they give the effect that the mountain is lying over towards the radar. If you go back to the Santa Barbara image in the last lecture, that relief distortion is quite evident in the 20 degree Seasat image. If we knew the local height, then the degree of relief displacement can be calculated. In principle, therefore, the availability of a digital terrain map for the region should allow relief displacement distortion to be corrected. Not only is the relief displaced, but the brightness of an image is modulated by topography, particularly in mountainous regions. On front slopes, the local angle of incidence will be smaller than expected, and thus the slopes will appear brighter. On back slopes, the angle of incidence will be larger than expected, making them darker than would otherwise be the case. This slide shows an example of relief displacement in a mountainous region, again with 20 degrees Seasat radar imagery. Slopes facing the radar illumination direction appear bright, while those away from the illumination appear darker. There is rather severe terrain distortion evident in the small circle, most easily seen by comparing the top image with the optical image at the bottom. In summary, because the formation of a radar image depends on resolving time delays in the range direction, several unusual geometric effects happen, which are not apparent in optical imagery. First, the recorded image appears compressed at near range, as against far range for optical imagery. Secondly, tall structures lay over towards the radar. Thirdly, relief is displaced towards the radar. Remember also, the displayed brightness of an image is modified by relief, according to the scattering characteristics of a surface at different angles of incidence. Finally, if mountainous terrain where covered by volume scatterer, such as shrubland at C band, relief modulation of brightness would be less evident. The first three questions here should help consolidate your understanding of terrain distortions in radar imagery. 

### Module 3 Lecture 20: Geometric distortions in radar imagery, cont.

This lecture, we finalize our consideration of distortions that occur in radar image data. Let's start by looking at the material we have covered on geometric distortions in radar images Seetha. We can in fact make some general observations that our value in choosing look or incidence angles suited to particular purposes. First, for low-relief regions, larger look angles will emphasize topographic features through shadowing, making interpretation easier, for regions of high relief, larger look angles will minimize layover and release distortion, but will exaggerate shadowing. Relief distortion is worse for smaller look angles, low look angle missions such as Seasat designed, for oceanographic applications often produced distorted imagery of high relief terrain. Recall the slide of ocean features in lecture 18, which compared Seasat at 20 degrees with SIR-B at 40 degrees. Finally, from spacecraft altitude, reasonable swath widths are obtained with mid range look angles, that is about 35-50 degrees. For those angles, there is generally a little layover and little shadowing. Look angles in this range are good would for surface roughness interpretation. With this slide, we introduce a new right at in-between concept. Not unreasonably, we have been concentrating on imagery that is representative of the earth's surface. That means in the across swath direction, we have been concentrating on converting slant range resolution into ground range pixels. But that gives rise to strange features, such as near range compressional distortion, as we have seen. Instead of producing images on the ground plane, we could keep them in slant range format, as seen on this diagram. In such a view, image has range coordinates measured along the slant direction, rather than along the ground. A simple way to envisage the slant range product is to project it out to the side of the platform as shown. The advantage of slant range imagery is that it doesn't suffer the near range compressional distortion encountered when the ground range form is used. That can be appreciated by looking at the series of full concentric rings in the figure. The distance between pairs of which represent the slant range for solution of the system. The dotted curves illustrate that relief distortion occurs in both forms of imagery. Let's now think about how we might correct geometric errors in radar imagery. There are two broad approaches to geometric error correction, depending on the level of topographic relief in the region being imaged. First, for low-relief regions, in which shadowing and relief displacement distortions are not considered significant, near range compressional distortion can be removed readily because we can model its effect. There will be remaining areas in geometry caused by the same factors we saw with optical imagery, platform attitude and altitude variations in earth rotation and so on. They are corrected by the same techniques, and we will look at some aspects of that approach on the next slide. For regions of high relief, we have to account for severe errors like relief displacement. If we had available a digital terrain map of the region, it is possible to use it to model relief distortion since we know the mechanisms which give rise to it. An artificial radar image of the region can be created from the DTM, with shading added through the adoption of a surface scattering curve. The actual image is then registered to the artificial image, and remember the pixels in the actual image are placed in their distorted positions. The relief distortion effects are then removed from the real registered image by reversing the DTM distortions. The pixels will then had been placed on their correct DTM positions. When we looked at geometric distortion with optical images, we use control points and mapping polynomials. We can do the same with radar images. However, because of the presence of speckle, it is often difficult to locate naturally occurring control points to the required degree of accuracy. As a result, artificial control points are often created prior to recording the image data, by deploying devices that will give recognizable returns in the received imagery. The positions of those devices are usually accurately known, through GPS fixing, for example, so that image correction is assisted. Those artificial control points can be passive or active. We will now look at both. Although flat plates facing the incoming radar beam could be used to provide abroad facet-like reflection. They have to be aligned very accurately to ensure the maximum radar cross section is available. They use as control points, therefore is limited. Instead, trihedral corner reflectors are used. Having three faces, they have a large range of angles around boresight over which their radar cross section is within 3dB of maximum. In this slide, we see the radar cross sections. First, for a square trihedral reflector, which has a 23 degree beam angle, and secondly, a triangular trihedral reflector, which has a 40 degree beam angle. Sometimes corner reflectors are deployed in set patterns so that they are more easily recognizable in the recorded radar image. Knowing the radar cross section of the reflector, it can also be used for radiometric calibration of an image. Rather than relying on passive reflectors for control points and the calibration, active radar calibrated is can be used. They are simple transponders in that they have a receiver and a transmitter. The incident radar energy on reception, is amplified by a given amount and then re-transmitted. As a result, the return signal can be much larger than that from a passive corner reflector. One small problem with an active transponder, which in radar remote sensing is called an active radar calibrator or ARC, is that the inbuilt electronics will introduce a small time delay into the reflection. That means the calibrator will appear at a larger range in an image than it really is. That is handled by making up the time delay to a known amount, which thus gives a known range shift in the image, but which is easily corrected by subtraction. A variation on the ARC is to have different polarizations for the receiver and transmitter antennas, so that cross polarized imagery can be calibrated. They are then called PARCs or polarized active radar calibrators. Summarizing this lecture, we note first that shadowing in radar imagery is a problem in regions of high relief, the choice of incidence angle is significant in reducing shadowing, minimizing terrain distortion, and highlighting sea surface features. Mid-range look angles are good for land surface remote sensing. Small look angles are good for sea surface applications. Radar images can be presented in ground range or slant range format. Control Points for rectification of radar imagery can be synthesized using passive radar reflectors, such as trihedral corner reflectors. Finally, active radar calibrators can also be used for control points, and are also good for accurate radiometric calibration. That's the second question here, remind you about our treatment of the cardinal effect. 

### Module 3 Lecture 21: Radar interferometry

We now start a treatment of some innovative uses of radar imaging that are not available to us in the same form with optical imagery. But first, we summarize the benefits of radar imagery. As we have seen in this series of lectures on radar, remote sensing, imaging with radar brings a number of advantages, including that imaging can be carried out through Cloud cover, imaging can be carried out at any time of day or night, sea state features can be detected and mapped. Imaging is sensitive to soil moisture. For very dry surfaces and long wavelengths, subsurface imaging is possible. Floods can be detected beneath forest canopies. At long wavelengths, forests back scatter is dominated by trunks, allowing woody biomass to be assessed. At shorter wavelengths, green leaf biomass can be assessed. Finally, radar scattering mechanisms at different from but complimentary to those experienced with optical imagery, suggesting there are benefits in using the two imaging modalities together. Now, let's look at a very different style of operation. Because radar uses pure or coherent radiation for imaging, as against the incoherent sunlight used in optical imagery, we can interfere two radar signals intentionally, to achieve a remarkable new application called radar interferometry. It allows topographic mapping to be carried out and changes in topography with time to be detected. We have seen interference in radar before, as in the speckle created by the interfering backscattered electric field vectors from incremental scatterers within the pixel and of course, in inbred resonance. The relative phase angles of the fields cause the interference. Those phenomena though are natural scattering mechanisms that depend on phase difference. What we want to do now, is use the difference in phase between two radar signals deliberately. We will see that that will allow us to determine the height of a pixel, as well as its latitude and longitude, and how that height changes with time. The technique is commonly known as interferometric SAR or InSAR. Consider two radars operating side-by-side at the same altitude, irradiating a region on the ground at height, h above a datum. The horizontal distance between the radars, B is called the baseline. Because the two radars are at different slant distances to the target shown as R_1 and R_2, there will be a difference in their phases on reception back at the radars for a pulse that has transmitted from both radars simultaneously. That two-way phase difference can be shown to be given by the first expression on this slide. If we differentiate that phase difference formula with respect to the height of that region being irradiated, we get an expression for phase sensitivity, which is a function of the known system parameters. As an example, look at the interfering of two S_1 beams. Substituting the relevant numbers into the formula shows us that we get almost 10 degrees of phase difference between the beams for every one meter change in elevation. We can invert and integrate the last equation to get height as a function of the differential phase angle, in which alpha subscript IF is called the interferometric phase factor. The constant of integration can be found from the height and phase difference at a particular location if that is important. Thus, by finding the phase difference at each pixel position, we can determine the corresponding pixel height, allowing a topographic map to be determined. For completeness, we note that we can represent the two separate radar signals at position i j in terms of fields, as shown in the center of this slide. When they are received, the complex product is formed as shown. The angle at which is the interferometric phase difference. This type of signal is actually called the interferogram. There are, however, some practical difficulties with using phase. In a previous slide, we saw that the phase difference between the received signals is given by the formula shown in the first line of the slide. It shows that the phase difference varies because of a change in incidents angle. There are two mechanisms that affect incidence angle. The first is height variations, which is what we are interested in and the other is position across the swath, which occurs even for a flat earth, and effectively interferes with the height-dependent changes in incidence angle which we are trying to find. This is called the flat earth variation of phase difference. It needs to be removed from the recorded phase difference between the two radars, which is reasonably straightforward to do. The correction is sometimes called phase flattening. The second problem with phase, which needs to be compensated concerns its cyclic nature. That can be seen in the example on this slide, which illustrates how there is an ambiguity in interpreting the difference in phase between two sinusoids. Phase angle is modulo 2 pi, so even though the absolute phase might surpass 2 pi in practice, mathematically, it gets reset to the range between zero and two pi. Again, the effect can be compensated fairly easily before the image product that is the interferogram is produced. This process is called phase unwrapping. By phase flattening, and phase unwrapping as standard operations carried out when generating topographic information right to out imagery. Interestingly, if we have two radars irradiating the surface, there can be two styles of operation. The option exists for having both transmit and receive, or only one transmit and both receive. The latter style of operation, called the standard mode, is common when both radars are carried on the same platform. When two separate platforms that used or a single platform is used on successive orbits with a baseline between them that is called repaid pass interferometry, then the formal style of operation issues and it is called the ping pong mode. Having summarized the main spur which topographic detail can be mapped with radar interferometry, we present in this slide an example produced by the TanDEM-X topographic satellite mapping mission. This was acquired using two satellites operating in a standard interferometric mode, one transmitting and both receiving. This map of matter is a very good examples of the detail that can be obtained. Summarizing this lecture: Because SAR uses coherent radiation, which preserves the phase angle of the transmitted and received electric fields, it is possible to employ two radars side-by-side to detect terrain height. As a result, three-dimensional topographic maps can be created. Usually, for SAR, they are called interferograms. Phase variations associated with a change of incidence angle as though the earth was flat have to be removed when creating the interferogram. That is called phase flattening. The 2 pi ambiguity and phase must also be compensated for. That is called phase unwrapping. There are two styles of operation, the standard and the ping pong mode. The two radars can be on the same platform or onto platforms flying side-by-side. Alternatively, a single radar and platform can be used, but with images tagged on subsequent orbits, which are then interfered to form the interferogram that assumes that the landscape does not change during the repeat passes. These questions relate to three different aspects of SAR interferometry, all of which are important to the system designer, but probably less so to the use of the interferometric product, which would normally be calibrated in terms of height. 

### Module 3 Lecture 22: Radar interferometry for detecting change

Having looked at topographic mapping with SAR, we now explore issues for detecting topagraphic change. Suppose we now have two radars, one following the other in the same orbit or even as subsequent orbits of the same platform. And that they image the same region of terrain at times 1 and 2 as indicated in the figure. Imagine that between those two times, there has been a shift in topographic feature in the range direction as shown. That will show up as a shift in slant range as indicated. A two-way interferometric difference in phase will occur between the two acquisitions given by the formula shown here. It depends only on the change in slant range as a fraction of wavelength. For ERS-1 for which the wavelength is 5.6 centimeters, a change in phase shifter 180 degrees corresponds to a change in slant range of just 14 millimeters. At 23 degrees incidents, that corresponds to a lateral shift in the range direction of 5.5 millimeters. That is an amazing degree of sensitivity to change. Ideally, there should be no horizontal separation of the two radars in along track interferometry. If that is acheived, then from the work of our previous lecture, there can be no phase difference between the two returning signals resulting from unchanging terrain relief. In practice though, there will often be a horizontal baseline as well as a time separation which we call a temporal baseline. That means that the phase difference we're looking for associated with topographic variations between the two times can be obscured by the phase difference resulting from static topography. That latter has to be removed. Removal of the phase difference associated with static topography can be done if a digital terrain model is available for the region. That model can be used to synthesize pixel by pixel the topographic interferometric phase. That can then be subtracted from the total phase difference on reception, leaving only the phase difference resolving from topographic movement. An alternative technique is to have three radar acquisitions, two of which are used to create an interferogram from the static topography, that is a DEM. Again, that is used to remove the effect of topography in the phase difference of two other acquisitions. This latter technology goes by the name of digital interferometric SAR or DinSAR. This slide shows three common methods for achieving the temporal baseline needed for a long track interferometry. One involves two antennas on the same platform. Another entails using two passes of the same platform. And the third uses two platforms in the same orbit, one following the other's and so called TanDem operation. This is an early example of change detection using along track interferometry. It's based on two acquisitions of the ERS-1 satellite. Note the amazing sensitivity to the subsidence of Bologna. This is a second example in this case generated from the Japanese ALOS-2 satellites inbound SAR. It is the data of the 2019 California earthquake, again note the enormous sensitivity to small changes in the land form. And as a final example, we returned to the Mount Etna illustration. Now an acquisition by the TanDem-X satelite has been used with a topographic map produced previously with the Shuttle Radar Topography Mission, SRTM. To produce a map of lava flows from an 8th of April 2010 eruption of Mount Etna. The SRTM used interferometric SAR to produce global scale topagraphic maps. The SRTM DEM was acquired in February 2000 and the TanDem-X acquisition was in October 2010. The SRTM data was used to remove phase variations due to static topography. Allowing the phase changes associated with topographic change to be obtained from the TanDem-X mission. The left hand image here shows differential phase variations associated with topographic change. Then the largest phase variations are those associated with lava flow. On the right-hand image, the lava flow detected by a long track SAR interferometry has been overlaid on the topographic model generated by crosstrack SAR interferometry. This is a great example of why SAR imaging is such an important and useful technology. Nothing like this can be so easily achieved with optical imaging. Summarizing this lecture, we have first that topographic changes can be detected using a long track interferometry or ATI. ATI can be acheived by mounting two radars on a single platform such as an aircraft by using repeat passes of the same platform usually spacecraft or by flying platforms in TanDem. When using temporal phase difference, should it take topographic change the phase difference between acquisitions resulting from any horizontal baseline must be removed? ATI is very sensitive to small changes in topography with time. And finally, the website shown here gives some good recent examples of SAR interferometry. The last two questions here are two aspects of the same effect. 

### Module 3 Lecture 23: Some other considerations in radar remote sensing

Now, look at some other imaging modalities with radar imaging, expanding our understanding of the possibilities of this wonderful imaging technology. The form of radar imaging we have been looking at so far, is referred to as strip mode, because it produces a continuous strip of imagery, as with optical remote sensing platforms. Other image types are all sat possible, three of which are illustrated in these next three slides. Sometimes, the swath widths generated in strip mode are not wide enough for some applications. The ScanSAR mode, provides a wider swath, by breaking the imaging into cells in the across track and the along track directions. The antenna, used is steered electronically over the cells as shown in the diagram. Each cell, is treated as a mini swath, in terms of the signal processing carried out to form an image, composite swaths of several 100 kilometers, then become possible. The processing needed to produce a ScanSAR image, is more complex than with conventional SAR, because of the need to join the scanned cells, but that is manageable. If the antenna beam, does not look directly to broadside, but for example, looks forward, the system is said to have squint. That can happen if the platform yaws, in which case, it is unintentional or it can be the result of a planned maneuver. It leads to coupling of the range, and azimuth dimensions of the image and to geometric distortion. If the antenna is squinted forward and then steered backwards as it passes, and continues to irradiate a target, as seen in the left-hand diagram. Then, higher resolution of that target region is possible, at the expense of resolution over the rest of the domain, this is called spotlight imaging. The image of, Sydney, shown on the right hand of the slide, is a great example of spotlight imaging. In which exceptionally good spatial resolution is achieved. We will see that in the next slide, in which the region around the bridge is expanded. The image was recorded by TerraSAR-X and created by averaging three high resolution spotlight images. The images were recorded on descending parsers, and the look direction was from the East, which is the right, in the image. Here, we see the former image zoomed in to the vicinity of the Sydney Harbour Bridge. Although, there are three images of the bridge, and is that unusual, one of them is highly detailed. The form of construction is easily seen, highlighting the exceptionally good spatial resolution, achieved with the spotlight mode. Take note of the other comments on the slide, concerning the features of the bridge and the image. We now need to say something about the types of radar image products available. For those of you with an engineering, or physics background, this should be straightforward. Radar image providers, make data available in a range of formats. Some tailored to particular applications, apart from interferometric and the tomographic products we will see later. Most will be derivatives of two fundamental formats, single look complex format and scattering coefficient format. Remember, that radar antennas radiate and receive electric fields, even though we often discuss the overall radar operation in terms of power and power density. If the amplitude and phase angle of the received field is recorded, that is called a complex signal. It is also a single look, in the sense that multi-look processing, to reduce speckle is carried out after reception. Taking these two properties together, the data as recorded will be single look complex. Mathematically, after it has been processed to extract the ranging pulse data from the underlying carrier frequency, the received signal can be described by the equation shown. When providing the single look complex data to the user, the radar operator, converts the received field into the complex elements of the scattering matrix, shown here. In which the operating polarization, is indicated by subscripts, as we saw earlier. Most often, the user will acquire scattering coefficient imagery, from the operator. Single look complex data, is modified in two ways to produce that form. First, the field amplitude is squared to produce intensity, which is directly related to received power. Secondly, look averaging is performed to reduce speckle and to produce a resolution cell that is roughly square in shape. In the single look complex product, the resolution cells will often be highly rectangular in the anticipation of square cells resulting from look summing. Again, in a multi polarization radar, a matrix of scattering coefficients can be provided as shown. Here, we see a variant of the traditional remote sensing imaging radar. It is not strictly necessary to have the transmitter and receiver at the same location as we have done up to now. Instead, we could put them at different positions and on different platforms. Such an arrangement is called bistatic SAR, as against the monostatic SAR configuration that we have been using so far. Clearly, there must be some form of communication between the transmitter and receiver so that the ranging pulse delay from transmission to reception can be computed. But that is only a matter of a telecommunications link. The transmitter could be on a space platform and the receiver on an aircraft. Also, the system can use what we call transmitters of opportunity, such as the signals from navigation satellites, which also reflect from earth surface features. Interestingly, note that the sun is an energy source of opportunity, in optical remote sensing. Although the analysis is more complicated, shadowing effects are different for bistatic SAR, and Earth's surface features behave differently because just their backscatter is no longer important, scattering at angles different from incidence angle, then become important. We can generalize the bistatic concept, it is possible to envisage a system with many transmitters and receivers. Variously called multistatic or network radars, their contemplated use is more common in surveillance applications than remote sensing, but with the trend towards clusters of small satellites for remote sensing applications, simple multistatic constellations are readily envisaged. Note that each receiver in such a configuration receives scattered signals from the target as a result of every transmitter. We now come to the next generalization of the operation of imaging radar. Standard synthetic aperture radar generates images of the landscape in the two horizontal spatial dimensions with detail in elevation projected onto the two-dimensional plane. In that respect, it is similar to optical imaging. InSAR, using two radars deployed across end or along track, takes the next step and enables the mapping of topographic relief and changes in relief with time, but it does not permit discrimination of detail vertically, such as the internal structure of a forest. By appropriately utilizing several radars, usually with vertical separation, for example, as passes of the same platform on different orbits, it is possible to identify vertical structure with the technique known as SAR tomography, or sometimes TomoSAR. Tomography resolves vertical detail by employing a synthesized vertical aperture much as azimuthal detail is resolved using aperture synthesis in normal SAR. This slide shows a recent example of TomoSAR, which illustrates the vertical resolution of forest detail. Five radar acquisitions recorded by the German Aerospace F-SAR system on 10th of June 2014, were used to derive the tomographic information. The F-SAR system is fully polarmetric, but was operated at L band for this experiment. It can also operate at X, C, S, and P bands. The top left-hand picture shows a radar image. A white transect indicates where forest vertical detail will be mapped, the bottom left-hand picture is a radar map of the canopy top or ground as appropriate. The canopy top information along the transit is seen on the very bottom right-hand picture, that allows a comparison with the canopy vertical or tomographic detail shown in the top right-hand picture. We have now completed our coverage of radar remote sensing. By way of summary of this last lecture, note that widespread widths can be achieved using the ScanSAR mode. The spotlight mode allows higher than usual resolution to be obtained over selected target regions. Bistatic radars use two platforms for imaging. Multistatic radar involves several transmitting and receiving antennas, each receiver can detect signals that result from scattering from each transmitter. Vertical detail can be resolved within a volumetric or compound scatterer using tomographic SAR, and TomoSAR uses a baseline orthogonal to the direction of propagation. The second question here is particularly important, it involves a scattering situation often encountered in radar imaging when multiple bounces of the incident radiation can occur before it is returned to the receiver. 

### Module 3 Lecture 24: The course in review

We have come to the end of our three modules, in which we have looked at optical and radar modes of image acquisition in remote sensing, and their applications. In this final short lecture, we'll look at the course in overview and consider what might be possible if we use more than one imaging technology in combination. In summary, our course has covered: sources of radiation for remote sensing, the importance of the atmosphere, optical remote sensing imagery, which included means for correcting errors and means for analysis, machine learning methods, including feature selection, and sampling and accuracy assessment, and radar remote sensing imagery, including distortions, understanding radar scattering, bistatic and multistatic radars, and interferometry and tomography. Now, let's look at how we can use the different images types together. Bringing different datasets together to extract information unable to be derived from a single source on its own is called data fusion. In remote sensing, that term is often applied just to the registration step, which ensures that the various image types are co-registered, and then registered to a map. But in many fields, data fusion refers to the fusion of evidence so that joint decisions can be made. We now wish to explore that interpretation briefly in remote sensing. To see why that is important, consider what we could do with registered optical and radar imagery. One example might be to use machine learning methods to create a thematic map of land cover from optical imagery and then overlay that on a digital terrain model derived from radar interferometry so that topographic influence on land cover can then be assessed visually. Another example is to use thermal imagery to map temperature gradients in the water outflows from a power station while using optical data to map the surrounding land use. In many cases, such combinations of different data types can be done manually, but the more general case is interesting. For example, how can we devise machine learning techniques to combine the diagnostic capabilities of both radar and optical imagery to identify land covers or land use? In this slide, we look at some possibilities of what could be done using the two data types together to make joint inferences about cover types. On the left-hand side, we see the sorts of labels one might find with thematic mapping from optical data, along with those which might result from the analysis of radar imagery. On the right-hand side, we see what refinements of the label for a pixel might be possible if we use the two data sources in combination. For example, a pixel which exhibits the characteristics of vegetation in optical imagery, but behaves as a corner reflector in radar imagery, would most likely be a forested pixel. Similarly, a vegetated pixel exhibiting Bragg scattering in radar might indicate row cropping, and so on. How can we perform such combined inferences? One approach is to fuse evidence at the label level, much as humans would do. The method for analyzing each individual data type would be matched to that data. For example, a support vector classifier might be used to analyze a contributing hyperspectral image whereas a knowledge of the incidence angle and polarization dependencies of scattering types in radar might be used to analyze co-registered radar imagery. We could then use several different approaches to combine those recommendations, including sets of rules, such as the one illustrated here. Such rules arise from the knowledge of expert image interpreters. There are other approaches, including probably the use of convolutional neural networks. We still have to see concrete examples of how they can merge evidence from different data types to derive pixel labels. One prospect might be to operate CNN's [inaudible]. The first layer is to analyze the individual data types, and the later layer is to do the fusion. Time will tell. In summary, radar, optical, and thermal imagery, each provide quite different diagnostic tools for understanding ground covers, that is, thematic classes. Optical imagery is determined by vegetation pigmentation, cellular structure and moisture content, by soil mineralogy and moisture content, and biomaterials suspended in water and by the bottom material. Radar scattering response is determined by the geometry of the scattering elements on the earth's surface and by moisture content, via the other property of dielectric constant. Finally, one way of forming joint inferences from the results of radar and optical image analysis is to make logical decisions based on user knowledge of how ground covers behave in both imaging modalities. This question illustrates how expert systems, based on production rules, can be used to merge evidence from different data types. 

## Course Closing Comments

You have now completed all three modules of the course, and hopefully you have undertaken the quizzes and tests too. As a result, you should now be familiar with optical and radar remote sensing imaging technologies and have an appreciation of how they're used in practice. You should also have a good appreciation of the major methods for image analysis that are used in remote sensing, particularly for optical data. That includes both supervised and unsupervised approaches. If you are interested in pursuing radar remote sensing further, you will find that it has its own analytical methods as well. Methods that exploit the special nature of radar images and what they tell us about the landscape. You are now in the position to pursue a second more advanced course in the theory and technologies of remote sensing or a more in-depth treatment of machine learning, and artificial intelligence procedures if they are of interest to you. More particularly though, you can focus in detail on a particular application domain. Developing application specific knowledge in remote sensing by building on the treatment in this course, following, you will find some suggestions I've made for further reading. I wish you well in your endeavors, and trust you enjoy applying the tools and techniques of remote sensing in your own field. 

{% include links.md %}
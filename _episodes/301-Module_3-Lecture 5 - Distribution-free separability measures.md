---
title: Module 3 Lecture 5 - Distribution-free separability measures
teaching: 
exercises: 
questions:
- "???"
objectives:
- "= = ="
keypoints:
- "- - -"
---

### Module 3 Lecture 5: Distribution-free separability measures

We now look at separability measures that can be used when it is not possible to represent classes by probability distributions. We will look at two different measures. The reason we need to develop separability measures that can be used with [inaudible] class models is so it can handle data of hyperspectral dimensionality. In such a case, there are insufficient training samples for the generation of reliable covariance matrices. Separability measures are still possible in these circumstances, but they are a bit more complicated to develop since we can't rely on measures that look at the difference between say, two normal distributions. ReliefF is one of the commonly used measures for feature selection, that is, for evaluating existing features, and seeing which of them could be discarded. This measure gives features a weight, which is adjusted by reference to the classes of data being used. Those features with weights below a user-specified threshold are discarded. We will develop the technique by reference to the two class, two-dimensional dataset shown on this slide. The classes have been drawn intentionally so that one feature x_1, does not aid separation while the other x_2, does. The process commences by selecting a pixel at random from one of the classes. We then find its nearest neighbors in the same and in the other class as shown. We now want to derive a measure that gives more weight to feature x_2, than feature x_1 with respect to those chosen pixels. Remember, we have two features, x_1 and x_2. We now define a weight for each feature that tells us how important it is with regard to class separation. Call these weights Omega_1 and Omega_2, respectively, but please don't confuse those symbols with the same symbols we use for classes. Here they refer to weights. The weights are initially set at zero and then updated using the pixel we have chosen randomly. We will shortly choose further random pixels to give us a better measure of the weights, but for the moment, just concentrate on the one in the diagram. For the weight corresponding to the i_th feature, we use the updating rule shown. In the rule, x_i is a feature of the randomly chosen pixel, x_ i superscript s is the corresponding feature of the nearest neighbor from the same class, and the x_i superscript o is the corresponding feature of the nearest neighbor from the other class. D is a distance measure. Applying the updating rule to the diagram shown, we see that the adjustment will be small and negative for feature x_1, but large and positive for feature x_2. That means the weight for the second feature increases, indicating its relative importance, while that for the first feature drops. The same process is carried out several times using a set of m randomly chosen sample pixels and updating the weights each time. The distance values are normalized by the number of samples m. If there is a reasonable distribution of pixels in each class. A little thought will show that the weight for the feature x_2 will go on increasing relative to that for x_1. A helpful modification is to use a set of the K nearest neighbors to the randomly chosen pixels as indicated in the diagram. In this case, the updating rule becomes as shown by the formula which indicates normalization by both the number of trials and the number of nearest neighbors. Again, the process is initialized with all weights Omega_i set to zero. Each weight is then updated by selecting m random samples using the rule above. At the completion of the process, those features x_i with weight values above a threshold are kept for subsequent classification. The ReliefF method, as originally formulated, applies to just two classes. It is readily extended to the more usual multi-class situation by adding in other class term, for each of the classes other than that from which the random sample is taken, leading to the more complete formula shown on the slide. The x_i superscript ok and they are features of the kth nearest neighbors of the current random pixel in each of the other classes. The probability expression p(o) over 1 minus p(s) whites the contributions from the other classes in proportion to their prior probabilities. In all these formulas, the distance measure can be any convenient metric. Euclidean and city block distances are the most commonly used. Another feature selection technique that avoids the need to use probability distribution models for classes, is non-parametric discriminant analysis or in the, like canonical analysis, it sets up measures of the distributions of pixel vectors within classes and the dispersion of the classes themselves in spectral space. It then sets and access transformation, which provides maximum separation between the classes based on those measures. Instead of using covariance matrices, though, it employs the concept of scattering matrices in the manner now to be developed. This is a bit complicated conceptually, but not hard mathematically. If you found the concepts difficult, it will affect your understanding of the equations. Since this procedure is only one of a number of distribution free methods, you may wish to pass over the material if you find it too complicated. But if you wish to employ it in practice, software is available that will guide you through issues. In its simplest form, NDA examines the relationship between the training pixels of one class and the nearest neighbor training pixels from another class. At the bottom of the slide, we show an expression for x, which represents pixel drive from classes, that is the nearest neighbor of pixel i from class r as shown in the diagram. Pay particular attention to how the subscript has been set up. It says j is a member of class s, which is a nearest neighbor of i, which is a member of class r. We can describe the distribution of class r pixels with respect to their nearest neighbors in class s by a covariance like calculation. However, because we're now not describing the distribution of pixels about a class main, which is a parametric description, it is better to use a different term than covariance matrix. To talk about the scatter of pixels with respect to each other, we use the term scatter matrix. The scatter of all of the training pixels from class r about their nearest neighbors in class s, is defined by the scattering matrix definition shown in the equation, which actually computes the expected value of the distance between those pixels times the transpose of that distance, giving a matrix effectively of the distance squared. In this expression, x subscript i is, remember r is the i pixel from class r. The amica r conditionality reminds us that the calculation is determined by pixels from class r. We then do a similar calculation for the scatter of the training pixels from class s about their class r nearest neighbors and then average the two measures. Usually, the average is weighted by the prior probabilities or relative abundances of the classes as shown in the first formula on the bottom of the slide. Often in the a is not just the nearest neighbor, but instead a set of k class is training pixels as the nearest neighborhood for each class r training pixel. The local mean over that neighborhood is then use in the calculation of the between class scattering matrix, giving rise to the second formula for the between class scatter matrix which includes a main vector term for the k nearest neighbors in class s shown on the next slide. The main vector in the previous calculations is given by the expression shown here. Note that if K, the size of the neighborhood, is the same as the total number of training pixels available in class s, the local mean becomes the class mean, and the between class scatter matrices resemble covariance matrices, although taken about the mean of the opposite class rather than the mean of the same class. This approach has to be made applicable to a multi-class situation. In doing so, we note that there are as many weighted means of the pixels from the other class as there are other classes. This is illustrated in the diagram for the case of three classes: r, s, and t. It is easier to express the expectations in algebraic form so that for c total classes the among-class scatter matrix is as in the formula given. In this expression, the inner sum computes the expected scatter between the N_r training pixels from class r and the mean of the nearest neighbors in class c different for each training pixel. The middle sum changes the class c, still relating to the training pixels from class r. The outer sum changes the class r for which the training pixels are being considered. The latter computation is weighted by the prior probability for the class. Having derived an expression for how the classes are scattered with respect to each other, we need to look at how the pixels scatter within the classes. We define the mean vector of a class based on the k nearest neighbors of the i_th pixel in class r from the same class as shown. This leads to the within-class scatter matrix in the second expression. How do we use the two scatter matrices: S_W and S_A to carry out feature reduction? We are looking for a new set of axes in which S_A looks as large as possible while at the same time, S_W looks as small as possible. We define the axis transformation in the usual way by the matrix equation Y equals D transpose X, and look for a y coordinate system in which we can maximize the joint measure, which we'll call J. That's defined as the trace of the within-class matrix inversed times the among-class matrix. Once the new axes have been found, we will have a maximum class separation in the first, followed by the next best separation in second, and so on. We then retain a subset of those transformed bands or axes as the feature-reduced representation of the data that we wish to classify. For high dimensional imagery, we need measures of separability that do not depend on the need to compute a covariance matrix. The method known as ReliefF devises a weighting scheme. Each feature or band is allocated a weight to indicate its significance. Bands with weights less than a user-specified threshold can be discarded when undertaking a classification. Non-parametric Discriminant Analysis, NDA, parallels canonical analysis in that it seeks a new set of axes in which to represent the pixel vectors, such that classes have maximum separation in the first new axis, next best separation in the next axis, and so on. Non-parametric scatter matrices are used in the calculations. After transformation, those low order features which do not help class separation are discarded, resulting in a data-set with smaller dimensionality. Please consult the solutions if you need to in order to help you consolidate your understanding of distribution free feature reduction techniques. 

> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
>    {: .solution}
 {: .challenge}

{% include links.md %}

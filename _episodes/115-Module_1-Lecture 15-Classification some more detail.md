---
title: Module 1 Lecture 15 - Classification some more detail
teaching: 
exercises: 
questions:
- "???"
  objectives:
- "= = ="
  keypoints:
- "- - -"
---
### Module 1 Lecture 15 Classification: some more detail

In this lecture, we will consolidate some of the concepts from the last and compare the attributes of a human interpreter undertaking image analysis compared with analysis being carried out by a classification algorithm. Remember, we are dealing with a two-stage process for machine labeling, training and classification. In the first, a human interpreter is involved in identifying training data as well as selecting the particular classifier to use, as we will see later. Let's now look at what a human can do in image interpretation that a machine currently finds difficult and vice versa. In this slide, we compare the performance of a human interpreter to that of a computer-based algorithm from a number of important perspectives. First, when an image can consist of thousands to millions of pixels, it is impractical for a human to try to work in general at the scale of an individual pixel. By comparison, because of the speed of a computer working at the pixel level is practical and normal. Because we estimate area in an image by counting pixels, if a human interpreter cannot easily work at the pixel level then the ability to estimate areas is also limited. Again, by contrast, area estimation by computer is very easy. We have already discussed this next one to some extent. But when a sensor has more than three bands, it is difficult for a human interpreter to maintain attention over all bands when carrying out an analysis. A machine, by comparison, can handle any dimensionality provided we setup the mathematics suitably which we are about to start in this lecture. Human beings are able to process only about 16 different levels of brightness between black and white. A computer can handle any number of brightness levels when looking at the properties of a given pixel right right to the set of values defined by the radiometric resolution of the sensor. Recall, an eight-bit sensor can represent 256 levels of brightness per band. An area which is easy for human beings to work with, but let's say if a computer algorithm is the determination of shape, proximity, and general spatial analysis. Humans are easily able to discern crop fields, roadways, lakes, and other shapes, and their locations and juxtapositions, whereas quite complex software algorithms are required for shape and spatial analysis by machine. A conclusion that can be drawn from this analysis is that the most successful image interpretation exercises in remote sensing are those in which the skills of the analysts are used to best effect with the powerful properties of computer algorithms. As inferred in the last slide, most image interpretation exercises in remote sensing would be based on machine classification, now often called machine learning, but will nevertheless involve the input of a skilled human analysts. We'll always keep that in mind, even though this course will be heavily focused on machine methods for image interpretation. The idea is to use human knowledge optimally to get the best results out of the machine classification algorithms in such a way that applying human knowledge on a small part of the data yields an understanding of the whole image. In the labeling stage, usually that's a very good investment in time and money. This slide represents the classification process in four blocks. The first is the potentially expensive one, in which the analyst has to find some labeled training data to feed into the classification algorithm. As noted here, in a real exercise, this can be a time-consuming and expensive step because it often involves field visits. The second step is to use the labeled training data to try in the particular classifier algorithm that the analyst has chosen to use. Software is always available for that task. The third step is the important one and where we reap the benefits of classification. After having put on lot of effort into training a classifier, we now set it to work labeling the whole image. The final step is to examine the thematic map produced by the classifier to see where the error is made in a classification, and there always will be some are within the bounds acceptable to the application at hand. Are we happy that certain crops are labeled with 95 percent accuracy, or do we want to do better, for example? In practice, we often find that we have to iterate or redo parts of our classifications to get the levels of accuracy required. Incidentally, how can we assess the accuracy of the thematic map? To do so requires further labeled samples, just like the samples we use for training. Indeed, during the acquisition of training pixels, the analyst will at the same time set aside some labeled pixels as testing data to be used when looking at how well the classifier has performed. We will consider that in some detail later in the course. Here we have shown the two steps of training and labeling in a very simple classification exercise. The image is a segment of a four-band Landsat multispectral scanner image with four predominant classes; vegetation, burned vegetation, urban, and water. Here it is easy to select training data from an inspection of the color composite image product as shown in the middle image. The training pixels of water are shown in purple, those for vegetation are shown as green, those for burned vegetation are in red, and those for the urban regions in deep blue. Those training pixels are fed to the classifier to produce the thematic map and table of areas on the right-hand side. Much of our attention in image understanding will now focus on machine methods for analysis, based on training as just illustrated, or in some cases using algorithms that discover the structure of the spectral space in which the regions are labeled afterwards. That is, rather than during a training step. We will look at a number of techniques. Some are simple and easy to use, but other more complex algorithms often give better results. As seen in the illustrations in this slide, straight lines might be simple separating boundaries, but higher curves can often separate more interwoven datasets. In general, the latter are more difficult to implement than just placing straight lines between the classes. Up to now, we have talked about lines, planes, and surfaces, which are concepts we normally associate with two and three-dimensional drawings. Once the dimensionality exceeds three, we use terms like hyperplanes and hypersurfaces. To finish off this lecture, we want to revisit the spectral space itself, since it is fundamental to the development of many machine learning methods. In a later lecture, we will also see how to transform the space into other, sometimes more effective representations of our recorded remote-sensing image data. Recall that the spectral space is just a Cartesian coordinate system in which pixels plot according to their brightnesses in each band or axis. At this point, it is of value to remind you that the next lecture, we'll commence a detailed analysis which involves properties of vectors and matrices. If you do not have that background and wish to follow the development, please consult some standard introductory treatments of that material. You can also consult appendix C of J.A. Richards Remote Sensing Digital Image Analysis, fifth edition, Springer Berlin, 2013, which develops the material in a form matched to how we will use it in remote sensing. Each pixel point is represented by a column vector, which is given a single symbol in lowercase bold font. It will have as many entries as there are bands. For an N-band sensor, the column vector will be N-dimensional. By representing the vector measurements in this form, we can use the whole field of vector and matrix algebra in developing our Machine Learning Tools. This summary simply reminds us of two aspects of the lecture, the human machine comparison and the importance of the spectral domain. The first question in this quiz asks you to think about the size of the spectral space. What does that also tell you about its density for a typical image that might consist of, say, a million pixels? 

> ## Quiz
>
> 1. ?
>
> > ## Solution
> >
> > 1. 
>    {: .solution}
    {: .challenge}

{% include links.md %}